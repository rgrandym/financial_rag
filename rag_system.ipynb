{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f434f05",
   "metadata": {
    "id": "1f434f05"
   },
   "source": [
    "# Modular Financial RAG System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eabf2f",
   "metadata": {},
   "source": [
    "NEW USER (no database):\n",
    "Section 1 (Setup) â†’ Section 3-4 (Process docs) â†’ Section 5+ (RAG)\n",
    "\n",
    "RETURNING USER (existing database):\n",
    "Section 1 (Setup) â†’ Section 5+ (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c96398",
   "metadata": {
    "id": "e6c96398"
   },
   "source": [
    "## 1. Core Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bfda1e",
   "metadata": {
    "id": "87bfda1e"
   },
   "source": [
    "#### 1.1. Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ui3k1skvrRQ0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37161,
     "status": "ok",
     "timestamp": 1759680925368,
     "user": {
      "displayName": "Nick Bradshaw",
      "userId": "04897433706475065009"
     },
     "user_tz": -60
    },
    "id": "ui3k1skvrRQ0",
    "outputId": "675ce84d-a165-4df5-c5b2-2ffd8e5cad5a"
   },
   "outputs": [],
   "source": [
    "# Install neccessary libraries\n",
    "!pip install --quiet sentence-transformers transformers chromadb pymupdf rank-bm25 python-docx ipywidgets spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6923b2",
   "metadata": {
    "id": "7b6923b2"
   },
   "source": [
    "#### 1.2 Enable jupyter extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "zGtKk9j0rZ98",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 367,
     "status": "ok",
     "timestamp": 1759680925739,
     "user": {
      "displayName": "Nick Bradshaw",
      "userId": "04897433706475065009"
     },
     "user_tz": -60
    },
    "id": "zGtKk9j0rZ98",
    "outputId": "1454a52a-4507-414e-b86b-f0c68c40606b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(55633) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: kernel kernelspec migrate run troubleshoot trust\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n"
     ]
    }
   ],
   "source": [
    "# Enable widgets\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc203c75",
   "metadata": {
    "id": "fc203c75"
   },
   "source": [
    "#### 1.3 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cf23e5c",
   "metadata": {
    "executionInfo": {
     "elapsed": 49764,
     "status": "ok",
     "timestamp": 1759680975507,
     "user": {
      "displayName": "Nick Bradshaw",
      "userId": "04897433706475065009"
     },
     "user_tz": -60
    },
    "id": "2cf23e5c"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Essential imports\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "from pathlib import Path\n",
    "import os, json, re, threading\n",
    "from typing import Dict, List, Optional, Any, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import requests\n",
    "import docx\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import CrossEncoder\n",
    "from transformers import pipeline\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "import pickle\n",
    "import re\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as widgets\n",
    "import datetime\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "import shutil\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import subprocess\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import snapshot_download\n",
    "from mlx_lm import load, generate\n",
    "import traceback\n",
    "import threading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd216ff",
   "metadata": {},
   "source": [
    "#### Section 1.4. Local Config\n",
    "\n",
    "This section establishes the core directory structure and configuration settings for the RAG system. It defines the foundational paths and validates the environment setup.\n",
    "\n",
    "Run this cell once. Make sure the path to the desired data directory is correct and uncommented. This will be the dataset processed in the following sections. If another dataset have already been processed and stored, then comment out the path to those files. This allows to create different collections in the vector database.\n",
    "\n",
    "**Directory Configuration:**\n",
    "- Sets `ROOT` as the current working directory using `Path.cwd()`\n",
    "- Defines three critical directories: `DATA_DIR` (source documents), `STORE_DIR` (ChromaDB storage), and `MODELS_DIR` (model cache)\n",
    "- Creates `file_directories` list specifying which subdirectories to scan for documents\n",
    "\n",
    "**Environment Validation:**\n",
    "- Performs existence checks on all defined paths\n",
    "- Scans and reports ChromaDB files if the store directory exists\n",
    "- Provides diagnostic output showing directory status and file counts\n",
    "\n",
    "**System Configuration:**\n",
    "- Establishes `CONFIG` dictionary with key parameters:\n",
    "  - `embedding_model`: BGE large model for semantic search\n",
    "  - `finbert_model`: Financial sentiment analysis model\n",
    "  - `llm_model`: Ollama model for response generation\n",
    "  - `chunk_size` and `overlap`: Text processing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "401807ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path verification:\n",
      "   ROOT: /Users/rodrigograndy/Desktop/coding_projects/posts_website_drafts/rag_boe\n",
      "   DATA_DIR exists: True\n",
      "   STORE_DIR exists: True\n",
      "   MODELS_DIR exists: True\n",
      "   ChromaDB files found: 4\n",
      "      3f516e47-f1a9-48bb-adb7-79543f55aa64\n",
      "      bm25_indices\n",
      "      chroma.sqlite3\n",
      "      c82d869e-4e75-4e96-af7b-b45927f70131\n",
      "Config: 7 settings loaded\n"
     ]
    }
   ],
   "source": [
    "# Core configuration to work from the current directory\n",
    "ROOT = Path.cwd() \n",
    "DATA_DIR = ROOT / 'data'\n",
    "STORE_DIR = ROOT / 'chroma_store'\n",
    "MODELS_DIR = ROOT / 'models'\n",
    "\n",
    "# Verify paths exist\n",
    "print(\"Path verification:\")\n",
    "print(f\"   ROOT: {ROOT}\")\n",
    "print(f\"   DATA_DIR exists: {DATA_DIR.exists()}\")\n",
    "print(f\"   STORE_DIR exists: {STORE_DIR.exists()}\")\n",
    "print(f\"   MODELS_DIR exists: {MODELS_DIR.exists()}\")\n",
    "\n",
    "if STORE_DIR.exists():\n",
    "    chroma_files = list(STORE_DIR.glob('*'))\n",
    "    print(f\"   ChromaDB files found: {len(chroma_files)}\")\n",
    "    for file in chroma_files[:5]:  # Show first 5 files\n",
    "        print(f\"      {file.name}\")\n",
    "\n",
    "file_directories = [\n",
    "    # DATA_DIR / 'pra_rulebook',\n",
    "    # Add back other directories when needed:\n",
    "    DATA_DIR / 'earnings_transcripts',\n",
    "    DATA_DIR / 'transcripts_analyst_meeting',\n",
    "]\n",
    "\n",
    "CONFIG = {\n",
    "    'embedding_model': 'BAAI/bge-large-en-v1.5',\n",
    "    'finbert_model': 'yiyanghkust/finbert-tone',\n",
    "    'mistral_7b_4bit_mlx': \"mlx-community/Mistral-7B-Instruct-v0.3-4bit\",\n",
    "    'llama_8b_4bit_mlx': \"mlx-community/Llama-3.1-8B-Instruct-4bit\",\n",
    "    'llama_3_2_3b_4bit_mlx': \"mlx-community/Llama-3.2-3B-Instruct-4bit\",\n",
    "    'chunk_size': 600,\n",
    "    'overlap': 100\n",
    "}\n",
    "\n",
    "print(f\"Config: {len(CONFIG)} settings loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeeee68f",
   "metadata": {},
   "source": [
    "#### 1.5.1 Function to Download Models if Required\n",
    "\n",
    "This section checks for the presence of required models in the MODELS_DIR and downloads them if they are not already available. It ensures that all necessary models are locally cached for offline use and faster loading times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0af3afec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download model from HuggingFace Hub\n",
    "def download_hf_model(repo_id, local_dir=\"models\"):\n",
    "    \"\"\"\n",
    "    Download a model from HuggingFace Hub.\n",
    "    \n",
    "    Args:\n",
    "        repo_id (str): HuggingFace model repository ID (e.g., 'mlx-community/Mixtral-8x7B-Instruct-v0.1-4bit')\n",
    "        local_dir (str): Local directory to save the model (default: 'models')\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to the downloaded model\n",
    "    \"\"\"\n",
    "    # Load environment variables from .env file\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Get HuggingFace token from environment\n",
    "    hf_token = os.getenv('HF_TOKEN')\n",
    "    \n",
    "    if not hf_token:\n",
    "        print(\"Warning: HF_TOKEN not found in .env file. Attempting download without authentication.\")\n",
    "        print(\"Some models may require authentication to download.\")\n",
    "    \n",
    "    # Create models directory if it doesn't exist\n",
    "    models_path = Path(local_dir)\n",
    "    models_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Extract model name from repo_id for the subdirectory\n",
    "    model_name = repo_id.split('/')[-1]\n",
    "    model_path = models_path / model_name\n",
    "    \n",
    "    try:\n",
    "        print(f\"Downloading model: {repo_id}\")\n",
    "        print(f\"Destination: {model_path}\")\n",
    "        \n",
    "        # Download the model\n",
    "        downloaded_path = snapshot_download(\n",
    "            repo_id=repo_id,\n",
    "            local_dir=str(model_path),\n",
    "            token=hf_token,\n",
    "            resume_download=True,  # Resume if interrupted\n",
    "            local_dir_use_symlinks=False  # Avoid symlinks for compatibility\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ“ Model downloaded successfully to: {downloaded_path}\")\n",
    "        return downloaded_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Error downloading model: {str(e)}\")\n",
    "        print(f\"\\nTroubleshooting tips:\")\n",
    "        print(f\"1. Verify the repo_id is correct: {repo_id}\")\n",
    "        print(f\"2. Check if the model exists at: https://huggingface.co/{repo_id}\")\n",
    "        print(f\"3. Ensure your HF_TOKEN has the necessary permissions\")\n",
    "        print(f\"4. Some models require accepting a license agreement on HuggingFace first\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb5c832",
   "metadata": {},
   "source": [
    "#### 1.5.2 Download Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfc2220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to download desired model.\n",
    "# Models repo IDs:\n",
    "# mistral_7b_4bit_mlx = \"mlx-community/Mistral-7B-Instruct-v0.3-4bit\"\n",
    "# llama_8b_4bit_mlx = \"mlx-community/Llama-3.1-8B-Instruct-4bit\"\n",
    "# llama_3_2_3b_4bit_mlx = \"mlx-community/Llama-3.2-3B-Instruct-4bit\"\n",
    "\n",
    "# # Usage\n",
    "# model_repo = llama_3_2_3b_4bit_mlx\n",
    "# try:\n",
    "#     model_path = download_hf_model(model_repo, local_dir='models')\n",
    "#     print(f\"\\nModel ready at: {model_path}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"\\nFailed to download model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c136a34",
   "metadata": {},
   "source": [
    "#### 1.5.3 Model Loading\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85b5a65c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0,
     "referenced_widgets": [
      "833b03027f334e738ce01bb374d443c7",
      "bce9801e5bf741e5802ec329b896b865",
      "25ebfb9caa314661b0ab5141d017758c",
      "6b8368f50f434023b3151b72301aa39a",
      "b9d3d39bcfb244bc9a891e2d59cedb90",
      "31f95e14f4a248729c23f5631b1853bf",
      "eb3fe9ae2cd4493189b6d887726e1e83",
      "d241d8f02dd245edbefaccc0499cc64d",
      "23c4ae2ca8f5472a95550619a144687e",
      "edf6a04bded343ee9166d3eae6d64ab1",
      "6f3ee0bde6ab41e281fed26154e62103",
      "64607f894dee4f1ca4d091a7a57ec50b",
      "a9155532191244819dcd06dd91fbcdeb",
      "b9b3496df4ec4d21b58f483488a81c58",
      "b0dfa7efbdc941bbab21ca7a69f3e288",
      "37baf24762364d0290cd6b647f7e90b8",
      "da1a9a7561654e3baa8d5e5d0c752bfb",
      "6b457f6bdc724fd49699c24220d8c3de",
      "efe44030aab94184b0033e4da4a79158",
      "78ad3fa9cda44354a406e245277bc06c",
      "68381eb90f49438ebb082bf35be5b0c7",
      "e4ac3c8083c44d649b3ada9e1cdb5e87",
      "c75919cc28a6413dbbbed67b3626482b",
      "1e5e4a4748ce45a18da266533d9c85e2",
      "4bcf1bcb4ed14ebd927654daa156e47c",
      "56884ad067b94258826ce20bc5327807",
      "59f07e8d71634443bb25868524f4cea9",
      "31456934173048929144dc3f620f7edd",
      "c07c4842d1134e74bea6598bbad88766",
      "a3d711a16e264baca2c220562b610380",
      "2616ee7a88484474b70321af416a0176",
      "508a29f08800481ab68e1ee9b655bc6f",
      "9b4a1ecb8bc445b69b3dfdc6e18fd658",
      "f16b068e44bd4344a6826201e574e563",
      "56b385661ac84a56b93d545f597890bd",
      "ad49598e795f4d42ac8ff98436b6f842",
      "6bdbb23315804666bfc78bdfe1c7a54f",
      "e5381ec6ff6a4abdb5ea26b3e92b918d",
      "f161ff575f3d41258f107bd48d999e2a",
      "640c048bc0334f8183710dded3534467",
      "de421b64497a426792f0eb3362984d5f",
      "5b64fbbf9b954f01ae804fbb984b79e7",
      "4cc56f7d6eba4c86be3c9c370033543f",
      "8c96043c30be48d191b6ceef997a5a0f",
      "610c6838db514d8a826f49899438c302",
      "d4904c83f08f4089be8f34a6f416d39d",
      "e7ec48374ac443d0ab0e092ef32516b9",
      "4cb64aa997db4b14afb7a21635a0bfd0",
      "40bae95b0e714dd0a833cfc6a8609eee",
      "8f3f797232bf41d58f0c302a156fe6e3",
      "333cab60c2da494694b92af7d03ee31c",
      "325f8734571442049c1a0f2d1442a0ac",
      "711e33111d9f480c9a634f3ec23c63c4",
      "141234bcec25497bb1c019566f17960f",
      "9170bfb81b3d4a11827455ff1fe8e4cd",
      "9757ad7f25c54c2f9349332160d6111b",
      "35bbb9f8bae24e99bbbf08dfbe766b75",
      "d4792208411c49f9b50cbd629efdac2c",
      "e861f7a725574e368fefbfeea0fbab92",
      "f18c5e64a0ee42adbe415805ec6e7c7c",
      "067359b8e9164cb4a279a44a8bb023b1",
      "143789d7c3d34a479c14cc3536578649",
      "16a23106d15f412995eb9ba0329c1886",
      "d43b73ef7dcf4768a6bfb702e0a54249",
      "d7711e822a624d3ea0bfbb814a620401",
      "011125111788426885b873cafbd87a78",
      "2a0d6f5245814cd39b5790b97e61deb8",
      "ccbeba7ed9ec4194a5a277cf4754d32b",
      "7a8a48a861774310b3f4be6f6375eae6",
      "45275f1f6a62484ba93727f83deecba2",
      "53aec41823db4ae99f038b95f884f5f6",
      "ec28d70c853c4302bc9c53811f8dd241",
      "6fc3e6fe1c684a638504dea7e1b664c6",
      "2a10cc5788f845f1a05ff9c277e3d318",
      "0d747272701a4dbea08cb3a315b3ed2f",
      "7a0bda4f4f6a4b44b36511d35d946199",
      "ba38df98b23d4d42bf7a950be5e5a8d0",
      "07a40955d5ae4b7a96419adfb80ad2a2",
      "a33ed0a08b0c4b3f9eb5a0e9fcf6991d",
      "98010a8969af428b91ec62cf5c92c6c7",
      "4f1116b208c14ad2b58f926b7f2ee91e",
      "9ce72a461cd848f7a65c8396466f5c3b",
      "e8795a4f719840c3a02d9bd0cc822e8e",
      "ea78889dd13b4ce18a0ed64e46572cb4",
      "aef208e7e95443fb82b78e192050219c",
      "6073bfc4b1744d148a4fae22a751ec3b",
      "f7b7ec82c79c442c8ff7fd341db187f3",
      "f10815bd490146cb8ccc0a7ebc1091ba",
      "56b9c6143d70447c8bb465e739b3f7be",
      "53d00d8abfce4a089712dc8dc313e551",
      "605bdb8f930d406ab4b75bd313368171",
      "a9b702ed3ca14c6caa7c50b8cc75b808",
      "9b55655f52e141359c1fa416c6258130",
      "ffbe959a1106473ab2a0ebfe989018c3",
      "0a380cc99b1a445d9360116d41fb92c4",
      "590bb1e289474fc2b8b4b44080ea959f",
      "70b16cb1feef442e995bad215de8502f",
      "baa965f5e6b14c46a046d393fba6eab9",
      "608e3756b92d4a298eb9cd237ca40819",
      "8c833970702c4cf09e893f64f2485c0b",
      "bb943a030ce847f4baec8737b2a9da2c",
      "0bf2fb6b6f084cec912a4c6c4168b223",
      "7f04e87948cc4ade8dc23cb39377ad8f",
      "51228e359a1f40b59d86fd23337e104e",
      "0640cd714c0f4eb1a22726bf46b5dd18",
      "aab40088907042ffa62b271529bcf09d",
      "7c9acbfc6d2b47a0918f7a4b3346c3e0",
      "1d1a81d1e3eb4e53b70636edfee84e84",
      "fba7798ba0d54431b8742df5f26ba51b",
      "a828c12cde2f4df896d1eb55c871519c",
      "02248ae2fb414bffba462faec4e10688",
      "605f28c35ef4440cae089adae8cce346",
      "99745fb34d784204bf8a6584dbd58d11",
      "98e05433f6d644d68319f9ee6ad0365e",
      "9e871ff59d3d4839ae34944515127c8f",
      "e5185ba9714648eba24081581e8b309f",
      "70b39d198820496e973ff45773b670ce",
      "ac7f2ab4a21a4465bd7f6f13a1d6b3a4",
      "8ae89d9f25ec41cdbfd7bd1de22d7cc0",
      "d01771254f1747a1b7f06a8bc10d333e",
      "73ca92175cfc4616b103334eecaa2e7b",
      "d4c30e040f2a49d0855e4445b487a763",
      "27fc018c9245466ebc03ee5f94bbb540",
      "dcf77be27f0b4eb98dcc1950b76fc5d9",
      "ab18b97898d44a90a295809f45c1efda",
      "1beafb96d3d94a868938b93b92e8dbd3",
      "3089b5d9bff6476f8632837f58bedf95",
      "14858ecbb38946758db1f1fd5d3fe2e6",
      "6ee385ee9ec2478786a2afacaed46903",
      "d2548a924bf64ff3b48b8ee390c9b192",
      "06739ce3b5a54ab38fcdf0c596b9aa82",
      "34613f7903194d29b9544094280ad8e5",
      "7526b1d3d6b046bd8151241402cb2185",
      "4a4b5dba9b3543e686338a482f83d7d5",
      "308459b8458b4affb27a58bd8b7c2e66",
      "47295889af0740c8b319bd7db2369b98",
      "24c7270a584b41be8820167b95a95772",
      "e8c4514f72c941b69089d35281e972fc",
      "d1f4028c2c754343b68e8b7cb5d059b5",
      "5b99e78a40ea444eab58749ed67c2cc4",
      "585c8e51b9a64aa9ba9f43a59744b8ff",
      "a23a8299b72642828e8bf2b9b3e9c6dd",
      "02f5083e008841188b41e5d385f54c7b",
      "055352abc36643b8bf092d0ee9262e53",
      "3c20a3fb99a3438692cb5107774b0ce3",
      "677b520b744c4187807e9479d6ad4eaa",
      "b87f211d6b714915a3544d78972584b2",
      "0df775e4bcff4c6cacf7de8343acc494",
      "655d8c6e9eff49e4b8959eee9181a12b",
      "38b5bef79d3a45fbb4d2783406c625b4",
      "428946310cd848f991a5943a8a288fbd",
      "4a307d1b81e447d6a231e13a86fa8276",
      "5e8566d45d3242d1bda993cffd6dd24a",
      "f650c5c75dbc4a01a2f2dcfe9c952b00",
      "4e50b0fc38b448078c0f284cb3f96d59",
      "f509299b1ff74309bc7301637c6aea84",
      "72fc22111351409ea86296557f01d0c7",
      "47f4738cf0d840b1a7f6e6533eeed61f",
      "d32c6bd92d304d8a89c2c78e49a387a5",
      "f9bac6a1fb5446a293c460944919c2a4",
      "09702809260543fc8d2d098ba2eff48b",
      "e0160cfe36c74422b1091fa6eb4ae8bd",
      "2f222110d2fa4738810ad83b97b4b7f0",
      "ffadaf728bbf44cc8e36446a7aa2af46",
      "7b7cff9050cb45f987d4ef2d65e75c4c"
     ]
    },
    "executionInfo": {
     "elapsed": 42304,
     "status": "ok",
     "timestamp": 1759681017843,
     "user": {
      "displayName": "Nick Bradshaw",
      "userId": "04897433706475065009"
     },
     "user_tz": -60
    },
    "id": "85b5a65c",
    "outputId": "5695dcc9-6327-4475-962d-3627df16c1ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading local embedding model from: /Users/rodrigograndy/Desktop/coding_projects/posts_website_drafts/rag_boe/models/embeddings/BAAI/bge-large-en-v1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading local FinBERT model from: /Users/rodrigograndy/Desktop/coding_projects/posts_website_drafts/rag_boe/models/financial/finbert-tone\n",
      "Models loaded: embedding + FinBERT\n"
     ]
    }
   ],
   "source": [
    "# Load embedding models (prefer local models/ directory)\n",
    "from typing import Optional\n",
    "def resolve_local_model_path(model_identifier: str, models_dir: Path = MODELS_DIR) -> Optional[str]:\n",
    "    if not model_identifier:\n",
    "        return None\n",
    "    # last segment of HF id or simple folder name\n",
    "    name = model_identifier.split('/')[-1]\n",
    "    try:\n",
    "        # find first directory under MODELS_DIR with matching folder name\n",
    "        for candidate in models_dir.rglob(name):\n",
    "            if candidate.is_dir():\n",
    "                return str(candidate)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def load_embedding_model(model_name: str) -> SentenceTransformer:\n",
    "    local = resolve_local_model_path(model_name)\n",
    "    if local:\n",
    "        print(f\"Loading local embedding model from: {local}\")\n",
    "        return SentenceTransformer(local)\n",
    "    print(f\"Loading embedding model from HF Hub identifier: {model_name}\")\n",
    "    return SentenceTransformer(model_name)\n",
    "\n",
    "def load_finbert_model(model_name: str):\n",
    "    local = resolve_local_model_path(model_name)\n",
    "    if local:\n",
    "        print(f\"Loading local FinBERT model from: {local}\")\n",
    "        return pipeline('sentiment-analysis', model=local, return_all_scores=True)\n",
    "    print(f\"Loading FinBERT model from HF Hub identifier: {model_name}\")\n",
    "    return pipeline('sentiment-analysis', model=model_name, return_all_scores=True)\n",
    "\n",
    "# Instantiate (will prefer local cached models under `models/` if present)\n",
    "embedding_model = load_embedding_model(CONFIG['embedding_model'])\n",
    "finbert = load_finbert_model(CONFIG['finbert_model'])\n",
    "\n",
    "print(\"Models loaded: embedding + FinBERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4021d05",
   "metadata": {},
   "source": [
    "#### 1.5.4 Local LLM Engine \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75547fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LocalLLMEngine with MLX ready (Apple Silicon optimized)\n",
      "\n",
      "Supported MLX models:\n",
      "  â€¢ llama_8b_4bit_mlx: 8B (4bit mlx, ~4.3GB)\n",
      "  â€¢ mistral_7b_4bit_mlx: 7B (4bit mlx, ~3.9GB)\n",
      "  â€¢ llama_3_2_3b_4bit_mlx: 3B (4bit mlx, ~1.6GB)\n",
      "\n",
      "ðŸ’¡ Install MLX: pip install mlx mlx-lm\n"
     ]
    }
   ],
   "source": [
    "# Local LLM Engine - MLX Optimized for Apple Silicon\n",
    "class LocalLLMEngine:\n",
    "    \"\"\"Local LLM engine using MLX for Apple Silicon optimization\"\"\"\n",
    "\n",
    "    RECOMMENDED_MODELS = {\n",
    "        # Keys align with CONFIG keys in section 1.4\n",
    "        'llama_8b_4bit_mlx': {\n",
    "            'name': 'Meta-Llama-3.1-8B-Instruct-4bit',\n",
    "            'params': '8B',\n",
    "            'quant': '4bit mlx',\n",
    "            'size_gb': 4.3,\n",
    "            'mlx_repo': 'mlx-community/Llama-3.1-8B-Instruct-4bit'\n",
    "        },\n",
    "        'mistral_7b_4bit_mlx': {\n",
    "            'name': 'Mistral-7B-Instruct-v0.3-4bit',\n",
    "            'params': '7B',\n",
    "            'quant': '4bit mlx',\n",
    "            'size_gb': 3.9,\n",
    "            'mlx_repo': 'mlx-community/Mistral-7B-Instruct-v0.3-4bit'\n",
    "        },\n",
    "        'llama_3_2_3b_4bit_mlx': {\n",
    "            'name': 'Llama-3.2-3B-Instruct-4bit',\n",
    "            'params': '3B',\n",
    "            'quant': '4bit mlx',\n",
    "            'size_gb': 1.6,\n",
    "            'mlx_repo': 'mlx-community/Llama-3.2-3B-Instruct-4bit'\n",
    "        },\n",
    "    }\n",
    "    MODEL_FILE_EXTS = (\"*.safetensors\", \"*.pt\", \"*.bin\", \"model.safetensors\")\n",
    "\n",
    "    def __init__(self, models_dir: Path, model_key: str, auto_warm: bool = True):\n",
    "        \"\"\"Initialize and load the model. Set auto_warm=False to skip the background warmup.\n",
    "        \"\"\"\n",
    "        self.models_dir = Path(models_dir)\n",
    "        if not self.models_dir.exists():\n",
    "            raise FileNotFoundError(f\"Models directory not found: {self.models_dir}\")\n",
    "\n",
    "        self.model_key = model_key\n",
    "        # Accept either a recommendation key (like 'llama_3_2_3b_4bit_mlx')\n",
    "        # or a repo_id / folder name (like 'mlx-community/Llama-3.2-3B- Instruct-4bit' or 'Llama-3.2-3B-Instruct-4bit')\n",
    "        self.model_info = self.RECOMMENDED_MODELS.get(model_key)\n",
    "        if not self.model_info:\n",
    "            # Allow passing a direct directory name or repo id as model_key\n",
    "            self.model_info = {\n",
    "                'name': model_key,\n",
    "                'params': 'unknown',\n",
    "                'quant': 'unknown',\n",
    "                'size_gb': 0.0\n",
    "            }\n",
    "\n",
    "        # Try to resolve the actual model path (may be nested)\n",
    "        self.model_path = self._resolve_model_path()\n",
    "\n",
    "        if not self.model_path or not self.model_path.exists():\n",
    "            # Provide helpful diagnostic output\n",
    "            candidates = self._list_candidate_model_dirs()\n",
    "            msg = (\n",
    "                f\"Model not found: {self.models_dir / (self.model_info.get('name') or self.model_key)}\\n\"\n",
    "                \"Expected MLX model directory with safetensors files.\\n\\n\"\n",
    "                f\"Attempted to locate model for key '{self.model_key}' (name='{self.model_info.get('name')}').\\n\"\n",
    "                f\"Detected candidate model directories under {self.models_dir}:\\n\"\n",
    "            )\n",
    "            for c in candidates:\n",
    "                msg += f\"  - {c}\\n\"\n",
    "            msg += (\"\\nOptions:\\n - Ensure model files (.safetensors/.pt/.bin) are present under the named directory,\\n\"\n",
    "                    \" - Or create a symlink matching the expected name, e.g.:\\n\"\n",
    "                    f\"     ln -s \\\"ActualModelFolder\\\" \\\"{self.models_dir / (self.model_info.get('name') or self.model_key)}\\\"\\n\")\n",
    "            raise FileNotFoundError(msg)\n",
    "\n",
    "        # Initialize MLX\n",
    "        try:\n",
    "            import mlx.core as mx  # noqa: F401\n",
    "            from mlx_lm import load, generate\n",
    "            print(f\"Loading MLX model from: {self.model_path} ...\")\n",
    "            self.model, self.tokenizer = load(str(self.model_path))\n",
    "            self.generate = generate\n",
    "            self.available = True\n",
    "            self.model_info = {**self.model_info, 'resolved_path': str(self.model_path)}\n",
    "            print(f\"âœ… MLX Model loaded: {self.model_info.get('name')} (resolved: {self.model_path})\")\n",
    "            # Background warmup to reduce first-call latency (optional)\n",
    "            if auto_warm:\n",
    "                threading.Thread(target=lambda: self._safe_warmup(), daemon=True).start()\n",
    "            \n",
    "        except ImportError as e:\n",
    "            self.available = False\n",
    "            raise ImportError(\n",
    "                \"MLX not installed.\\nInstall with: pip install mlx mlx-lm\\n\"\n",
    "                f\"Import error: {e}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            self.available = False\n",
    "            raise RuntimeError(f\"Failed to load MLX model from {self.model_path}: {e}\\n{traceback.format_exc()}\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, models_dir: Path, config: dict, key: str):\n",
    "        \"\"\"\n",
    "        Factory: create a LocalLLMEngine from CONFIG mapping.\n",
    "        Accepts either a recommended key (like 'llama_3_2_3b_4bit_mlx') or a repo-id string.\n",
    "        \"\"\"\n",
    "        model_key_or_repo = config.get(key) or key\n",
    "        return cls(models_dir, model_key_or_repo)\n",
    "\n",
    "    def _safe_warmup(self):\n",
    "        \"\"\"Run a single short generate to prime caches, ignoring errors.\"\"\"\n",
    "        try:\n",
    "            # Use a very small call to warm model/tokenizer\n",
    "            self.generate_response(\"Hi\", max_tokens=1)\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    def _resolve_model_path(self) -> Path:\n",
    "        \"\"\"\n",
    "        Resolve the best model directory path. Strategy:\n",
    "         1. Use the last segment of model_info['name'] (handles 'mlx-community/ModelName').\n",
    "         2. models_dir / that name (direct)\n",
    "         3. Search inside for nested dirs with model files\n",
    "         4. Otherwise scan models_dir for any directory that contains model files and pick best candidate\n",
    "        \"\"\"\n",
    "        raw_name = str(self.model_info.get('name') or self.model_key)\n",
    "        base_name = raw_name.split('/')[-1]  # handle 'org/ModelName' strings\n",
    "\n",
    "        candidate_root = self.models_dir / base_name\n",
    "\n",
    "        # 1) Direct path exists and has model files\n",
    "        if candidate_root.exists() and candidate_root.is_dir():\n",
    "            if self._dir_contains_model_files(candidate_root):\n",
    "                return candidate_root\n",
    "            # maybe model files are nested one-level down or deeper -> search inside\n",
    "            nested = self._find_nested_model_dir(candidate_root)\n",
    "            if nested:\n",
    "                return nested\n",
    "\n",
    "        # 2) Try variants and direct model_key folder\n",
    "        variants = [\n",
    "            candidate_root,\n",
    "            self.models_dir / self.model_key,\n",
    "            self.models_dir / raw_name,\n",
    "            self.models_dir / base_name.lower(),\n",
    "            self.models_dir / base_name.replace('-', '_')\n",
    "        ]\n",
    "        for v in variants:\n",
    "            if v.exists() and v.is_dir() and self._dir_contains_model_files(v):\n",
    "                return v\n",
    "            if v.exists() and v.is_dir():\n",
    "                nested = self._find_nested_model_dir(v)\n",
    "                if nested:\n",
    "                    return nested\n",
    "\n",
    "        # 3) Global scan for any folder containing model files (prioritize by folder name similarity)\n",
    "        candidates = self._list_candidate_model_dirs()\n",
    "        if not candidates:\n",
    "            return None\n",
    "\n",
    "        wanted_tokens = set(base_name.lower().replace('-', ' ').split())\n",
    "        best = None\n",
    "        best_score = -1\n",
    "        for c in candidates:\n",
    "            name = c.name.lower()\n",
    "            score = sum(1 for t in wanted_tokens if t and t in name)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best = c\n",
    "\n",
    "        return best or candidates[0]\n",
    "\n",
    "    def _dir_contains_model_files(self, p: Path) -> bool:\n",
    "        # Look for typical model files directly in directory\n",
    "        for globpat in (\"*.safetensors\", \"*.pt\", \"*.bin\", \"model.safetensors\"):\n",
    "            if any(p.glob(globpat)):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def _find_nested_model_dir(self, top: Path) -> Path:\n",
    "        # Depth-first search for a directory that contains model files\n",
    "        for fpat in (\"**/*.safetensors\", \"**/*.pt\", \"**/*.bin\"):\n",
    "            for m in top.glob(fpat):\n",
    "                return m.parent\n",
    "        return None\n",
    "\n",
    "    def _list_candidate_model_dirs(self):\n",
    "        # Return a list of paths under models_dir that look like model dirs (contain model files)\n",
    "        candidates = []\n",
    "        for sub in sorted(self.models_dir.iterdir()):\n",
    "            if not sub.is_dir():\n",
    "                continue\n",
    "            if self._dir_contains_model_files(sub):\n",
    "                candidates.append(sub)\n",
    "            else:\n",
    "                nested = self._find_nested_model_dir(sub)\n",
    "                if nested:\n",
    "                    candidates.append(nested)\n",
    "        return candidates\n",
    "\n",
    "    def generate_response(self, prompt: str, temperature: float = 0.0,\n",
    "                         max_tokens: int = 4000) -> str:\n",
    "        \"\"\"Generate response using MLX\"\"\"\n",
    "        if not self.available:\n",
    "            return \"LLM not available. Model not loaded.\"\n",
    "\n",
    "        try:\n",
    "            # Format prompt for instruction-tuned models\n",
    "            formatted_prompt = f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "            # The installed version of mlx_lm.generate does not accept 'temp' or 'temperature'.\n",
    "            # For temperature=0.0 (greedy decoding) we can omit sampler.\n",
    "            response = self.generate(\n",
    "                self.model,\n",
    "                self.tokenizer,\n",
    "                prompt=formatted_prompt,\n",
    "                max_tokens=max_tokens,\n",
    "                verbose=False\n",
    "            )\n",
    "\n",
    "            return response.strip()\n",
    "\n",
    "        except TypeError as e:\n",
    "            return f\"MLX Argument Error: {str(e)}. Try updating mlx-lm or checking generate() arguments.\"\n",
    "        except Exception as e:\n",
    "            return f\"Error generating response: {str(e)[:200]}\"\n",
    "\n",
    "print(\"âœ… LocalLLMEngine with MLX ready (Apple Silicon optimized)\")\n",
    "print(\"\\nSupported MLX models:\")\n",
    "for key, info in LocalLLMEngine.RECOMMENDED_MODELS.items():\n",
    "    print(f\"  â€¢ {key}: {info['params']} ({info['quant']}, ~{info['size_gb']}GB)\")\n",
    "print(\"\\nðŸ’¡ Install MLX: pip install mlx mlx-lm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688d9fc2",
   "metadata": {},
   "source": [
    "#### 1.5.5 Instantiate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c361e089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MLX model from: /Users/rodrigograndy/Desktop/coding_projects/posts_website_drafts/rag_boe/models/Llama-3.2-3B-Instruct-4bit ...\n",
      "âœ… MLX Model loaded: mlx-community/Llama-3.2-3B-Instruct-4bit (resolved: /Users/rodrigograndy/Desktop/coding_projects/posts_website_drafts/rag_boe/models/Llama-3.2-3B-Instruct-4bit)\n",
      "Local LLM instantiated: /Users/rodrigograndy/Desktop/coding_projects/posts_website_drafts/rag_boe/models/Llama-3.2-3B-Instruct-4bit\n"
     ]
    }
   ],
   "source": [
    "def create_local_llm_from_config(config_key: str, load_llm: bool = False):\n",
    "    \"\"\"\n",
    "    If load_llm=True, create and return a LocalLLMEngine instance resolved from CONFIG.\n",
    "    Otherwise returns None.\n",
    "    \"\"\"\n",
    "    if not load_llm:\n",
    "        print(\"LLM instantiation skipped (pass load_llm=True to create local LLM).\")\n",
    "        return None\n",
    "    try:\n",
    "        # Use from_config if available, else fall back to direct constructor\n",
    "        if hasattr(LocalLLMEngine, \"from_config\"):\n",
    "            engine = LocalLLMEngine.from_config(MODELS_DIR, CONFIG, config_key)\n",
    "        else:\n",
    "            engine = LocalLLMEngine(MODELS_DIR, config_key)\n",
    "        print(\"Local LLM instantiated:\", engine.model_info.get('resolved_path'))\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        print(\"Failed to instantiate local LLM:\", e)\n",
    "        raise\n",
    "\n",
    "# Usage:\n",
    "llm_small = create_local_llm_from_config('llama_3_2_3b_4bit_mlx', load_llm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c70546",
   "metadata": {},
   "source": [
    "#### 1.5.6 Test Small Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ec2809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Local LLM Engine\n",
    "# test_prompt = \"Are you ready to work?\"\n",
    "# response = llm_small.generate_response(test_prompt, temperature=0.0, max_tokens=50)\n",
    "# print(\"Test Prompt:\")\n",
    "# print(test_prompt)\n",
    "# print(\"\\nGenerated Response:\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1bd9b3",
   "metadata": {},
   "source": [
    "#### 1.6.1 Metadata Extraction using LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d84b1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata extractor - optimized for speed (first 3 pages only)\n",
    "def _parse_json_from_llm(text: str) -> Any:\n",
    "    m = re.search(r\"(\\{.*\\}|\\[.*\\])\", text, re.S)\n",
    "    if m:\n",
    "        try:\n",
    "            return json.loads(m.group(1))\n",
    "        except Exception:\n",
    "            pass\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Could not parse JSON from LLM response: {e}\\nRaw (truncated): {text[:400]}\")\n",
    "\n",
    "def _collect_person_candidates(full_text: str, max_people: int = 40) -> List[Dict[str,Any]]:\n",
    "    \"\"\"Optimized: reduced from 60 to 40 for speed\"\"\"\n",
    "    candidates = {}\n",
    "    patterns = [\n",
    "        r\"\\bBy\\s+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+){0,3})\\b\",\n",
    "        r\"\\bAuthor[s]?:\\s*([A-Z][A-Za-z ,\\.-]{2,80})\",\n",
    "        r\"\\bPrepared by[:\\s]*([A-Z][A-Za-z ,\\.-]{2,80})\",\n",
    "        r\"\\bReport author[s]?:\\s*([A-Z][A-Za-z ,\\.-]{2,80})\"\n",
    "    ]\n",
    "    for pat in patterns:\n",
    "        for m in re.findall(pat, full_text):\n",
    "            name = m.strip().strip(\",;. \")\n",
    "            candidates[name] = candidates.get(name, 0) + 6\n",
    "    for m in re.findall(r\"\\b([A-Z][a-z]+(?:\\s+[A-Z][a-z]+){1,3})\\b\", full_text):\n",
    "        name = m.strip()\n",
    "        if len(name.split()) <= 4:\n",
    "            candidates.setdefault(name, 0)\n",
    "            candidates[name] += 1\n",
    "    collected = []\n",
    "    for name, score in sorted(candidates.items(), key=lambda kv: -kv[1])[:max_people]:\n",
    "        occs = []\n",
    "        for match in re.finditer(rf\"\\b{re.escape(name)}\\b\", full_text):\n",
    "            start = max(0, match.start()-80)\n",
    "            end = min(len(full_text), match.end()+80)\n",
    "            ctx = re.sub(r\"\\s+\", \" \", full_text[start:end]).strip()\n",
    "            occs.append(ctx)\n",
    "            if len(occs) >= 2:  # Reduced from 3 to 2\n",
    "                break\n",
    "        collected.append({\"name\": name, \"score\": score, \"count\": len(list(re.finditer(rf\"\\b{re.escape(name)}\\b\", full_text))), \"examples\": occs})\n",
    "    return collected\n",
    "\n",
    "def _company_tokens_from_filename_and_headings(path: Path, full_text: str) -> List[str]:\n",
    "    stem = path.stem.replace(\"_\", \" \").replace(\"-\", \" \")\n",
    "    tokens = []\n",
    "    for tok in re.split(r\"[\\s,()]+\", stem):\n",
    "        if tok and tok[0].isalpha() and len(tok) > 2:\n",
    "            tokens.append(tok)\n",
    "    # Reduced from 30 to 20 lines for speed\n",
    "    for ln in (full_text.splitlines()[:20]):\n",
    "        ln = ln.strip()\n",
    "        if ln and len(ln) < 140 and any(c.isalpha() for c in ln):\n",
    "            tokens.append(ln)\n",
    "    # dedupe, prefer shorter tokens first\n",
    "    seen = []\n",
    "    for t in tokens:\n",
    "        short = t.strip()\n",
    "        if short and short not in seen:\n",
    "            seen.append(short)\n",
    "    return seen[:6]\n",
    "\n",
    "def _matches_company_in_context(name: str, company_tokens: List[str], text: str) -> bool:\n",
    "    for tok in company_tokens:\n",
    "        pat1 = rf\"{re.escape(name)}\\s*,\\s*{re.escape(tok)}\"\n",
    "        pat2 = rf\"{re.escape(name)}\\s*\\(\\s*{re.escape(tok)}\\s*\\)\"\n",
    "        pat3 = rf\"{re.escape(tok)}[^\\n]{{0,30}}{re.escape(name)}\"\n",
    "        if re.search(pat1, text, re.IGNORECASE) or re.search(pat2, text, re.IGNORECASE) or re.search(pat3, text, re.IGNORECASE):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def extract_file_metadata_strict_authors(filepath: Path, llm_engine=None, max_chars: int = 20000) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fast metadata extraction from document header (first 3 pages).\n",
    "    Optimized for speed - extracts only essential metadata for chunk tagging.\n",
    "    \n",
    "    Returns: dict with filename, file_extension, company_name, publication_year, \n",
    "    publication_quarter, title, doc_type, authors, verification, deterministic_evidence\n",
    "    \"\"\"\n",
    "    if llm_engine is None:\n",
    "        llm_engine = globals().get(\"llm_engine\", None)\n",
    "    if not llm_engine or not getattr(llm_engine, \"available\", False):\n",
    "        raise ValueError(\"An available LLM engine is required (llm_engine.available == True).\")\n",
    "\n",
    "    p = Path(filepath)\n",
    "    filename = p.name\n",
    "    file_ext = p.suffix.lower()\n",
    "\n",
    "    # Read ONLY first 3 pages for metadata (OPTIMIZATION)\n",
    "    try:\n",
    "        if file_ext == \".pdf\":\n",
    "            import fitz\n",
    "            doc = fitz.open(str(p))\n",
    "            # KEY CHANGE: Only read first 3 pages instead of all pages\n",
    "            pages_to_read = min(3, doc.page_count)\n",
    "            parts = [doc[i].get_text(\"text\") for i in range(pages_to_read)]\n",
    "            doc.close()\n",
    "            text = \"\\n\".join(parts)\n",
    "        elif file_ext == \".docx\":\n",
    "            import docx\n",
    "            doc = docx.Document(str(p))\n",
    "            # Read paragraphs but limit total length\n",
    "            parts = []\n",
    "            char_count = 0\n",
    "            for para in doc.paragraphs:\n",
    "                if para.text.strip():\n",
    "                    parts.append(para.text)\n",
    "                    char_count += len(para.text)\n",
    "                    if char_count > max_chars:\n",
    "                        break\n",
    "            text = \"\\n\".join(parts)\n",
    "        else:\n",
    "            text = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to read document text: {e}\")\n",
    "\n",
    "    # Limit to max_chars\n",
    "    text = text[:max_chars]\n",
    "\n",
    "    # Deterministic evidence to guide LLM (optimized)\n",
    "    person_candidates = _collect_person_candidates(text, max_people=40)  # Reduced from 80\n",
    "    company_tokens = _company_tokens_from_filename_and_headings(p, text)\n",
    "    year_candidates = re.findall(r\"\\b(19|20)\\d{2}\\b\", text[:2000])  # Reduced from 4000\n",
    "    year_candidates = sorted(set(year_candidates), reverse=True)\n",
    "\n",
    "    # Reduced from 40 to 25 candidates\n",
    "    compact_people = [{\"i\": i, \"name\": c[\"name\"], \"count\": c[\"count\"], \"examples\": c[\"examples\"][:1]} \n",
    "                      for i, c in enumerate(person_candidates[:25])]\n",
    "\n",
    "    # Optimized prompt - shorter and more focused\n",
    "    prompt = (\n",
    "        \"Extract metadata from document header. Return JSON with keys: company_name (string|null), \"\n",
    "        \"publication_year (int|null), publication_quarter (Q1..Q4|null), title (string|null), \"\n",
    "        \"doc_type (report/transcript/rule/policy/other|null), \"\n",
    "        \"authors: [{name:string, affiliation:string}] - include ONLY authors affiliated with the publishing company.\\n\\n\"\n",
    "        f\"FILENAME: {filename}\\n\"\n",
    "        f\"COMPANY_TOKENS: {json.dumps(company_tokens[:4], ensure_ascii=False)}\\n\"\n",
    "        f\"YEAR_CANDIDATES: {json.dumps(year_candidates[:3])}\\n\"\n",
    "        \"PERSON_CANDIDATES:\\n\" + json.dumps(compact_people[:20], ensure_ascii=False) + \"\\n\\n\"\n",
    "        \"DOCUMENT HEADER (first 3 pages):\\n\" + text[:12000] + \"\\n\\n\"  # Reduced from 20000\n",
    "        \"Return JSON only.\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        resp = llm_engine.generate_response(prompt, temperature=0.0, max_tokens=500)  # Reduced from 700\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM call failed: {e}\")\n",
    "\n",
    "    parsed = _parse_json_from_llm(resp)\n",
    "    if not isinstance(parsed, dict):\n",
    "        raise ValueError(\"LLM did not return a JSON object.\")\n",
    "\n",
    "    # Normalize and strictly verify authors\n",
    "    raw_auths = parsed.get(\"authors\") or []\n",
    "    verified_authors = []\n",
    "    verification = []\n",
    "    cmp_tokens = [t.lower() for t in company_tokens if isinstance(t, str)]\n",
    "    \n",
    "    for a in raw_auths:\n",
    "        if not isinstance(a, dict):\n",
    "            continue\n",
    "        name = (a.get(\"name\") or \"\").strip()\n",
    "        aff = (a.get(\"affiliation\") or \"\").strip()\n",
    "        if not name or not aff:\n",
    "            verification.append({\"name\": name or None, \"affiliation\": aff or None, \"kept\": False, \"reason\": \"Missing name or affiliation\"})\n",
    "            continue\n",
    "        \n",
    "        # Deterministic check: affiliation contains any company token\n",
    "        aff_l = aff.lower()\n",
    "        matched_token = None\n",
    "        for tok in cmp_tokens:\n",
    "            if tok and tok in aff_l:\n",
    "                matched_token = tok\n",
    "                break\n",
    "        \n",
    "        # Context check if token not in affiliation\n",
    "        context_match = False\n",
    "        if not matched_token:\n",
    "            if _matches_company_in_context(name, company_tokens, text):\n",
    "                context_match = True\n",
    "        \n",
    "        keep = bool(matched_token or context_match)\n",
    "        reason = \"affiliation matched company token\" if matched_token else (\"context match\" if context_match else \"no company evidence\")\n",
    "        verification.append({\"name\": name, \"affiliation\": aff, \"kept\": bool(keep), \"reason\": reason})\n",
    "        if keep:\n",
    "            verified_authors.append({\"name\": name, \"affiliation\": aff})\n",
    "\n",
    "    def _safe_int(v):\n",
    "        try:\n",
    "            return int(v)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    # Final output - optimized for chunk metadata\n",
    "    out = {\n",
    "        \"filename\": filename,\n",
    "        \"file_extension\": file_ext,\n",
    "        \"company_name\": parsed.get(\"company_name\") or (company_tokens[0] if company_tokens else None),\n",
    "        \"publication_year\": _safe_int(parsed.get(\"publication_year\")) or (int(year_candidates[0]) if year_candidates else None),\n",
    "        \"publication_quarter\": parsed.get(\"publication_quarter\") or None,\n",
    "        \"title\": parsed.get(\"title\") or None,\n",
    "        \"doc_type\": parsed.get(\"doc_type\") or None,\n",
    "        \"authors\": verified_authors,\n",
    "        \"verification\": verification,\n",
    "        \"deterministic_evidence\": {\n",
    "            \"top_person_candidates\": [c[\"name\"] for c in person_candidates[:10]],\n",
    "            \"inferred_company_tokens\": company_tokens,\n",
    "            \"year_candidates\": year_candidates[:5]\n",
    "        }\n",
    "    }\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e6ed70",
   "metadata": {},
   "source": [
    "#### 1.6.2 Test metadata extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc34b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filename': 'barclays_1Q20_earnings_transcript.pdf', 'file_extension': '.pdf', 'company_name': 'Barclays PLC', 'publication_year': 2020, 'publication_quarter': 'Q1', 'title': 'Barclays PLC Q1 2020 Results', 'doc_type': 'report', 'authors': [{'name': 'Tushar Morzaria', 'affiliation': 'Barclays PLC'}], 'verification': [{'name': 'Alvaro Serrano', 'affiliation': 'Morgan Stanley', 'kept': False, 'reason': 'no company evidence'}, {'name': 'Tushar Morzaria', 'affiliation': 'Barclays PLC', 'kept': True, 'reason': 'affiliation matched company token'}, {'name': 'Jonathan Pierce', 'affiliation': 'Numis', 'kept': False, 'reason': 'no company evidence'}, {'name': 'Joseph Dickerson', 'affiliation': 'Jefferies', 'kept': False, 'reason': 'no company evidence'}], 'deterministic_evidence': {'top_person_candidates': ['Tushar Morzaria', 'Jonathan Pierce', 'Alvaro Serrano', 'Morgan Stanley', 'Group Finance Director \\nOn', 'United States', 'Investor Relations', 'Numis \\nTwo', 'Group Finance Director \\nThe', 'Numis \\nCan'], 'inferred_company_tokens': ['barclays', 'earnings', 'transcript', 'Barclays PLC Q1 2020 Results', '29 April 2020', 'Results call Q&A transcript (amended in places to improve accuracy and readability)'], 'year_candidates': ['20']}}\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "# From section 1.5.5 we use llm_small for metadata extraction\n",
    "# meta = extract_file_metadata_strict_authors(Path(\"data/earnings_transcripts/barclays_1Q20_earnings_transcript.pdf\"), llm_engine=llm_small)\n",
    "# print(meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0236bba",
   "metadata": {},
   "source": [
    "#### 1.7.1 Enhanced Document Processor\n",
    "\n",
    "For chunking and downstream quality (accurate boundaries, keep tables together, attach correct section metadata): use use_layout=True.\n",
    "Do not enable preserve_layout=True in regular/production runs (itâ€™s for debugging or when you explicitly need raw pages). Instead, keep raw pages available in cache only when requested or during development.\n",
    "Store only the structured metadata (headers, table regions, page_count, char positions) in your pipeline and DB. Avoid storing raw_layout_pages in the vector DB â€” itâ€™s large and not needed for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61be627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Enhanced Document Processor ready\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Document Processor - Optimized for accurate structure extraction\n",
    "EXTRACTION_CACHE: Dict[str, Dict[str, Any]] = {}\n",
    "EXTRACTION_CACHE_DIR = Path(\".extract_cache\")\n",
    "EXTRACTION_CACHE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "class EnhancedDocumentProcessor:\n",
    "    \"\"\"\n",
    "    Comprehensive document processor optimized for structure-aware chunking.\n",
    "    Extracts: headers, sections, tables, lists, emphasis for intelligent chunking.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: Optional[Path] = None):\n",
    "        self.supported_formats = ['.pdf', '.docx', '.txt', '.md', '.csv']\n",
    "        self.cache_dir = cache_dir or EXTRACTION_CACHE_DIR\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Minimal cleaning - preserve structure markers\"\"\"\n",
    "        if not text:\n",
    "            return ''\n",
    "        if isinstance(text, (bytes, bytearray)):\n",
    "            try:\n",
    "                text = text.decode('utf8')\n",
    "            except Exception:\n",
    "                text = text.decode('utf8', errors='replace')\n",
    "        text = text.replace('\\x00', '')\n",
    "        text = re.sub(r\"\\r\\n|\\r\", \"\\n\", text)\n",
    "        # Preserve paragraph breaks\n",
    "        text = re.sub(r\"\\n{4,}\", \"\\n\\n\\n\", text)\n",
    "        return text.strip()\n",
    "\n",
    "    def _read_text_file(self, p: Path) -> Tuple[str, Dict[str, Any]]:\n",
    "        \"\"\"Read plain text with markdown structure detection\"\"\"\n",
    "        text = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        \n",
    "        lines = text.split('\\n')\n",
    "        structure = []\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            stripped = line.strip()\n",
    "            # Markdown headers\n",
    "            if stripped.startswith('#'):\n",
    "                level = len(stripped) - len(stripped.lstrip('#'))\n",
    "                structure.append({\n",
    "                    'type': 'header',\n",
    "                    'level': min(level, 6),\n",
    "                    'text': stripped.lstrip('#').strip(),\n",
    "                    'line': i,\n",
    "                    'char_position': sum(len(l) + 1 for l in lines[:i])\n",
    "                })\n",
    "            # Detect lists\n",
    "            elif re.match(r'^\\s*[\\-\\*\\+]\\s', line):\n",
    "                structure.append({\n",
    "                    'type': 'list_item',\n",
    "                    'text': stripped.lstrip('-*+ '),\n",
    "                    'line': i\n",
    "                })\n",
    "        \n",
    "        meta = {\n",
    "            'structure': structure if structure else None,\n",
    "            'has_markdown': len(structure) > 0,\n",
    "            'total_lines': len(lines)\n",
    "        }\n",
    "        return text, meta\n",
    "\n",
    "    def _read_docx(self, p: Path) -> Tuple[str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        DOCX extraction with comprehensive structure preservation.\n",
    "        Preserves: headings, emphasis, tables, lists.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import docx\n",
    "        except Exception as e:\n",
    "            raise ImportError(\"python-docx required. Install: pip install python-docx\") from e\n",
    "        \n",
    "        doc = docx.Document(str(p))\n",
    "        parts = []\n",
    "        structure = []\n",
    "        tables_data = []\n",
    "        char_position = 0\n",
    "        \n",
    "        for para in doc.paragraphs:\n",
    "            text = para.text.strip()\n",
    "            if not text:\n",
    "                continue\n",
    "            \n",
    "            style_name = para.style.name.lower() if para.style else ''\n",
    "            \n",
    "            # Detect headers\n",
    "            if 'heading' in style_name:\n",
    "                level_match = re.search(r'heading[\\s_]*(\\d+)', style_name)\n",
    "                level = int(level_match.group(1)) if level_match else 1\n",
    "                structure.append({\n",
    "                    'type': 'header',\n",
    "                    'level': level,\n",
    "                    'text': text,\n",
    "                    'char_position': char_position\n",
    "                })\n",
    "                parts.append(f\"{'#' * level} {text}\")\n",
    "            else:\n",
    "                # Check for bold/emphasis\n",
    "                if any(run.bold for run in para.runs):\n",
    "                    structure.append({\n",
    "                        'type': 'emphasis',\n",
    "                        'style': 'bold',\n",
    "                        'text': text[:50],\n",
    "                        'char_position': char_position\n",
    "                    })\n",
    "                parts.append(text)\n",
    "            \n",
    "            char_position += len(text) + 1\n",
    "        \n",
    "        # Extract tables\n",
    "        for tbl_idx, tbl in enumerate(doc.tables):\n",
    "            rows = []\n",
    "            for row in tbl.rows:\n",
    "                cells = [cell.text.strip() for cell in row.cells]\n",
    "                if any(cells):\n",
    "                    rows.append(cells)\n",
    "            \n",
    "            if rows:\n",
    "                # Create markdown table\n",
    "                table_md = self._format_table(rows)\n",
    "                parts.append(f\"\\n{table_md}\\n\")\n",
    "                tables_data.append({\n",
    "                    'index': tbl_idx,\n",
    "                    'rows': len(rows),\n",
    "                    'cols': len(rows[0]) if rows else 0,\n",
    "                    'char_position': char_position\n",
    "                })\n",
    "                structure.append({\n",
    "                    'type': 'table',\n",
    "                    'index': tbl_idx,\n",
    "                    'char_position': char_position\n",
    "                })\n",
    "                char_position += len(table_md) + 2\n",
    "        \n",
    "        combined = '\\n\\n'.join(parts)\n",
    "        meta = {\n",
    "            'structure': structure if structure else None,\n",
    "            'tables': tables_data if tables_data else None,\n",
    "            'structure_counts': {\n",
    "                'headers': sum(1 for s in structure if s['type'] == 'header'),\n",
    "                'tables': len(tables_data),\n",
    "                'emphasis': sum(1 for s in structure if s['type'] == 'emphasis')\n",
    "            }\n",
    "        }\n",
    "        return combined, meta\n",
    "\n",
    "    def _format_table(self, rows: List[List[str]]) -> str:\n",
    "        \"\"\"Convert table rows to markdown format\"\"\"\n",
    "        if not rows:\n",
    "            return \"\"\n",
    "        \n",
    "        # Use first row as header\n",
    "        header = rows[0]\n",
    "        body = rows[1:] if len(rows) > 1 else []\n",
    "        \n",
    "        md = \"| \" + \" | \".join(header) + \" |\\n\"\n",
    "        md += \"| \" + \" | \".join([\"---\"] * len(header)) + \" |\\n\"\n",
    "        \n",
    "        for row in body:\n",
    "            # Pad row if needed\n",
    "            row_padded = row + [''] * (len(header) - len(row))\n",
    "            md += \"| \" + \" | \".join(row_padded[:len(header)]) + \" |\\n\"\n",
    "        \n",
    "        return md\n",
    "\n",
    "    def _read_pdf(self, p: Path, use_layout: bool = True, preserve_layout: bool = False) -> Tuple[str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Enhanced PDF extraction using PyMuPDF with detailed structure tracking.\n",
    "        \n",
    "        Args:\n",
    "            p: Path to PDF file\n",
    "            use_layout: Extract tables and structure information with font details\n",
    "            preserve_layout: Keep original layout with positioning\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import fitz  # PyMuPDF\n",
    "        except Exception as e:\n",
    "            raise ImportError(\"PyMuPDF required. Install: pip install pymupdf\") from e\n",
    "        \n",
    "        structure = []\n",
    "        tables_data = []\n",
    "        parts = []\n",
    "        char_position = 0\n",
    "        \n",
    "        doc = fitz.open(str(p))\n",
    "        \n",
    "        try:\n",
    "            for page_num, page in enumerate(doc, 1):\n",
    "                if preserve_layout:\n",
    "                    # Extract with layout preservation - maintains spacing\n",
    "                    page_text = page.get_text(\"text\", sort=True) or \"\"\n",
    "                else:\n",
    "                    # Standard extraction\n",
    "                    page_text = page.get_text() or \"\"\n",
    "                \n",
    "                if use_layout:\n",
    "                    # Extract text with font information for structure detection\n",
    "                    blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "                    \n",
    "                    for block in blocks:\n",
    "                        if block.get(\"type\") == 0:  # Text block\n",
    "                            for line in block.get(\"lines\", []):\n",
    "                                line_text = \"\"\n",
    "                                avg_size = 0\n",
    "                                bold_count = 0\n",
    "                                \n",
    "                                for span in line.get(\"spans\", []):\n",
    "                                    line_text += span.get(\"text\", \"\")\n",
    "                                    avg_size += span.get(\"size\", 0)\n",
    "                                    if \"bold\" in span.get(\"font\", \"\").lower():\n",
    "                                        bold_count += 1\n",
    "                                \n",
    "                                line_text = line_text.strip()\n",
    "                                if not line_text:\n",
    "                                    continue\n",
    "                                \n",
    "                                # Detect headers by font size or bold\n",
    "                                spans = line.get(\"spans\", [])\n",
    "                                if spans:\n",
    "                                    avg_size = avg_size / len(spans)\n",
    "                                    # If font is large (>14pt) or bold, likely a header\n",
    "                                    if avg_size > 14 or bold_count > 0:\n",
    "                                        structure.append({\n",
    "                                            'type': 'header',\n",
    "                                            'text': line_text[:80],\n",
    "                                            'page': page_num,\n",
    "                                            'font_size': round(avg_size, 1),\n",
    "                                            'is_bold': bold_count > 0,\n",
    "                                            'char_position': char_position\n",
    "                                        })\n",
    "                    \n",
    "                    # Extract tables using PyMuPDF's table detection\n",
    "                    tabs = page.find_tables()\n",
    "                    if tabs.tables:\n",
    "                        for tbl_idx, table in enumerate(tabs.tables):\n",
    "                            try:\n",
    "                                rows = table.extract()\n",
    "                                if rows and any(any(cell for cell in row) for row in rows):\n",
    "                                    # Clean up rows\n",
    "                                    clean_rows = []\n",
    "                                    for row in rows:\n",
    "                                        clean_row = [str(cell).strip() if cell else '' for cell in row]\n",
    "                                        if any(clean_row):\n",
    "                                            clean_rows.append(clean_row)\n",
    "                                    \n",
    "                                    if clean_rows:\n",
    "                                        table_md = self._format_table(clean_rows)\n",
    "                                        \n",
    "                                        tables_data.append({\n",
    "                                            'page': page_num,\n",
    "                                            'index': tbl_idx,\n",
    "                                            'rows': len(clean_rows),\n",
    "                                            'cols': len(clean_rows[0]) if clean_rows else 0\n",
    "                                        })\n",
    "                                        structure.append({\n",
    "                                            'type': 'table',\n",
    "                                            'page': page_num,\n",
    "                                            'char_position': char_position\n",
    "                                        })\n",
    "                                        parts.append(table_md)\n",
    "                                        char_position += len(table_md)\n",
    "                            except Exception:\n",
    "                                pass  # Skip problematic tables\n",
    "                \n",
    "                # Additional header detection by pattern\n",
    "                lines = page_text.split('\\n')\n",
    "                for line_idx, line in enumerate(lines):\n",
    "                    stripped = line.strip()\n",
    "                    # Heuristic: ALL CAPS or short lines (likely headers)\n",
    "                    if stripped and stripped.isupper() and 3 <= len(stripped.split()) <= 8:\n",
    "                        # Check if not already detected\n",
    "                        if not any(s['type'] == 'header' and stripped in s['text'] for s in structure):\n",
    "                            structure.append({\n",
    "                                'type': 'header',\n",
    "                                'text': stripped[:80],\n",
    "                                'page': page_num,\n",
    "                                'char_position': char_position + line_idx * 50\n",
    "                            })\n",
    "                \n",
    "                parts.append(page_text)\n",
    "                char_position += len(page_text) + 1\n",
    "        \n",
    "        finally:\n",
    "            doc.close()\n",
    "        \n",
    "        combined = '\\n\\n'.join(parts)\n",
    "        meta = {\n",
    "            'structure': structure if structure else None,\n",
    "            'tables': tables_data if tables_data else None,\n",
    "            'structure_counts': {\n",
    "                'headers': sum(1 for s in structure if s['type'] == 'header'),\n",
    "                'tables': len(tables_data)\n",
    "            },\n",
    "            'preserve_layout': preserve_layout\n",
    "        }\n",
    "        return combined, meta\n",
    "\n",
    "    def _read_csv(self, p: Path) -> Tuple[str, Dict[str, Any]]:\n",
    "        \"\"\"Read CSV and convert to markdown table\"\"\"\n",
    "        import csv\n",
    "        \n",
    "        with open(p, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            reader = csv.reader(f)\n",
    "            rows = list(reader)\n",
    "        \n",
    "        if not rows:\n",
    "            return \"\", {'structure': None}\n",
    "        \n",
    "        table_md = self._format_table(rows)\n",
    "        meta = {\n",
    "            'structure': [{\n",
    "                'type': 'table',\n",
    "                'rows': len(rows),\n",
    "                'cols': len(rows[0]) if rows else 0,\n",
    "                'char_position': 0\n",
    "            }],\n",
    "            'tables': [{'rows': len(rows), 'cols': len(rows[0]) if rows else 0}],\n",
    "            'structure_counts': {'tables': 1}\n",
    "        }\n",
    "        return table_md, meta\n",
    "\n",
    "    def _cache_key(self, p: Path) -> str:\n",
    "        \"\"\"Create unique cache key from file stats\"\"\"\n",
    "        stat = p.stat()\n",
    "        return f\"{p.stem}_{stat.st_size}_{int(stat.st_mtime)}\"\n",
    "\n",
    "    def _cache_load(self, p: Path) -> Optional[Tuple[str, Dict[str, Any]]]:\n",
    "        \"\"\"Load from cache if available\"\"\"\n",
    "        key = self._cache_key(p)\n",
    "        cache_file = self.cache_dir / f\"{key}.json\"\n",
    "        \n",
    "        if not cache_file.exists():\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            return data['text'], data['meta']\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def _cache_save(self, p: Path, text: str, meta: Dict[str, Any]) -> None:\n",
    "        \"\"\"Save to cache\"\"\"\n",
    "        key = self._cache_key(p)\n",
    "        cache_file = self.cache_dir / f\"{key}.json\"\n",
    "        \n",
    "        try:\n",
    "            with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump({'text': text, 'meta': meta}, f)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def extract_text(\n",
    "        self, \n",
    "        file_path: Path,\n",
    "        use_layout: bool = True,\n",
    "        preserve_layout: bool = False,\n",
    "        max_chars: Optional[int] = None,\n",
    "        use_cache: bool = True\n",
    "    ) -> Tuple[str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Extract text with structure from single file.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the file\n",
    "            use_layout: Extract tables and structure information\n",
    "            preserve_layout: Preserve original layout with spacing (PDFs only)\n",
    "            max_chars: Maximum characters to return (None = no limit)\n",
    "            use_cache: Use cached results if available\n",
    "            \n",
    "        Returns: (text, metadata)\n",
    "        \"\"\"\n",
    "        p = Path(file_path)\n",
    "        ext = p.suffix.lower()\n",
    "        \n",
    "        if ext not in self.supported_formats:\n",
    "            raise ValueError(f\"Unsupported format: {ext}\")\n",
    "        \n",
    "        # Check cache\n",
    "        if use_cache:\n",
    "            cached = self._cache_load(p)\n",
    "            if cached:\n",
    "                text, meta = cached\n",
    "                # Apply max_chars if specified\n",
    "                if max_chars and len(text) > max_chars:\n",
    "                    text = text[:max_chars]\n",
    "                    meta['truncated'] = True\n",
    "                    meta['original_length'] = len(cached[0])\n",
    "                return text, meta\n",
    "        \n",
    "        key = self._cache_key(p)\n",
    "        if use_cache and key in EXTRACTION_CACHE:\n",
    "            text = EXTRACTION_CACHE[key]['text']\n",
    "            meta = EXTRACTION_CACHE[key]['meta']\n",
    "            # Apply max_chars if specified\n",
    "            if max_chars and len(text) > max_chars:\n",
    "                text = text[:max_chars]\n",
    "                meta['truncated'] = True\n",
    "                meta['original_length'] = len(EXTRACTION_CACHE[key]['text'])\n",
    "            return text, meta\n",
    "        \n",
    "        # Extract\n",
    "        if ext == '.pdf':\n",
    "            text, meta = self._read_pdf(p, use_layout=use_layout, preserve_layout=preserve_layout)\n",
    "        elif ext == '.docx':\n",
    "            text, meta = self._read_docx(p)\n",
    "        elif ext == '.csv':\n",
    "            text, meta = self._read_csv(p)\n",
    "        else:\n",
    "            text, meta = self._read_text_file(p)\n",
    "        \n",
    "        text = self.clean_text(text)\n",
    "        \n",
    "        # Add universal metadata\n",
    "        meta.setdefault('filename', p.name)\n",
    "        meta.setdefault('file_extension', ext)\n",
    "        meta.setdefault('file_size_bytes', p.stat().st_size)\n",
    "\n",
    "        # Cache before truncation\n",
    "        if use_cache:\n",
    "            EXTRACTION_CACHE[key] = {\"text\": text, \"meta\": meta.copy()}\n",
    "            self._cache_save(p, text, meta)\n",
    "\n",
    "        # Apply max_chars if specified\n",
    "        if max_chars and len(text) > max_chars:\n",
    "            meta['truncated'] = True\n",
    "            meta['original_length'] = len(text)\n",
    "            text = text[:max_chars]\n",
    "\n",
    "        return text, meta\n",
    "\n",
    "    def process_files(\n",
    "        self, \n",
    "        file_paths: List[Path], \n",
    "        use_layout: bool = True,\n",
    "        show_progress: bool = True\n",
    "    ) -> List[Tuple[Path, Optional[str], Optional[Dict[str, Any]]]]:\n",
    "        \"\"\"\n",
    "        Batch process files with structure extraction.\n",
    "        \"\"\"\n",
    "        files = [Path(fp) for fp in file_paths]\n",
    "        results: List[Tuple[Path, Optional[str], Optional[Dict[str, Any]]]] = []\n",
    "        success = 0\n",
    "\n",
    "        with tqdm(total=len(files), desc=\"Extracting structure\", disable=not show_progress) as pbar:\n",
    "            for fp in files:\n",
    "                try:\n",
    "                    text, metadata = self.extract_text(fp, use_layout=use_layout, use_cache=True)\n",
    "                    results.append((fp, text, metadata))\n",
    "                    success += 1\n",
    "                    \n",
    "                    # Show structure info in progress\n",
    "                    if metadata.get('structure_counts'):\n",
    "                        counts = metadata['structure_counts']\n",
    "                        pbar.set_postfix(\n",
    "                            headers=counts.get('headers', 0),\n",
    "                            tables=counts.get('tables', 0)\n",
    "                        )\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"\\n  âš ï¸  {fp.name}: {str(e)[:80]}\")\n",
    "                    results.append((fp, None, {\"error\": str(e), \"filename\": fp.name}))\n",
    "                finally:\n",
    "                    pbar.update(1)\n",
    "\n",
    "        if show_progress:\n",
    "            total_structures = sum(\n",
    "                len(m.get('structure', [])) \n",
    "                for _, _, m in results if m and 'structure' in m\n",
    "            )\n",
    "            print(f\"\\nâœ… Extraction complete:\")\n",
    "            print(f\"   Files: {success}/{len(files)} ({success/len(files)*100:.1f}%)\")\n",
    "            print(f\"   Structures detected: {total_structures:,}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "print(\"âœ… Enhanced Document Processor ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e0892e",
   "metadata": {},
   "source": [
    "#### 1.7.2. Test Enhanced Document Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09254ca2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "EnhancedDocumentProcessor.extract_text() got an unexpected keyword argument 'preserve_layout'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Test extraction with layout preservation\u001b[39;00m\n\u001b[32m      2\u001b[39m processor = EnhancedDocumentProcessor()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m text, meta = \u001b[43mprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/earnings_transcripts/barclays_1Q20_earnings_transcript.pdf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_layout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreserve_layout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_chars\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# force fresh extraction for verification\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMETA KEYS:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlist\u001b[39m(meta.keys()))\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- RECONSTRUCTED LAYOUT (first 4000 chars) ---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: EnhancedDocumentProcessor.extract_text() got an unexpected keyword argument 'preserve_layout'"
     ]
    }
   ],
   "source": [
    "# Test extraction with layout preservation\n",
    "processor = EnhancedDocumentProcessor()\n",
    "text, meta = processor.extract_text(\n",
    "    Path(\"data/earnings_transcripts/barclays_1Q20_earnings_transcript.pdf\"),\n",
    "    use_layout=True,\n",
    "    preserve_layout=True,\n",
    "    max_chars=None,\n",
    "    use_cache=False   # force fresh extraction for verification\n",
    ")\n",
    "print(\"META KEYS:\", list(meta.keys()))\n",
    "print(\"\\n--- RECONSTRUCTED LAYOUT (first 4000 chars) ---\\n\")\n",
    "print(text[:4000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d156dea",
   "metadata": {},
   "source": [
    "#### 1.7.3 Chunking Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "37b4dc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Structure-aware chunking ready\n",
      "   Features: header preservation, table integrity, section awareness\n"
     ]
    }
   ],
   "source": [
    "def chunk_with_structure_and_metadata(\n",
    "    text: str,\n",
    "    meta: Dict,\n",
    "    chunk_size: int = 600,\n",
    "    overlap: int = 100,\n",
    "    chunk_id_prefix: str = \"chunk\"\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Structure-aware chunking that preserves layout, tables, and headers.\n",
    "    Uses meta['structure'] and meta['tables'] from EnhancedDocumentProcessor.\n",
    "    \n",
    "    Returns chunks with enriched metadata including section headers.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    # Collect structure boundaries\n",
    "    boundaries = set([0, len(text)])\n",
    "    structure_map = {}  # char_pos -> structure info\n",
    "    \n",
    "    for s in (meta.get('structure') or []):\n",
    "        if 'char_position' in s:\n",
    "            pos = int(s['char_position'])\n",
    "            boundaries.add(pos)\n",
    "            structure_map[pos] = s\n",
    "    \n",
    "    for t in (meta.get('tables') or []):\n",
    "        if 'char_position' in t:\n",
    "            pos = int(t['char_position'])\n",
    "            boundaries.add(pos)\n",
    "            structure_map[pos] = t\n",
    "    \n",
    "    # Sort boundaries\n",
    "    b = sorted(boundaries)\n",
    "    \n",
    "    chunks = []\n",
    "    cid = 0\n",
    "    current_header = None\n",
    "    \n",
    "    # Process each section between boundaries\n",
    "    for i in range(len(b) - 1):\n",
    "        start_section = b[i]\n",
    "        end_section = b[i + 1]\n",
    "        \n",
    "        # Update current header if we're at a header boundary\n",
    "        if start_section in structure_map:\n",
    "            s = structure_map[start_section]\n",
    "            if s.get('type') == 'header':\n",
    "                current_header = {\n",
    "                    'text': s['text'],\n",
    "                    'level': s.get('level'),\n",
    "                    'page': s.get('page')\n",
    "                }\n",
    "        \n",
    "        section_text = text[start_section:end_section].strip()\n",
    "        if not section_text:\n",
    "            continue\n",
    "        \n",
    "        # Check if this section is a table - keep it intact\n",
    "        is_table = (start_section in structure_map and \n",
    "                   structure_map[start_section].get('type') in ['table_region', 'visual_element'])\n",
    "        \n",
    "        if is_table or len(section_text) <= chunk_size:\n",
    "            # Keep small sections or tables intact\n",
    "            chunk = {\n",
    "                \"chunk_id\": f\"{chunk_id_prefix}_{cid}\",\n",
    "                \"text\": section_text,\n",
    "                \"offset\": {\"start\": start_section, \"end\": end_section},\n",
    "                \"size\": len(section_text),\n",
    "                \"section_header\": current_header,\n",
    "                \"is_table\": is_table,\n",
    "                \"boundary_type\": \"table\" if is_table else \"section\"\n",
    "            }\n",
    "            chunks.append(chunk)\n",
    "            cid += 1\n",
    "        else:\n",
    "            # Sliding window within section\n",
    "            s_idx = 0\n",
    "            while s_idx < len(section_text):\n",
    "                end_idx = min(s_idx + chunk_size, len(section_text))\n",
    "                \n",
    "                # Find natural break\n",
    "                window = section_text[s_idx:end_idx]\n",
    "                cut = None\n",
    "                for sep in ['\\n\\n', '. ', '; ', '\\n', ' ']:\n",
    "                    pos = window.rfind(sep)\n",
    "                    if pos > chunk_size // 3:\n",
    "                        cut = s_idx + pos + len(sep)\n",
    "                        break\n",
    "                \n",
    "                actual_end = cut if (cut and cut > s_idx) else end_idx\n",
    "                chunk_text = section_text[s_idx:actual_end].strip()\n",
    "                \n",
    "                if chunk_text:\n",
    "                    chunk = {\n",
    "                        \"chunk_id\": f\"{chunk_id_prefix}_{cid}\",\n",
    "                        \"text\": chunk_text,\n",
    "                        \"offset\": {\"start\": start_section + s_idx, \"end\": start_section + actual_end},\n",
    "                        \"size\": len(chunk_text),\n",
    "                        \"section_header\": current_header,\n",
    "                        \"is_table\": False,\n",
    "                        \"boundary_type\": \"natural\" if cut else \"hard\"\n",
    "                    }\n",
    "                    chunks.append(chunk)\n",
    "                    cid += 1\n",
    "                \n",
    "                # Advance with overlap\n",
    "                s_idx = max(actual_end - overlap, s_idx + 1)\n",
    "    \n",
    "    # Merge tiny chunks\n",
    "    merged = []\n",
    "    for chunk in chunks:\n",
    "        if (merged and \n",
    "            not chunk['is_table'] and \n",
    "            not merged[-1]['is_table'] and\n",
    "            len(chunk['text']) < 120 and \n",
    "            len(merged[-1]['text']) + len(chunk['text']) < chunk_size * 1.5):\n",
    "            \n",
    "            prev = merged[-1]\n",
    "            prev['text'] += '\\n\\n' + chunk['text']\n",
    "            prev['offset']['end'] = chunk['offset']['end']\n",
    "            prev['size'] = len(prev['text'])\n",
    "            prev['boundary_type'] = 'merged'\n",
    "        else:\n",
    "            merged.append(chunk)\n",
    "    \n",
    "    return merged\n",
    "\n",
    "print(\"âœ… Structure-aware chunking ready\")\n",
    "print(\"   Features: header preservation, table integrity, section awareness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3fc173",
   "metadata": {},
   "source": [
    "#### 1.7.4 Test Chunking Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d19bff47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: Algorithmic Trading_16-09-2025.pdf\n",
      "Page count (meta): 4\n",
      "Detected structure counts: {'headers': 29, 'lists': 0, 'emphasis': 13, 'tables': 6}\n",
      "Detected tables (count): 6\n",
      "Generated chunks: 26\n"
     ]
    }
   ],
   "source": [
    "# Test Chunking with structure\n",
    "processor = EnhancedDocumentProcessor()\n",
    "\n",
    "pdf_path = Path(\"data/pra_rulebook/Algorithmic Trading_16-09-2025.pdf\")\n",
    "text, meta = processor.extract_text(pdf_path, use_layout=True, preserve_layout=False, use_cache=False)\n",
    "\n",
    "chunks = chunk_with_structure_and_metadata(\n",
    "    text,\n",
    "    meta,\n",
    "    chunk_size=600,\n",
    "    overlap=100,\n",
    "    chunk_id_prefix=pdf_path.stem\n",
    ")\n",
    "\n",
    "# Print summary\n",
    "print(f\"Document: {pdf_path.name}\")\n",
    "print(f\"Page count (meta): {meta.get('page_count')}\")\n",
    "print(f\"Detected structure counts: {meta.get('structure_counts')}\")\n",
    "print(f\"Detected tables (count): {len(meta.get('tables') or [])}\")\n",
    "print(f\"Generated chunks: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dce5e847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chunk 1: id=barclays_1Q20_earnings_transcript_0, size=175, is_table=False, header=None\n",
      "Excerpt: 1  ### Barclays PLC Q1 2020 Results  ### 29 April 2020  Results call Q&A transcript (amended in places to improve accuracy and readability)  ### Alvaro Serrano, Morgan Stanley\n",
      "\n",
      "Chunk 2: id=barclays_1Q20_earnings_transcript_5, size=574, is_table=False, header=Alvaro Serrano, Morgan Stanley\n",
      "Excerpt: Iâ€™ve got one on provisions and another one on capital. Obviously, the provision taken in the quarter is a very credible and sizeable one, but Iâ€™m just asking for some help [around] how we could think about â€“  in the next few quarters â€“  the total provisions we might see this year? [Could you] give u...\n",
      "\n",
      "Chunk 3: id=barclays_1Q20_earnings_transcript_6, size=360, is_table=False, header=Alvaro Serrano, Morgan Stanley\n",
      "Excerpt: unemployment? And also, on the oil price overlay in the provision, what oil price are you assuming? And the second question on capital. Obviously, you have said that youâ€™re going to dip below 13% in Q2. During the quarter, weâ€™ve had quite a lot of exceptions given by the PRA in Markets, e.g. RWAs on...\n",
      "\n",
      "Chunk 4: id=barclays_1Q20_earnings_transcript_7, size=549, is_table=False, header=Alvaro Serrano, Morgan Stanley\n",
      "Excerpt: ven by the PRA in Markets, e.g. RWAs on VAR breaches. Thereâ€™s also the IFRS 9 transitional benefit. Can you help us quantify what the benefit was? When I think about the RWAs going forward, is there a risk that some of this mitigation comes back? Is 13.5% still your target? Beyond this year, and in ...\n",
      "\n",
      "âœ… chunk_with_structure_and_metadata test passed\n"
     ]
    }
   ],
   "source": [
    "# Show first 4 chunk summaries\n",
    "for i, c in enumerate(chunks[:4]):\n",
    "    hdr = c.get('section_header') and c['section_header'].get('text') or None\n",
    "    print(f\"\\nChunk {i+1}: id={c['chunk_id']}, size={c['size']}, is_table={c.get('is_table')}, header={hdr}\")\n",
    "    print(\"Excerpt:\", c['text'][:300].replace(\"\\n\", \" \") + (\"...\" if len(c['text']) > 300 else \"\"))\n",
    "\n",
    "# Basic assertions\n",
    "assert len(chunks) > 0, \"No chunks produced\"\n",
    "# Offsets are non-decreasing\n",
    "for prev, cur in zip(chunks, chunks[1:]):\n",
    "    assert cur['offset']['start'] >= prev['offset']['start'], \"Chunk offsets not increasing\"\n",
    "\n",
    "# If tables were detected, assert at least one chunk is marked as table\n",
    "if meta.get('tables'):\n",
    "    assert any(c.get('is_table') for c in chunks), \"Tables detected in meta but no chunk marked as table\"\n",
    "\n",
    "print(\"\\nâœ… chunk_with_structure_and_metadata test passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0cefc69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: barclays_1Q20_earnings_transcript.pdf\n",
      "Page count: 14\n",
      "\n",
      "Structure counts: {'headers': 14, 'lists': 0, 'emphasis': 36, 'tables': 2}\n",
      "\n",
      "=== STRUCTURE ELEMENTS (50) ===\n",
      "By type:\n",
      "  header: 14\n",
      "  emphasis: 36\n",
      "\n",
      "=== FIRST 10 STRUCTURE ELEMENTS ===\n",
      "1. Type: header, Page: 1\n",
      "   Level: 3, Font size: 10.0, Bold: True\n",
      "   Text: Barclays PLC Q1 2020 Results\n",
      "2. Type: header, Page: 1\n",
      "   Level: 3, Font size: 10.0, Bold: True\n",
      "   Text: 29 April 2020\n",
      "3. Type: emphasis, Page: 1\n",
      "   Bold: True, Italic: False\n",
      "   Text: Results call Q&A transcript (amended in places to improve accuracy and readability)\n",
      "4. Type: header, Page: 1\n",
      "   Level: 3, Font size: 10.0, Bold: True\n",
      "   Text: Alvaro Serrano, Morgan Stanley\n",
      "5. Type: emphasis, Page: 1\n",
      "   Bold: True, Italic: False\n",
      "   Text: Iâ€™ve got one on provisions and another one on capital. Obviously, the provision taken in the quarter\n",
      "6. Type: emphasis, Page: 2\n",
      "   Bold: True, Italic: False\n",
      "   Text: In terms of metrics to look at, I think unemployment is probably the most important metric for us. A\n",
      "7. Type: emphasis, Page: 2\n",
      "   Bold: True, Italic: False\n",
      "   Text: Two questions as well, on provisions and then RWAs. On provisions, I want to understand this transit\n",
      "8. Type: emphasis, Page: 3\n",
      "   Bold: True, Italic: False\n",
      "   Text: For the stock of stage one and two provisions, transitional relief fell from 85% to 70%. And then as\n",
      "9. Type: emphasis, Page: 3\n",
      "   Bold: True, Italic: False\n",
      "   Text: Can I just come back on the first question, or the answer to it? Are you saying that even if there w\n",
      "10. Type: header, Page: 3\n",
      "   Level: 3, Font size: 10.0, Bold: True\n",
      "   Text: Joseph Dickerson, Jefferies\n",
      "\n",
      "=== TABLES (2) ===\n",
      "1. Type: table_region, Page: 1\n",
      "   Cells: 1\n",
      "2. Type: table_region, Page: 13\n",
      "   Cells: 1\n",
      "\n",
      "=== TEXT SAMPLE (first 2000 chars with structure markers) ===\n",
      "1\n",
      "\n",
      "### Barclays PLC Q1 2020 Results\n",
      "\n",
      "\n",
      "### 29 April 2020\n",
      "\n",
      "Results call Q&A transcript (amended in places to improve accuracy and readability)\n",
      "\n",
      "### Alvaro Serrano, Morgan Stanley\n",
      "\n",
      "Iâ€™ve got one on provisions and another one on capital. Obviously, the provision taken in the quarter is a\n",
      "very credible and sizeable one, but Iâ€™m just asking for some help [around] how we could think about â€“  in\n",
      "the next few quarters â€“  the total provisions we might see this year?\n",
      "[Could you] give us a bit of colour on how the payment holidays are evolving? Is this a metric we should\n",
      "look at as a leading indicator of what provisions might look like, or should we look at unemployment?\n",
      "And also, on the oil price overlay in the provision, what oil price are you assuming?\n",
      "And the second question on capital. Obviously, you have said that youâ€™re going to dip below 13% in Q2.\n",
      "During the quarter, weâ€™ve had quite a lot of exceptions given by the PRA in Markets, e.g. RWAs on VAR\n",
      "breaches. Thereâ€™s also the IFRS 9 transitional benefit. Can you help us quantify what the benefit was?\n",
      "When I think about the RWAs going forward, is there a risk that some of this mitigation comes back? Is\n",
      "13.5% still your target? Beyond this year, and in all the discussions you might have at the end of the year\n",
      "around capital distribution, would you want to get closer to 13.5% before you distribute capital?\n",
      "Tushar Morzaria, Group Finance Director\n",
      "On provisions first and then Iâ€™ll come on to capital. Obviously, itâ€™s a very uncertain area, itâ€™s a month or so\n",
      "into this crisis, so these are difficult paths to forecast. But if our assumptions are correct around the\n",
      "economy, as youâ€™ve seen on slide 15, [â€¦] then first of all I would start with our current pre-crisis existing\n",
      "run rate of impairments, and that was running at about Â£400m to Â£500m a quarter. Think of that as cash\n",
      "losses in any one quarter.\n",
      "Weâ€™ve built IFRS 9 provisions, a book up of Â£1.4bn, I think thatâ€™s probably a reasonable proxy of additional\n",
      "cash losses that we wou\n"
     ]
    }
   ],
   "source": [
    "# DIAGNOSTIC: Check what's actually being detected in meta\n",
    "processor = EnhancedDocumentProcessor()\n",
    "pdf_path = Path(\"data/earnings_transcripts/barclays_1Q20_earnings_transcript.pdf\")\n",
    "text, meta = processor.extract_text(pdf_path, use_layout=True, preserve_layout=False, use_cache=False)\n",
    "\n",
    "print(f\"Document: {pdf_path.name}\")\n",
    "print(f\"Page count: {meta.get('page_count')}\")\n",
    "print(f\"\\nStructure counts: {meta.get('structure_counts')}\")\n",
    "\n",
    "# Show structure breakdown\n",
    "structure = meta.get('structure', [])\n",
    "print(f\"\\n=== STRUCTURE ELEMENTS ({len(structure)}) ===\")\n",
    "print(f\"By type:\")\n",
    "from collections import Counter\n",
    "type_counts = Counter(s['type'] for s in structure)\n",
    "for typ, count in type_counts.items():\n",
    "    print(f\"  {typ}: {count}\")\n",
    "\n",
    "# Sample first 10 structure elements\n",
    "print(f\"\\n=== FIRST 10 STRUCTURE ELEMENTS ===\")\n",
    "for i, s in enumerate(structure[:10]):\n",
    "    print(f\"{i+1}. Type: {s['type']}, Page: {s.get('page')}\")\n",
    "    if s['type'] == 'header':\n",
    "        print(f\"   Level: {s.get('level')}, Font size: {s.get('font_size')}, Bold: {s.get('is_bold')}\")\n",
    "    if s['type'] == 'emphasis':\n",
    "        print(f\"   Bold: {s.get('bold')}, Italic: {s.get('italic')}\")\n",
    "    print(f\"   Text: {s['text'][:100]}\")\n",
    "\n",
    "# Show tables\n",
    "tables = meta.get('tables', [])\n",
    "print(f\"\\n=== TABLES ({len(tables)}) ===\")\n",
    "for i, t in enumerate(tables[:5]):\n",
    "    print(f\"{i+1}. Type: {t.get('type')}, Page: {t.get('page')}\")\n",
    "    if t.get('type') == 'table_region':\n",
    "        print(f\"   Cells: {len(t.get('cells', []))}\")\n",
    "    if t.get('type') == 'visual_element':\n",
    "        print(f\"   Bbox: {t.get('bbox')}\")\n",
    "        print(f\"   Width x Height: {t.get('width'):.1f} x {t.get('height'):.1f}\")\n",
    "\n",
    "# Show text sample with markers\n",
    "print(f\"\\n=== TEXT SAMPLE (first 2000 chars with structure markers) ===\")\n",
    "print(text[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b7adf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAGNOSTIC: Check PRA rulebook table detection\n",
    "processor = EnhancedDocumentProcessor()\n",
    "pdf_path = Path(\"data/pra_rulebook/Algorithmic Trading_16-09-2025.pdf\")\n",
    "text, meta = processor.extract_text(pdf_path, use_layout=True, preserve_layout=False, use_cache=False)\n",
    "\n",
    "print(f\"Document: {pdf_path.name}\")\n",
    "print(f\"Page count: {meta.get('page_count')}\")\n",
    "print(f\"\\nDetected tables: {len(meta.get('tables', []))}\")\n",
    "\n",
    "# Show ALL detected tables with details\n",
    "tables = meta.get('tables', [])\n",
    "for i, t in enumerate(tables, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TABLE {i}:\")\n",
    "    print(f\"  Type: {t.get('type')}\")\n",
    "    print(f\"  Page: {t.get('page')}\")\n",
    "    \n",
    "    if t.get('type') == 'table_region':\n",
    "        cells = t.get('cells', [])\n",
    "        print(f\"  Cells count: {len(cells)}\")\n",
    "        print(f\"  Cell samples:\")\n",
    "        for j, cell in enumerate(cells[:5], 1):\n",
    "            print(f\"    {j}. {cell[:60]}\")\n",
    "    \n",
    "    # Check for table_cell entries that got grouped\n",
    "    if t.get('bbox'):\n",
    "        bbox = t['bbox']\n",
    "        width = bbox[2] - bbox[0] if len(bbox) >= 3 else 0\n",
    "        height = bbox[3] - bbox[1] if len(bbox) >= 4 else 0\n",
    "        print(f\"  Bbox: {bbox}\")\n",
    "        print(f\"  Width x Height: {width:.1f} x {height:.1f}\")\n",
    "        \n",
    "        # Calculate aspect ratio\n",
    "        if height > 0:\n",
    "            aspect_ratio = width / height\n",
    "            print(f\"  Aspect ratio: {aspect_ratio:.2f}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"\\nSample text to see what's between the 'tables':\")\n",
    "print(text[1000:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ca4969",
   "metadata": {},
   "source": [
    "#### Fix Summary\n",
    "\n",
    "**Fixed Issues:**\n",
    "1. **Header Detection**: Now properly detects bold text as headers (level 3) instead of marking them as \"emphasis\"\n",
    "   - Added detection for bold + reasonable size (speaker names, section titles)\n",
    "   - Added pattern matching for \"FirstName LastName, Company\" format\n",
    "   - Headers < 80 chars with bold styling are now recognized\n",
    "\n",
    "2. **Table Detection**: Improved to avoid false positives\n",
    "   - Added page number exclusion (single digits are not table cells)\n",
    "   - Added minimum size threshold for visual elements (50x20 pixels)\n",
    "   - Only mark blocks as potential table cells if font size is below median\n",
    "\n",
    "3. **Emphasis vs Headers**: Fixed distinction\n",
    "   - Emphasis only applies to longer text (> 30 chars)\n",
    "   - Short bold text is now correctly classified as headers\n",
    "\n",
    "**Run the diagnostic cell below to verify the fix.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0134d9",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis functions\n",
    "\n",
    "Provide a small, robust wrapper around a Transformers sentiment pipeline (FinBERT) to produce per-chunk financial sentiment labels and confidence scores; plus a simple batch runner with progress reporting.\n",
    "\n",
    "- analyze_fin_sentiment(text: str, model: pipeline) -> Dict[str, float]\n",
    "  - Inputs: a text string and a Transformers pipeline instance (expected FinBERT-style pipeline returning label/score entries).\n",
    "  - Preprocessing: truncates input to the first 512 characters to respect model/token limits.\n",
    "  - Invocation: calls model(text) and expects results shaped as a list containing score dictionaries.\n",
    "  - Selection: chooses the label with the highest score (max by 'score').\n",
    "  - Output: returns {'sentiment': <label.lower()>, 'confidence': <rounded_score>} where confidence is rounded to 3 decimals.\n",
    "  - Error handling: on any exception returns a safe fallback {'sentiment': 'neutral', 'confidence': 0.0}.\n",
    "\n",
    "- batch_sentiment_analysis(chunks: List[str], model: pipeline) -> List[Dict[str, float]]\n",
    "  - Inputs: list of chunk texts and a pipeline model.\n",
    "  - Behavior: iterates over chunks, runs analyze_fin_sentiment for each, and collects results.\n",
    "  - Progress: displays progress with tqdm (\"Sentiment\").\n",
    "  - Output: list of per-chunk sentiment dicts in the same order as input.\n",
    "\n",
    "- Design notes / assumptions\n",
    "  - Built for FinBERT-style outputs where model(text) â†’ [[{label, score}, ...]]; the wrapper extracts the best-scoring label accordingly.\n",
    "  - Truncation to 512 chars prevents token-limit failures at the cost of possibly losing context on very long chunks.\n",
    "  - Conservative fallback ensures downstream pipeline robustness when the sentiment model fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "568d1978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_fin_sentiment(text: str, model: pipeline) -> Dict[str, float]:\n",
    "    \"\"\"Get financial sentiment with confidence scores\"\"\"\n",
    "    try:\n",
    "        text = text[:512]  # Truncate to model limit\n",
    "        results = model(text)\n",
    "        best_result = max(results[0], key=lambda x: x['score'])\n",
    "        return {\n",
    "            'sentiment': best_result['label'].lower(),\n",
    "            'confidence': round(best_result['score'], 3)\n",
    "        }\n",
    "    except Exception:\n",
    "        return {'sentiment': 'neutral', 'confidence': 0.0}\n",
    "\n",
    "def batch_sentiment_analysis(chunks: List[str], model: pipeline) -> List[Dict[str, float]]:\n",
    "    \"\"\"Analyze sentiment for multiple chunks\"\"\"\n",
    "    results = []\n",
    "    for chunk in tqdm(chunks, desc=\"Sentiment\"):\n",
    "        results.append(analyze_fin_sentiment(chunk, model))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cb9446",
   "metadata": {},
   "source": [
    "#### Embedding Generation Functions\n",
    "\n",
    "Utilities for producing vector embeddings from text chunks, handling batching, progress reporting, normalization and safe fallback to the embedding model loaded in Section 1.5.\n",
    "\n",
    "- EMBEDDING_MODEL\n",
    "  - List of candidate embedding model names used as configuration/choices; BGE v1.5 is marked as the selected model.\n",
    "\n",
    "- generate_embeddings(chunks: List[str], model: SentenceTransformer) -> np.ndarray\n",
    "  - Purpose: Encode a list of texts into embeddings using a SentenceTransformer instance.\n",
    "  - Output: numpy.ndarray of embeddings (empty array when input empty).\n",
    "  - Behavior: Calls model.encode(..., show_progress_bar=True, normalize_embeddings=True).\n",
    "  - Assumption: Caller supplies a valid SentenceTransformer (no model loading/fallback inside this function).\n",
    "\n",
    "- embed_texts(texts: List[str], model: SentenceTransformer = None, batch_size: int = 32, show_progress: bool = True, normalize: bool = True) -> List[List[float]]\n",
    "  - Purpose: Batch-safe embedding runner with optional progress display and normalization.\n",
    "  - Output: flat list of embedding vectors (each a Python list of floats).\n",
    "\n",
    "- embed_chunks(chunks: List[Dict], **kwargs) -> List[Dict]\n",
    "  - Purpose: High-level helper to embed a list of chunk dicts while preserving chunk metadata.\n",
    "  - Behavior: Extracts texts, calls embed_texts, zips embeddings back into chunk dict copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd720da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Embedding generation functions ready\n",
      "   Using model: BAAI/bge-large-en-v1.5\n",
      "   Model loaded in Section 1.5: True\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings\n",
    "def generate_embeddings(chunks: List[str], model: SentenceTransformer) -> np.ndarray:\n",
    "    \"\"\"Generate embeddings for text chunks\"\"\"\n",
    "    if not chunks:\n",
    "        return np.array([])\n",
    "    return model.encode(chunks, show_progress_bar=True, normalize_embeddings=True)\n",
    "\n",
    "# Embedding configuration\n",
    "EMBEDDING_MODEL = [\n",
    "    'all-MiniLM-L6-v2',\n",
    "    'thenlper/gte-large',\n",
    "    'sentence-transformers/all-mpnet-base-v2',\n",
    "    'BAAI/bge-large-en-v1.5',  # Selected model (index 3)\n",
    "]\n",
    "\n",
    "# Embed texts with progress and normalization\n",
    "def embed_texts(\n",
    "    texts: List[str], \n",
    "    model: SentenceTransformer = None,\n",
    "    batch_size: int = 32, \n",
    "    show_progress: bool = True, \n",
    "    normalize: bool = True\n",
    ") -> List[List[float]]:\n",
    "    \"\"\"Embed texts with progress and normalization.\"\"\"\n",
    "    if not texts: \n",
    "        return []\n",
    "    \n",
    "    # Use provided model or fall back to global embedding_model\n",
    "    if model is None:\n",
    "        if 'embedding_model' not in globals():\n",
    "            raise ValueError(\"No embedding model available. Please run Section 1.5 first.\")\n",
    "        model = globals()['embedding_model']\n",
    "    \n",
    "    vectors = []\n",
    "    iter_range = range(0, len(texts), batch_size)\n",
    "    \n",
    "    if show_progress:\n",
    "        iter_range = tqdm(iter_range, desc=\"Embedding\", unit=\"batch\")\n",
    "    \n",
    "    for i in iter_range:\n",
    "        batch = texts[i:i+batch_size]\n",
    "        emb = model.encode(batch, show_progress_bar=False, normalize_embeddings=normalize)\n",
    "        vectors.extend(emb.tolist())\n",
    "    \n",
    "    return vectors\n",
    "\n",
    "# Embed chunks with metadata preservation\n",
    "def embed_chunks(chunks: List[Dict], **kwargs) -> List[Dict]:\n",
    "    \"\"\"Embed chunks while preserving metadata.\"\"\"\n",
    "    texts = [chunk['text'] for chunk in chunks]\n",
    "    embeddings = embed_texts(texts, **kwargs)\n",
    "    return [{**chunk, 'embedding': emb} for chunk, emb in zip(chunks, embeddings)]\n",
    "\n",
    "print(\"âœ… Embedding generation functions ready\")\n",
    "print(f\"   Using model: {CONFIG['embedding_model']}\")\n",
    "print(f\"   Model loaded in Section 1.5: {'embedding_model' in globals()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2650b2e",
   "metadata": {},
   "source": [
    "#### ChromaDB setup with auto-repair\n",
    "\n",
    "Initialize a persistent ChromaDB client, ensure the store directory exists, handle common DB errors.\n",
    "\n",
    "- Key functions\n",
    "  - create_chroma_client(persist_dir: str) -> chromadb.Client\n",
    "    - Ensures the persistence directory exists.\n",
    "    - Attempts to instantiate a chromadb. PersistentClient and verify responsiveness via client.list_collections().\n",
    "    - On failure, logs the error, removes the existing store directory (shutil.rmtree), recreates it, re-instantiates the PersistentClient, and reports repair success. Returns the client instance.\n",
    "\n",
    "  - get_or_create_collection(client: chromadb.Client, name: str) -> chromadb.Collection\n",
    "    - Tries to retrieve an existing collection by name.\n",
    "    - If retrieval fails, creates a new collection with basic metadata (description) and returns it.\n",
    "    - Encapsulates collection creation/retrieval with simple error handling.\n",
    "\n",
    "  - display_database_status(client: chromadb.Client, store_dir: Path) -> None\n",
    "    - Prints a structured status report: ChromaDB version, database path, total collections.\n",
    "    - Iterates collections to query per-collection counts (collection.count()), aggregates total chunk/document counts, and prints per-collection diagnostics.\n",
    "    - Catches and reports exceptions per collection and for the overall status check.\n",
    "\n",
    "- Initialization flow (cell-level behavior)\n",
    "  - Calls create_chroma_client with STORE_DIR; if it succeeds, sets a global client.\n",
    "  - Optionally selects/creates a collection when CONFIG contains a collection_name, storing it in the global variable collection.\n",
    "  - Calls display_database_status to output diagnostics and guide next steps (e.g., run document processing if no collections).\n",
    "  - All major failure points print user-facing guidance (permissions, locks, readonly issues) and encourage re-running the repair path or restarting the kernel.\n",
    "\n",
    "- Error-handling & repair strategy\n",
    "  - Proactive: tests client responsiveness immediately after creation.\n",
    "  - Recoverable-failure approach: on error tries a filesystem-level repair (delete+recreate store directory) and reinstantiates the client.\n",
    "  - Informative: prints specific hints for common failure modes (readonly, permission, lock/busy).\n",
    "\n",
    "- Side effects & outputs\n",
    "  - Creates the persistence directory if missing.\n",
    "  - May remove and recreate the chroma store directory during \"repair\".\n",
    "  - Produces a human-readable database status report; returns client and collection objects for downstream cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d130b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ChromaDB...\n",
      "ChromaDB client ready\n",
      "ChromaDB Status Report\n",
      "==================================================\n",
      "Database location: /Users/rodrigograndy/Desktop/coding_projects/posts_website_drafts/rag_boe/chroma_store\n",
      "ChromaDB version: 1.0.21\n",
      "Total collections: 2\n",
      "\n",
      "Collection Details:\n",
      "   1. transcrips_barclasys: 5,591 chunks\n",
      "   2. pra_rules: 2,991 chunks\n",
      "\n",
      "Total documents: 8,582\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ChromaDB setup with auto-repair - Clean Implementation\n",
    "def create_chroma_client(persist_dir: str) -> chromadb.Client:\n",
    "    \"\"\"Create persistent ChromaDB client with auto-repair capability.\"\"\"\n",
    "    persist_path = Path(persist_dir)\n",
    "    persist_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        client = chromadb.PersistentClient(path=str(persist_path))\n",
    "        # Test client responsiveness\n",
    "        client.list_collections()\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        print(f\"ChromaDB issue detected: {e}\")\n",
    "        print(f\"Attempting database repair...\")\n",
    "        \n",
    "        # Simple repair: recreate directory\n",
    "        if persist_path.exists():\n",
    "            shutil.rmtree(persist_path, ignore_errors=True)\n",
    "        persist_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        client = chromadb.PersistentClient(path=str(persist_path))\n",
    "        print(f\"Database repaired successfully\")\n",
    "        return client\n",
    "\n",
    "def get_or_create_collection(client: chromadb.Client, name: str) -> chromadb.Collection:\n",
    "    \"\"\"Get existing collection or create new one.\"\"\"\n",
    "    try:\n",
    "        return client.get_collection(name)\n",
    "    except Exception:\n",
    "        return client.create_collection(\n",
    "            name=name,\n",
    "            metadata={\"description\": \"Financial documents collection\"}\n",
    "        )\n",
    "\n",
    "def display_database_status(client: chromadb.Client, store_dir: Path) -> None:\n",
    "    \"\"\"Display comprehensive database status information.\"\"\"\n",
    "    print(f\"ChromaDB Status Report\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    try:\n",
    "        # Get collections\n",
    "        collections = client.list_collections()\n",
    "        print(f\"Database location: {store_dir}\")\n",
    "        print(f\"ChromaDB version: {chromadb.__version__}\")\n",
    "        print(f\"Total collections: {len(collections)}\")\n",
    "        \n",
    "        if collections:\n",
    "            print(f\"\\nCollection Details:\")\n",
    "            total_documents = 0\n",
    "            \n",
    "            for i, col in enumerate(collections, 1):\n",
    "                try:\n",
    "                    collection_obj = client.get_collection(col.name)\n",
    "                    count = collection_obj.count()\n",
    "                    total_documents += count\n",
    "                    print(f\"   {i}. {col.name}: {count:,} chunks\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   {i}. {col.name}: Error ({str(e)[:30]}...)\")\n",
    "            \n",
    "            print(f\"\\nTotal documents: {total_documents:,}\")\n",
    "        else:\n",
    "            print(f\"\\nNo collections found\")\n",
    "            print(f\"Run document processing (Sections 3-4) to populate database\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Status check failed: {e}\")\n",
    "        \n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "# Initialize ChromaDB client\n",
    "print(\"Initializing ChromaDB...\")\n",
    "try:\n",
    "    client = create_chroma_client(str(STORE_DIR))\n",
    "    print(f\"ChromaDB client ready\")\n",
    "    \n",
    "    # Initialize collection variable for global use\n",
    "    collection = None\n",
    "    if CONFIG.get('collection_name'):\n",
    "        try:\n",
    "            collection = get_or_create_collection(client, CONFIG['collection_name'])\n",
    "            print(f\"Collection '{CONFIG['collection_name']}' ready\")\n",
    "        except Exception as e:\n",
    "            print(f\"Collection setup warning: {e}\")\n",
    "    \n",
    "    # Display comprehensive status\n",
    "    display_database_status(client, STORE_DIR)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ChromaDB initialization failed: {e}\")\n",
    "    print(f\"Check your environment and permissions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f9b680",
   "metadata": {},
   "source": [
    "#### Collection management functions\n",
    "\n",
    "Small utility layer that wraps common ChromaDB collection operations with defensive checks, friendly messages and simple error handling. Exposes four primary helpers for listing, creating, inspecting and selecting collections.\n",
    "\n",
    "Key functions\n",
    "\n",
    "- list_existing_collections(client) -> List[str]\n",
    "  - Calls client.list_collections(), extracts collection.name; on exception prints an error and returns an empty list.\n",
    "\n",
    "- create_new_collection(client, collection_name, description: str = None) -> chromadb.Collection | None\n",
    "  - Purpose: Create (or retrieve) a named collection and attach simple metadata.\n",
    "  - Behavior:\n",
    "    - First checks existing collections (via list_existing_collections) and returns the existing collection if present.\n",
    "    - Otherwise calls client.create_collection(name=..., metadata={...}).\n",
    "    - Contains targeted error handling: inspects exception text and prints actionable messages for common states (read-only store, permission issues, DB locks/busy); returns None on failure.\n",
    "\n",
    "- get_collection_info(collection: chromadb.Collection) -> Dict[str, Any]\n",
    "  - Purpose: Fetch lightweight diagnostics for a collection.\n",
    "  - Behavior: Calls collection.get() and returns a dict with name, metadata, document_count (len(ids) if present) and sample_ids (first 5). On error returns a dict with the collection name and an error string.\n",
    "\n",
    "- select_or_create_collection(client, collection_name: str = None) -> chromadb.Collection\n",
    "  - Purpose: Convenience chooser that uses a supplied name or CONFIG['collection_name'] to pick or create a collection.\n",
    "  - Behavior: Lists existing collections, returns existing collection if present, otherwise calls create_new_collection to create and return a new collection (prints status messages).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d4aa1a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 943,
     "status": "ok",
     "timestamp": 1759681021041,
     "user": {
      "displayName": "Nick Bradshaw",
      "userId": "04897433706475065009"
     },
     "user_tz": -60
    },
    "id": "2d4aa1a9",
    "outputId": "e9803f9b-bb1f-4c0d-c021-15aa114008d1"
   },
   "outputs": [],
   "source": [
    "# Collection management functions with enhanced error handling\n",
    "def list_existing_collections(client: chromadb.Client) -> List[str]:\n",
    "    \"\"\"List all existing collections in the ChromaDB instance\"\"\"\n",
    "    try:\n",
    "        collections = client.list_collections()\n",
    "        return [collection.name for collection in collections]\n",
    "    except Exception as e:\n",
    "        print(f\"Error listing collections: {e}\")\n",
    "        return []\n",
    "# Create new collection with error handling\n",
    "def create_new_collection(client: chromadb.Client, collection_name: str, description: str = None) -> chromadb.Collection:\n",
    "    \"\"\"Create a new collection with the specified name, with enhanced error handling\"\"\"\n",
    "    try:\n",
    "        # Check if collection already exists\n",
    "        existing_collections = list_existing_collections(client)\n",
    "        if collection_name in existing_collections:\n",
    "            print(f\"Collection '{collection_name}' already exists. Retrieving existing collection.\")\n",
    "            return client.get_collection(collection_name)\n",
    "\n",
    "        # Create new collection\n",
    "        metadata = {\"description\": description or f\"Financial documents collection: {collection_name}\"}\n",
    "        collection = client.create_collection(name=collection_name, metadata=metadata)\n",
    "        print(f\"Created new collection: '{collection_name}'\")\n",
    "        return collection\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = str(e).lower()\n",
    "\n",
    "        # Handle specific database errors\n",
    "        if \"readonly\" in error_msg or \"1032\" in error_msg:\n",
    "            print(f\"Database is read-only. Attempting to repair...\")\n",
    "            print(\"Run the database repair cell above to fix this issue.\")\n",
    "            print(\"Alternative: Restart the notebook kernel and run the repair cell.\")\n",
    "\n",
    "        elif \"permission\" in error_msg or \"access\" in error_msg:\n",
    "            print(f\"Permission denied. Database directory may need permission fixes.\")\n",
    "            print(\"Run the database repair cell above to fix permissions.\")\n",
    "\n",
    "        elif \"lock\" in error_msg or \"busy\" in error_msg:\n",
    "            print(f\"Database is locked (another process may be using it).\")\n",
    "            print(\"Close other notebook instances and try again.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Unexpected error creating collection '{collection_name}': {e}\")\n",
    "            print(\"Try running the database repair cell above.\")\n",
    "\n",
    "        return None\n",
    "# Get collection info with document count\n",
    "def get_collection_info(collection: chromadb.Collection) -> Dict[str, Any]:\n",
    "    \"\"\"Get information about a collection including document count\"\"\"\n",
    "    try:\n",
    "        data = collection.get()\n",
    "        return {\n",
    "            'name': collection.name,\n",
    "            'metadata': collection.metadata,\n",
    "            'document_count': len(data['ids']) if data['ids'] else 0,\n",
    "            'sample_ids': data['ids'][:5] if data['ids'] else []\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {'name': collection.name, 'error': str(e)}\n",
    "\n",
    "def select_or_create_collection(client: chromadb.Client, collection_name: str = None) -> chromadb.Collection:\n",
    "    \"\"\"Select existing collection or create new one based on name\"\"\"\n",
    "    if not collection_name:\n",
    "        collection_name = CONFIG['collection_name']\n",
    "\n",
    "    existing_collections = list_existing_collections(client)\n",
    "\n",
    "    if collection_name in existing_collections:\n",
    "        print(f\"Using existing collection: '{collection_name}'\")\n",
    "        return client.get_collection(collection_name)\n",
    "    else:\n",
    "        print(f\"Collection '{collection_name}' not found. Creating new collection.\")\n",
    "        return create_new_collection(client, collection_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf4cf49",
   "metadata": {},
   "source": [
    "#### 1.8 Rag workflow variables.\n",
    "\n",
    "Initialize variables that would normally be created in Section 3 (document processing)\n",
    "\n",
    "- Small compatibility/bootstrapping cell that ensures later sections (5â€“7) can run even if document-processing (Sections 3â€“4) was skipped. It defines a set of global placeholders, preserves any existing values, and emits brief runtime hints.\n",
    "\n",
    "- all_chunks -> dict  \n",
    "  - Document-indexed structure mapping each file path to its extracted text, chunk list and merged metadata. Populated in Section 3 but initialized here as an empty dict to avoid NameError in downstream code.\n",
    "\n",
    "- all_embeddings -> list  \n",
    "  - Linear list/array of embeddings for all chunks. Downstream code expects embeddings to be present; initialized empty.\n",
    "\n",
    "- all_sentiments -> list  \n",
    "  - Per-chunk sentiment result list (FinBERT outputs). Initialized empty so sentiment-dependent steps can still run defensively.\n",
    "\n",
    "- results -> list  \n",
    "  - General-purpose list used earlier for (path, text, metadata) tuples from extraction. Initialized empty for compatibility.\n",
    "\n",
    "- all_chunk_texts -> list  \n",
    "  - Flat list of chunk texts (used by embedding/sentiment batch runners and Section 4.6). Added explicitly for Section 4.6 compatibility.\n",
    "\n",
    "- bm25_index -> None or index object  \n",
    "  - Placeholder for an in-memory BM25 index; set to None when not built.\n",
    "\n",
    "Behavior and design notes\n",
    "- Guarded initialization: each variable is only created if absent in globals() so re-running cells or preserving already-processed data is safe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1637367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized all_chunks (empty - populate via Section 3 if needed)\n",
      "Initialized all_embeddings (empty - populate via Section 3 if needed)\n",
      "Initialized all_sentiments (empty - populate via Section 3 if needed)\n",
      "Initialized results (empty - populate via Section 3 if needed)\n",
      "Initialized all_chunk_texts (empty - populate via Section 3 if needed)\n",
      "Initialized bm25_index (None - will be built in Section 5)\n",
      "All core functions defined and available\n",
      "Document discovery and metadata extraction\n",
      "Enhanced document processing (PDF, DOCX, TXT, MD, CSV)\n",
      "Text chunking with boundary respect\n",
      "Financial sentiment analysis\n",
      "Embedding generation with BGE model\n",
      "ChromaDB storage with metadata sanitization\n",
      "Collection management utilities\n",
      "Ready for use across all workflow scenarios!\n",
      "Section 5-7 compatibility variables initialized\n",
      "Added all_chunk_texts for section 4.6 compatibility\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty variables for Section 5-7 compatibility\n",
    "if 'all_chunks' not in globals():\n",
    "    all_chunks = {}  # Empty dict - would be populated by Section 3\n",
    "    print(\"Initialized all_chunks (empty - populate via Section 3 if needed)\")\n",
    "\n",
    "if 'all_embeddings' not in globals():\n",
    "    all_embeddings = []  # Empty list - would be populated by Section 3\n",
    "    print(\"Initialized all_embeddings (empty - populate via Section 3 if needed)\")\n",
    "if 'all_sentiments' not in globals():\n",
    "    all_sentiments = []  # Empty list - would be populated by Section 3\n",
    "    print(\"Initialized all_sentiments (empty - populate via Section 3 if needed)\")\n",
    "\n",
    "if 'results' not in globals():\n",
    "    results = []  # Empty list - would be populated by Section 3\n",
    "    print(\"Initialized results (empty - populate via Section 3 if needed)\")\n",
    "if 'all_chunk_texts' not in globals():\n",
    "    all_chunk_texts = []  # Empty list - would be populated by Section 3.10.3\n",
    "    print(\"Initialized all_chunk_texts (empty - populate via Section 3 if needed)\")\n",
    "\n",
    "if 'bm25_index' not in globals():\n",
    "    bm25_index = None  # Would be built in Section 5\n",
    "    print(\"Initialized bm25_index (None - will be built in Section 5)\")\n",
    "print(\"All core functions defined and available\")\n",
    "print(\"Document discovery and metadata extraction\")\n",
    "print(\"Enhanced document processing (PDF, DOCX, TXT, MD, CSV)\")\n",
    "print(\"Text chunking with boundary respect\")\n",
    "print(\"Financial sentiment analysis\")\n",
    "print(\"Embedding generation with BGE model\")\n",
    "print(\"ChromaDB storage with metadata sanitization\")\n",
    "print(\"Collection management utilities\")\n",
    "print(\"Ready for use across all workflow scenarios!\")\n",
    "print(\"Section 5-7 compatibility variables initialized\")\n",
    "print(\"Added all_chunk_texts for section 4.6 compatibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c135dd",
   "metadata": {
    "id": "f9c135dd"
   },
   "source": [
    "## 2. Document Processing\n",
    "\n",
    "This section includes functions for loading files, extracting and parsing text, chunking, doing sentiment analysis on chunks, creating embeddings, and storing them in ChromaDB.\n",
    "\n",
    "- Designed to handle text extraction from different file types (PDF, DOCX, TXT, .MD, .CSV)\n",
    "- Currently, it has been tested with PDF files only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347ce131",
   "metadata": {},
   "source": [
    "#### 2.2 Directory Verification\n",
    "\n",
    "Quick diagnostics to verify the presence and contents of the configured data directories before running document processing.\n",
    "\n",
    "- Inputs: uses globals established in Section 1.4 â€” DATA_DIR (root data folder) and file_directories (list of subdirectory Path objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "052fb7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directories:\n",
      "pra_rulebook: 41 PDF files\n"
     ]
    }
   ],
   "source": [
    "print(\"Data directories:\")\n",
    "for dir_path in file_directories:\n",
    "    if dir_path.exists():\n",
    "        files = [f for f in dir_path.glob('*.pdf')]  # Adjust extension as needed\n",
    "        print(f\"{dir_path.name}: {len(files)} PDF files\")\n",
    "    else:\n",
    "        print(f\"{dir_path.name}: Not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7156c2a5",
   "metadata": {},
   "source": [
    "#### 2.3 Extract Metadata from file names and content\n",
    "\n",
    "The class provides a single, consolidated extractor that pulls structured metadata from a file path (filename) and optionally from the document text, with domain-focused cues for financial documents and conflict resolution between filename and content-derived values.\n",
    "\n",
    "- Key methods\n",
    "  - __init__\n",
    "    - Loads spaCy (with a fallback downloader) into self.nlp when available.\n",
    "    - Initializes domain vocabularies: financial_terms and document_type_patterns used by content classifiers.\n",
    "\n",
    "  - _load_nlp_model\n",
    "    - Loads en_core_web_sm from spaCy package.\n",
    "\n",
    "  - extract_comprehensive_metadata(filepath: Path, text: str | None) -> Dict\n",
    "    - Top-level method: always extracts filename metadata, then if text is provided runs content extraction, merges content fields into the filename-derived metadata and applies conflict resolution.\n",
    "    - Returns a unified metadata dict suitable for merging into pipeline records.\n",
    "\n",
    "  - _extract_filename_metadata(filepath: Path) -> Dict\n",
    "    - Parses filename.stem to infer company (first underscore part), year (4- or 2-digit normalization), quarter (Q1â€“Q4 patterns) and a doc_type heuristic based on extension and name tokens.\n",
    "    - Returns filename, file_extension, company, year, quarter, doc_type and extraction_source='filename'.\n",
    "\n",
    "  - _classify_doc_type_from_filename(filepath, name) -> str\n",
    "    - Heuristic type classifier by extension and keywords in filename (e.g., 'transcript', 'meeting').\n",
    "\n",
    "  - _extract_content_metadata(text: str) -> Dict\n",
    "    - Extracts dates/years and quarter mentions from text.\n",
    "    - Runs document-type scoring via _classify_doc_type_from_content.\n",
    "    - Invokes spaCy entity extraction (_extract_entities_spacy) when available.\n",
    "    - Detects financial terms via _extract_financial_terms.\n",
    "    - Computes simple content stats (word_count, has_financial_data).\n",
    "    - Returns content-derived fields prefixed (e.g., primary_year_content, years_content, quarters_content, doc_type_content, extracted_entities, financial_terms, word_count).\n",
    "\n",
    "  - _classify_doc_type_from_content(text: str) -> Optional[str]\n",
    "    - Scores document_type_patterns against text (counts pattern matches) and returns the highest-scoring doc_type when present.\n",
    "\n",
    "  - _extract_entities_spacy(text: str) -> Dict\n",
    "    - Uses spaCy to extract limited sets of ORG (companies), PERSON, MONEY and PERCENT entities from a text slice (performance-limited to ~5k chars), deduplicates and caps results, returns under key 'extracted_entities'.\n",
    "\n",
    "  - _extract_financial_terms(text: str) -> Dict\n",
    "    - Simple keyword presence detection using the configured financial_terms categories; returns categorized matches as 'financial_terms'.\n",
    "\n",
    "  - _resolve_conflicts(metadata: Dict) -> Dict\n",
    "    - Merges filename and content outputs with deterministic precedence: prefers content-derived doc_type/year/quarter when present, annotates sources (e.g., doc_type_source, year_source, quarter_source).\n",
    "    - Produces a resolved metadata dict ready for downstream storage.\n",
    "\n",
    "- Outputs / fields to expect\n",
    "  - Combined metadata includes filename, file_extension, company, year, quarter, doc_type plus content-derived fields when text provided: primary_year_content, years_content, quarters_content, doc_type_content, extracted_entities, financial_terms, word_count, has_financial_data, and explicit *_source fields after conflict resolution.\n",
    "\n",
    "- Integration note\n",
    "  - The notebook initializes consolidated_metadata_extractor = ConsolidatedMetadataExtractor() and Section 2.7 calls extract_comprehensive_metadata(path, text) to produce the merged metadata used downstream (chunking, storage).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d0feef0",
   "metadata": {
    "id": "0d0feef0",
    "outputId": "dd679f9e-4232-43c1-984b-fac4aca0fdc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidated Metadata Extractor initialized\n",
      "â€¢ Filename parsing (company, year, quarter, doc_type)\n",
      "â€¢ Content analysis (dates, entities, financial terms)\n",
      "â€¢ spaCy NLP entity extraction\n",
      "â€¢ Conflict resolution (content takes precedence)\n",
      "â€¢ Financial domain optimization\n"
     ]
    }
   ],
   "source": [
    "# Extract Metadata from file names AND content\n",
    "class ConsolidatedMetadataExtractor:\n",
    "    \"\"\"Extract metadata from filenames and document content with financial domain focus\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nlp = self._load_nlp_model()\n",
    "        \n",
    "        # Financial terms for content analysis\n",
    "        self.financial_terms = {\n",
    "            'metrics': ['revenue', 'profit', 'earnings', 'ebitda', 'cash flow', 'margin'],\n",
    "            'instruments': ['bonds', 'stocks', 'derivatives', 'securities'],\n",
    "            'regulations': ['compliance', 'regulatory', 'pra', 'basel']\n",
    "        }\n",
    "        \n",
    "        # Document type patterns\n",
    "        self.document_type_patterns = {\n",
    "            'earnings_transcript': ['earnings call', 'quarterly results', 'q1', 'q2', 'q3', 'q4'],\n",
    "            'annual_report': ['annual report', 'form 10-k', '10-k'],\n",
    "            'regulatory_filing': ['form 8-k', '8-k', 'sec filing']\n",
    "        }\n",
    "    \n",
    "    def _load_nlp_model(self):\n",
    "        \"\"\"Load spaCy model from system installation.\"\"\"\n",
    "        try:\n",
    "            return spacy.load(\"en_core_web_sm\")\n",
    "        except OSError:\n",
    "            raise FileNotFoundError(\n",
    "                f\"spaCy model 'en_core_web_sm' not found. \"\n",
    "                f\"Please install it by running: python -m spacy download en_core_web_sm\"\n",
    "            )\n",
    "    \n",
    "    def extract_comprehensive_metadata(self, filepath: Path, text: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Extract metadata from both filename and content\n",
    "        \"\"\"\n",
    "        # Start with filename metadata\n",
    "        metadata = self._extract_filename_metadata(filepath)\n",
    "        \n",
    "        # Add content metadata if text provided\n",
    "        if text:\n",
    "            content_meta = self._extract_content_metadata(text)\n",
    "            metadata.update(content_meta)\n",
    "            metadata = self._resolve_conflicts(metadata)\n",
    "        \n",
    "        return metadata\n",
    "    \n",
    "    def _extract_filename_metadata(self, filepath: Path) -> Dict[str, Any]:\n",
    "        \"\"\"Extract metadata from filename\"\"\"\n",
    "        name = filepath.stem.lower()\n",
    "        parts = name.split('_')\n",
    "        \n",
    "        metadata = {\n",
    "            'filename': filepath.name,\n",
    "            'file_extension': filepath.suffix.lower(),\n",
    "            'extraction_source': 'filename'\n",
    "        }\n",
    "        \n",
    "        # Extract company (first part)\n",
    "        metadata['company'] = parts[0] if parts else 'unknown'\n",
    "        \n",
    "        # Extract year (4-digit or 2-digit)\n",
    "        year_match = re.search(r'(20\\d{2})|(\\d{2})', name)\n",
    "        if year_match:\n",
    "            year = year_match.group(0)\n",
    "            if len(year) == 2:\n",
    "                year_int = int(year)\n",
    "                year = str(2000 + year_int if year_int <= 50 else 1900 + year_int)\n",
    "            metadata['year'] = year\n",
    "        else:\n",
    "            metadata['year'] = 'unknown'\n",
    "        \n",
    "        # Extract quarter\n",
    "        quarter_match = re.search(r'([1-4]q|q[1-4])', name, re.IGNORECASE)\n",
    "        if quarter_match:\n",
    "            q = quarter_match.group(1).upper()\n",
    "            metadata['quarter'] = f\"Q{q[0]}\" if q[0].isdigit() else q\n",
    "        else:\n",
    "            metadata['quarter'] = 'unknown'\n",
    "        \n",
    "        # Classify document type\n",
    "        metadata['doc_type'] = self._classify_doc_type_from_filename(filepath, name)\n",
    "        \n",
    "        return metadata\n",
    "    \n",
    "    def _classify_doc_type_from_filename(self, filepath: Path, name: str) -> str:\n",
    "        \"\"\"Classify document type from filename\"\"\"\n",
    "        if filepath.suffix.lower() == '.pdf':\n",
    "            if 'transcript' in name:\n",
    "                return 'earnings_transcript'\n",
    "            elif 'meeting' in name:\n",
    "                return 'analyst_meeting'\n",
    "            return 'financial_document'\n",
    "        elif filepath.suffix.lower() in ['.docx', '.txt', '.md']:\n",
    "            return 'text_document'\n",
    "        return 'unknown'\n",
    "    \n",
    "    def _extract_content_metadata(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract metadata from document content\"\"\"\n",
    "        content_meta = {'extraction_source': 'content'}\n",
    "        \n",
    "        # Extract dates from content\n",
    "        years = re.findall(r'\\b(20\\d{2})\\b', text)\n",
    "        if years:\n",
    "            content_meta['years_content'] = sorted(set(years), reverse=True)[:5]\n",
    "            content_meta['primary_year_content'] = years[0]\n",
    "        \n",
    "        # Extract quarters\n",
    "        quarters = re.findall(r'\\b([Q1-4]|[1-4]Q)\\b', text, re.IGNORECASE)\n",
    "        if quarters:\n",
    "            content_meta['quarters_content'] = list(set(q.upper() for q in quarters))\n",
    "        \n",
    "        # Document type from content\n",
    "        doc_type = self._classify_doc_type_from_content(text)\n",
    "        if doc_type:\n",
    "            content_meta['doc_type_content'] = doc_type\n",
    "        \n",
    "        # Entity extraction with spaCy\n",
    "        if self.nlp:\n",
    "            entities = self._extract_entities_spacy(text)\n",
    "            content_meta.update(entities)\n",
    "        \n",
    "        # Financial terms\n",
    "        financial_info = self._extract_financial_terms(text)\n",
    "        content_meta.update(financial_info)\n",
    "        \n",
    "        # Content statistics\n",
    "        content_meta['word_count'] = len(text.split())\n",
    "        content_meta['has_financial_data'] = bool(financial_info.get('financial_terms'))\n",
    "        \n",
    "        return content_meta\n",
    "    \n",
    "    def _classify_doc_type_from_content(self, text: str) -> Optional[str]:\n",
    "        \"\"\"Classify document type from content\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        type_scores = {}\n",
    "        for doc_type, patterns in self.document_type_patterns.items():\n",
    "            score = sum(len(re.findall(re.escape(p.lower()), text_lower)) for p in patterns)\n",
    "            if score > 0:\n",
    "                type_scores[doc_type] = score\n",
    "        \n",
    "        return max(type_scores, key=type_scores.get) if type_scores else None\n",
    "    \n",
    "    def _extract_entities_spacy(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract entities using spaCy\"\"\"\n",
    "        entities = {\n",
    "            'companies': [],\n",
    "            'people': [],\n",
    "            'money': [],\n",
    "            'percentages': []\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            doc = self.nlp(text[:5000])  # Limit for performance\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == 'ORG':\n",
    "                    entities['companies'].append(ent.text)\n",
    "                elif ent.label_ == 'PERSON':\n",
    "                    entities['people'].append(ent.text)\n",
    "                elif ent.label_ == 'MONEY':\n",
    "                    entities['money'].append(ent.text)\n",
    "                elif ent.label_ == 'PERCENT':\n",
    "                    entities['percentages'].append(ent.text)\n",
    "            \n",
    "            # Deduplicate and limit\n",
    "            for key in entities:\n",
    "                entities[key] = list(set(entities[key]))[:5]\n",
    "            \n",
    "            return {'extracted_entities': entities}\n",
    "        except Exception as e:\n",
    "            return {'extracted_entities': entities}\n",
    "    \n",
    "    def _extract_financial_terms(self, text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract financial terms from content\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        found_terms = {}\n",
    "        for category, terms in self.financial_terms.items():\n",
    "            found = [term for term in terms if term in text_lower]\n",
    "            if found:\n",
    "                found_terms[category] = found\n",
    "        \n",
    "        return {'financial_terms': found_terms} if found_terms else {}\n",
    "    \n",
    "    def _resolve_conflicts(self, metadata: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Resolve conflicts between filename and content metadata\"\"\"\n",
    "        resolved = metadata.copy()\n",
    "        \n",
    "        # Prefer content for doc_type if confidence is high\n",
    "        doc_type_content = metadata.get('doc_type_content')\n",
    "        doc_type_filename = metadata.get('doc_type')\n",
    "        if doc_type_content:\n",
    "            resolved['doc_type'] = doc_type_content\n",
    "            resolved['doc_type_source'] = 'content'\n",
    "        else:\n",
    "            resolved['doc_type_source'] = 'filename'\n",
    "        \n",
    "        # Prefer content for year if available\n",
    "        year_content = metadata.get('primary_year_content')\n",
    "        if year_content:\n",
    "            resolved['year'] = year_content\n",
    "            resolved['year_source'] = 'content'\n",
    "        else:\n",
    "            resolved['year_source'] = 'filename'\n",
    "        \n",
    "        # Prefer content for quarter if available\n",
    "        quarters_content = metadata.get('quarters_content')\n",
    "        if quarters_content:\n",
    "            resolved['quarter'] = quarters_content[0]\n",
    "            resolved['quarter_source'] = 'content'\n",
    "        else:\n",
    "            resolved['quarter_source'] = 'filename'\n",
    "        \n",
    "        return resolved\n",
    "\n",
    "# Initialize\n",
    "try:\n",
    "    consolidated_metadata_extractor = ConsolidatedMetadataExtractor()\n",
    "    print(\"Consolidated Metadata Extractor initialized\")\n",
    "    print(\"â€¢ Filename parsing (company, year, quarter, doc_type)\")\n",
    "    print(\"â€¢ Content analysis (dates, entities, financial terms)\")\n",
    "    print(\"â€¢ spaCy NLP entity extraction\")\n",
    "    print(\"â€¢ Conflict resolution (content takes precedence)\")\n",
    "    print(\"â€¢ Financial domain optimization\")\n",
    "except Exception as e:\n",
    "    print(f\"Initialization failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7df06c",
   "metadata": {
    "id": "0f7df06c"
   },
   "source": [
    "#### 2.4 Document Text Extraction and Parsing Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e78f6602",
   "metadata": {
    "id": "e78f6602"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Document processor ready\n",
      "   Supported formats: ['.pdf', '.docx', '.txt', '.md', '.csv']\n",
      "   â€¢ PDF extraction (simple and layout modes)\n",
      "   â€¢ DOCX, TXT, MD, CSV support\n",
      "   â€¢ Automatic text cleaning\n"
     ]
    }
   ],
   "source": [
    "# Verify the processor is available\n",
    "if 'EnhancedDocumentProcessor' not in dir():\n",
    "    print(\"âŒ EnhancedDocumentProcessor not found\")\n",
    "    print(\"   Please run Section 1.7.4 first\")\n",
    "else:\n",
    "    print(\"âœ… Document processor ready\")\n",
    "    print(f\"   Supported formats: {EnhancedDocumentProcessor().supported_formats}\")\n",
    "    print(\"   â€¢ PDF extraction (simple and layout modes)\")\n",
    "    print(\"   â€¢ DOCX, TXT, MD, CSV support\")\n",
    "    print(\"   â€¢ Automatic text cleaning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31287738",
   "metadata": {},
   "source": [
    "#### 2.5 Run Data Processor\n",
    "\n",
    "Orchestrates document discovery and batch extraction, producing the notebook-wide results list used by downstream steps (chunking, metadata merging, embeddings, storage). Class is defined in section 1.7.4.\n",
    "- pdf_mode behavior: accepts \"auto\" | \"layout\" | \"simple\". In \"auto\" a heuristic (e.g., page_count > 3) picks layout extraction (detailed spans) vs simple text extraction; exceptions fall back to simpler mode and extraction metadata records extraction_mode and optional layout spans.\n",
    "- extract_text outputs: cleaned text (raises on missing/empty), and metadata dict describing extraction_mode and, for layout PDF, a layout list of span dicts.\n",
    "- process_files outputs: for each file a tuple (Path, text or None, metadata or {\"error\": \"...\"}); it captures per-file exceptions rather than aborting the full run and prints a simple progress summary.\n",
    "- Integration note: this cell sets the global results variable expected by sections 2.6â€“2.10. Downstream logic assumes results contains (path, text, extraction_meta) tuples for metadata merging, chunking, sentiment and embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e28a2dcf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "7daad003234b474daf0d91a4fcc15bb4",
      "6a04291428514dbdb0498885efaa01af",
      "febc47fe39844623a20ac1dc515ece75",
      "8f606ffbd4aa4fffbc20e1ac6d9befa0",
      "21c38215f12849bfb1e25f78bfdcb140",
      "2ddd12074a2048d2979f5fd980594c63",
      "870222a9c6664250ba5b289ae3f53a92",
      "75fdcd047c1f4cbe8fa1e09ff686f1e9",
      "d0ad5334aef748c0b138c6e97554566d",
      "398a0b724d6c40cdb18dbda2554472d6",
      "d554f4d63f3b498e8510afc02dd3ca38",
      "49cf1e972fae42c58b2465f75226b37b"
     ]
    },
    "id": "e28a2dcf",
    "outputId": "4487949a-c207-4c1f-8134-750ed10934c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supported file extensions: ['.pdf', '.docx', '.txt', '.md', '.csv']\n",
      "Data directories provided: [PosixPath('/Users/rodrigograndy/Desktop/coding_projects/posts_website_drafts/rag_boe/data/pra_rulebook')]\n",
      "Searching directory: /Users/rodrigograndy/Desktop/coding_projects/posts_website_drafts/rag_boe/data/pra_rulebook\n",
      "  Contents of /Users/rodrigograndy/Desktop/coding_projects/posts_website_drafts/rag_boe/data/pra_rulebook: ['Market Risk_16-09-2025.pdf', 'Own Funds and Eligible Liabilities (CRR)_16-09-2025.pdf', 'Benchmarking of Internal Approaches_16-09-2025.pdf', 'Public Disclosure_16-09-2025.pdf', 'Credit Valuation Adjustment Risk (CRR)_16-09-2025.pdf', 'Trading Book (CRR)_16-09-2025.pdf', 'Supervised Run-Off_16-09-2025.pdf', 'Reporting Pillar 2_16-09-2025.pdf', 'Large Exposures_16-09-2025.pdf', 'Resolution Assessment_16-09-2025.pdf', 'General Organisational Requirements_16-09-2025.pdf', 'Liquidity Coverage Requirement - UK Designated Investment Firms_16-09-2025.pdf', 'Designation_16-09-2025.pdf', 'Liquidity (CRR)_16-09-2025.pdf', 'Counterparty Credit Risk (CRR)_16-09-2025.pdf', 'Definition of Capital_16-09-2025.pdf', 'Related Party Transaction Risk_16-09-2025.pdf', 'Reporting (CRR)_16-09-2025.pdf', 'Credit Risk_16-09-2025.pdf', 'Securitisation_16-09-2025.pdf', 'Non-Performing Exposures Securitisation (CRR)_16-09-2025.pdf', 'Standardised Approach and Internal Ratings Based Approach to Credit Risk (CRR)_16-09-2025.pdf', 'Disclosure (CRR)_16-09-2025.pdf', 'Operational Resilience â€“ CRR Firms_16-09-2025.pdf', 'Regulatory Reporting_16-09-2025.pdf', 'Leverage Ratio (CRR)_16-09-2025.pdf', 'Internal Capital Adequacy Assessment_16-09-2025.pdf', 'Capital Buffers_16-09-2025.pdf', 'Algorithmic Trading_16-09-2025.pdf', 'Group Risk Systems_16-09-2025.pdf', 'Compliance and Internal Audit_16-09-2025.pdf', 'Leverage Ratio â€“ Capital Requirements and Buffers_16-09-2025.pdf', 'Risk Control_16-09-2025.pdf', 'SDDT Regime â€“ General Application_16-09-2025.pdf', 'Groups_16-09-2025.pdf', 'Audit Committee_16-09-2025.pdf', 'Waivers Transitional Provisions_16-09-2025.pdf', 'Operational Risk (CRR)_16-09-2025.pdf', 'Internal Liquidity Adequacy Assessment_16-09-2025.pdf', 'Liquidity Coverage Ratio (CRR)_16-09-2025.pdf', 'Large Exposures (CRR)_16-09-2025.pdf']\n",
      "  Searching for pattern '*.pdf' in /Users/rodrigograndy/Desktop/coding_projects/posts_website_drafts/rag_boe/data/pra_rulebook\n",
      "  Found 41 file(s) matching '*.pdf'\n",
      "  Sample found files: ['Market Risk_16-09-2025.pdf', 'Own Funds and Eligible Liabilities (CRR)_16-09-2025.pdf', 'Benchmarking of Internal Approaches_16-09-2025.pdf']\n",
      "  Searching for pattern '*.docx' in /Users/rodrigograndy/Desktop/coding_projects/posts_website_drafts/rag_boe/data/pra_rulebook\n",
      "  Found 0 file(s) matching '*.docx'\n",
      "  Searching for pattern '*.txt' in /Users/rodrigograndy/Desktop/coding_projects/posts_website_drafts/rag_boe/data/pra_rulebook\n",
      "  Found 0 file(s) matching '*.txt'\n",
      "  Searching for pattern '*.md' in /Users/rodrigograndy/Desktop/coding_projects/posts_website_drafts/rag_boe/data/pra_rulebook\n",
      "  Found 0 file(s) matching '*.md'\n",
      "  Searching for pattern '*.csv' in /Users/rodrigograndy/Desktop/coding_projects/posts_website_drafts/rag_boe/data/pra_rulebook\n",
      "  Found 0 file(s) matching '*.csv'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541ea641a87d4e119c00c0a8b23c6ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 41 files (41 succeeded, 0 failed)\n"
     ]
    }
   ],
   "source": [
    "# Default: auto-detect PDF complexity. Options: \"auto\", \"layout\", \"simple\". Auto should detect complexity of files (needs to be tested more)\n",
    "processor = EnhancedDocumentProcessor()\n",
    "files = discover_documents(file_directories)\n",
    "results = processor.process_files(files, pdf_mode=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3249742",
   "metadata": {},
   "source": [
    "#### 2.6 Check results\n",
    "\n",
    "Quick verification of the extraction output produced by the data processor (results list produced in 2.5).\n",
    "- Input: expects the global results variable which is a list of (Path, text, metadata) tuples.\n",
    "- Behavior:\n",
    "  - Inspects the first result (results[:1]) to avoid iterating the whole corpus.\n",
    "  - If the file's metadata contains a 'layout' key (PDF layout extraction), it prints the document name, the number of layout spans and a 500-char sample of the extracted text.\n",
    "  - Otherwise, if plain text is present, it prints the document name and a 500-char sample of the text.\n",
    "- Role in pipeline: lightweight, read-only sanity check to confirm extraction mode (layout vs simple) and that non-empty text was returned; no state mutation or downstream changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab3fa79b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ab3fa79b",
    "outputId": "cd429832-97a9-4662-8a40-21aa564644f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document: Market Risk_16-09-2025.pdf\n",
      "Extracted 204 text spans\n",
      "Sample text: Prudential Regulation Authority Rulebook\n",
      "Part\n",
      "Market Risk\n",
      "Printed on: 16/09/2025\n",
      "Rulebook at: 16/09/2025\n",
      "Related links\n",
      "PS7/13 - Strengthening capital standar\n",
      "ds: implementing CRD IV, feedback an\n",
      "d final rules\n",
      "https://www.bankofengland.co.uk/prudential-regula\n",
      "tion/publication/2013/strengthening-capital-standar\n",
      "ds-implementing-crd-4\n",
      "PS20/21 - Financial holding companie\n",
      "s: Further implementation\n",
      "https://www.bankofengland.co.uk/prudential-regula\n",
      "tion/publication/2021/june/financial-holding-compa\n",
      "nie...\n"
     ]
    }
   ],
   "source": [
    "# Check results with layout data\n",
    "for path, text, metadata in results[:1]:\n",
    "    if metadata and 'layout' in metadata:\n",
    "        print(f\"\\nDocument: {path.name}\")\n",
    "        print(f\"Extracted {len(metadata['layout'])} text spans\")\n",
    "        print(f\"Sample text: {text[:500]}...\")\n",
    "    elif text:\n",
    "        print(f\"\\nDocument: {path.name}\")\n",
    "        print(f\"Text: {text[:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fe7752",
   "metadata": {},
   "source": [
    "#### 2.7 Merge metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba62b70",
   "metadata": {
    "id": "4ba62b70",
    "outputId": "2d21863d-b571-496a-b8ea-1b322fa03114"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Metadata merged for 41 document(s)\n"
     ]
    }
   ],
   "source": [
    "failed_extractions = []\n",
    "\n",
    "for i, (path, text, extraction_meta) in enumerate(results):\n",
    "    try:\n",
    "        comprehensive_metadata = consolidated_metadata_extractor.extract_comprehensive_metadata(path, text)\n",
    "        \n",
    "        if extraction_meta:\n",
    "            comprehensive_metadata.update(extraction_meta)\n",
    "        \n",
    "        results[i] = (path, text, comprehensive_metadata)\n",
    "        \n",
    "    except Exception as e:\n",
    "        # TRACK FAILURES but continue processing\n",
    "        print(f\"Metadata extraction failed for {path.name}: {e}\")\n",
    "        \n",
    "        # Use basic metadata temporarily BUT flag it\n",
    "        file_meta = parse_filename_metadata(path)\n",
    "        merged_meta = merge_metadata(file_meta, extraction_meta)\n",
    "        merged_meta['metadata_extraction_failed'] = True  # FLAG IT\n",
    "        merged_meta['extraction_error'] = str(e)\n",
    "        results[i] = (path, text, merged_meta)\n",
    "        \n",
    "        failed_extractions.append({\n",
    "            'file': path.name,\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "# REPORT ALL FAILURES at the end\n",
    "if failed_extractions:\n",
    "    print(f\"\\nWARNING: {len(failed_extractions)} document(s) had metadata extraction failures:\")\n",
    "    for failure in failed_extractions:\n",
    "        print(f\"   â€¢ {failure['file']}: {failure['error']}\")\n",
    "    print(f\"\\n   These documents have incomplete metadata - review and fix before production!\")\n",
    "\n",
    "else:\n",
    "    print(f\"âœ… Metadata merged for {len(results)} document(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7af90e0",
   "metadata": {
    "id": "a7af90e0"
   },
   "source": [
    "#### 2.8 Chunking Function\n",
    "\n",
    "Creates chunks from extracted text.\n",
    "\n",
    "- Inputs relied on  \n",
    "  - results: list of (Path, text, extraction_meta) tuples created earlier.  \n",
    "  - chunk_text_enhanced(text, ...): boundary-aware chunker defined in Section 1.7.5.  \n",
    "  - extract_file_metadata(path) and merge_metadata(file_meta, extraction_meta).\n",
    "\n",
    "  - Invokes chunk_text_enhanced on the cleaned text with chunk_id_prefix=path.stem so chunk IDs are stable and document-scoped.\n",
    "  - Stores per-document record into all_chunks[str(path)] = { \"text\": text, \"chunks\": chunks, \"metadata\": merged_meta }.\n",
    "  - Tracks and reports total files and total chunks using a tqdm progress bar.\n",
    "\n",
    "- Outputs / side effects\n",
    "  - all_chunks populated with one entry per document; each entry contains original text, an ordered list of chunk dicts (chunk_id, text, offset, size, boundary_type), and merged metadata suitable for sentiment/embedding/storage steps.\n",
    "  \n",
    "- Design notes / important behaviors\n",
    "  - Ensures chunk IDs include filename stem for stable provenance and deduplication.\n",
    "  - Uses merge_metadata so extraction-derived fields (e.g., layout, extraction_mode, content-derived year/quarter) are preserved and attached to stored chunks.\n",
    "  - It reassigns all_chunks (overwrites existing mapping) â€” re-running will regenerate chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e31a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = EnhancedDocumentProcessor()\n",
    "text, meta = processor.extract_text(\n",
    "    Path(\"data/earnings_transcripts/barclays_1Q20_earnings_transcript.pdf\"),\n",
    "    use_layout=True,         # structured extraction for chunking\n",
    "    preserve_layout=False,   # do not keep raw pages in meta/cache output\n",
    "    use_cache=True\n",
    ")\n",
    "# meta now contains structure/tables/char positions useful for chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7435f2",
   "metadata": {},
   "source": [
    "##### 2.8.1 Display sample chunk with metadata\n",
    "\n",
    "Quick, read-only sanity check that prints a small sample of chunk records stored in all_chunks for human inspection.\n",
    "\n",
    "- Behavior:\n",
    "  - Iterates the first document in all_chunks (list(all_chunks.items())[:1]).\n",
    "  - For up to two chunks from that document, prints a concise summary:\n",
    "    - chunk index and chunk_id\n",
    "    - character length of chunk text\n",
    "    - boundary_type and offset mapping\n",
    "    - the chunk text itself (trimmed by the print)\n",
    "  - Uses basic dictionary access; does not modify state.\n",
    "\n",
    "- Output: human-readable console prints for manual verification (no return value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c4b29247",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4b29247",
    "outputId": "626a2125-70c8-4c92-87b2-abd9efd62c2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample chunks from '/Users/rodrigograndy/Desktop/coding_projects/posts_website_drafts/rag_boe/data/pra_rulebook/Market Risk_16-09-2025.pdf' (first 2):\n",
      "---\n",
      "\n",
      "Chunk 1 (id=Market Risk_16-09-2025_0, len=582):\n",
      "Boundary: natural, Offset: {'start': 0, 'end': 583}\n",
      "Text:\n",
      "Prudential Regulation Authority Rulebook\n",
      "Part\n",
      "Market Risk\n",
      "Printed on: 16/09/2025\n",
      "Rulebook at: 16/09/2025\n",
      "Related links\n",
      "PS7/13 - Strengthening capital standar\n",
      "ds: implementing CRD IV, feedback an\n",
      "d final rules\n",
      "https://www.bankofengland.co.uk/prudential-regula\n",
      "tion/publication/2013/strengthening-capital-standar\n",
      "ds-implementing-crd-4\n",
      "PS20/21 - Financial holding companie\n",
      "s: Further implementation\n",
      "https://www.bankofengland.co.uk/prudential-regula\n",
      "tion/publication/2021/june/financial-holding-compa\n",
      "nies-further-implementation\n",
      "Legislation.gov.uk\n",
      "http://www.legislation.gov.uk/\n",
      "Eur-Lex\n",
      "---\n",
      "\n",
      "Chunk 2 (id=Market Risk_16-09-2025_1, len=553):\n",
      "Boundary: natural, Offset: {'start': 483, 'end': 1037}\n",
      "Text:\n",
      "holding-compa\n",
      "nies-further-implementation\n",
      "Legislation.gov.uk\n",
      "http://www.legislation.gov.uk/\n",
      "Eur-Lex\n",
      "http://eur-lex.europa.eu/en/index.htm\n",
      "SS13/13 - Market risk\n",
      "http://www.bankofengland.co.uk/pra/Pages/publica\n",
      "tions/ss/2016/ss1313update2.aspx\n",
      "SS31/15 - The Internal Capital Adequa\n",
      "cy Assessment Process (ICAAP) and t\n",
      "he Supervisory Review and Evaluation\n",
      "Process (SREP)\n",
      "http://www.bankofengland.co.uk/pra/Pages/publica\n",
      "tions/ss/2015/ss3115update.aspx\n",
      "SS3/19 - Enhancing banksâ€™ and insurer\n",
      "sâ€™ approaches to managing the financia\n",
      "l risks from climate change\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show a small sample for quick inspection\n",
    "for name, doc in list(all_chunks.items())[:1]:\n",
    "    if doc.get('chunks'):\n",
    "        print(f\"\\nSample chunks from '{name}' (first 2):\\n---\\n\")\n",
    "        for i, chunk_dict in enumerate(doc['chunks'][:2]):\n",
    "            chunk_text = chunk_dict['text']  # Extract text from the dictionary\n",
    "            print(f\"Chunk {i+1} (id={chunk_dict['chunk_id']}, len={len(chunk_text)}):\")\n",
    "            print(f\"Boundary: {chunk_dict['boundary_type']}, Offset: {chunk_dict['offset']}\")\n",
    "            print(f\"Text:\\n{chunk_text}\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b9d6c50d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9d6c50d",
    "outputId": "1c16e7cc-9933-4a61-99d4-721387be8aed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis functions ready\n"
     ]
    }
   ],
   "source": [
    "# Financial sentiment analysis\n",
    "def analyze_fin_sentiment(text: str, model: pipeline) -> Dict[str, float]:\n",
    "    \"\"\"Get financial sentiment with confidence scores\"\"\"\n",
    "    try:\n",
    "        text = text[:512]  # Truncate to model limit\n",
    "        results = model(text)\n",
    "        best_result = max(results[0], key=lambda x: x['score'])\n",
    "        return {\n",
    "            'sentiment': best_result['label'].lower(),\n",
    "            'confidence': round(best_result['score'], 3)\n",
    "        }\n",
    "    except Exception:\n",
    "        return {'sentiment': 'undetermined', 'confidence': 0.0}\n",
    "\n",
    "def batch_sentiment_analysis(chunks: List[str], model: pipeline) -> List[Dict[str, float]]:\n",
    "    \"\"\"Analyze sentiment for multiple chunks\"\"\"\n",
    "    results = []\n",
    "    for chunk in tqdm(chunks, desc=\"Sentiment\"):\n",
    "        results.append(analyze_fin_sentiment(chunk, model))\n",
    "    return results\n",
    "\n",
    "print(f\"Sentiment analysis functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f55947",
   "metadata": {},
   "source": [
    "##### 2.9.1 Apply Sentiment Analysis to all chunks\n",
    "\n",
    "Batch-runs the FinBERT sentiment wrapper across every text chunk in all_chunks and writes per-chunk sentiment metadata back into each chunk dict.\n",
    "\n",
    "Preconditions\n",
    "- all_chunks is populated (each document entry contains a 'chunks' list).\n",
    "- finbert pipeline and analyze_fin_sentiment() are available.\n",
    "- analyze_fin_sentiment truncates input to 512 chars, returns {'sentiment': str, 'confidence': float} and falls back to {'sentiment':'undetermined','confidence':0.0} on errors.\n",
    "\n",
    "Key variables & flow\n",
    "- all_chunk_texts: flattened list of every chunk['text'] across all documents â€” used for a single progress pass.\n",
    "- chunk_file_mapping: parallel list recording which document each chunk came from (provenance).\n",
    "- all_sentiments: list collecting analyze_fin_sentiment results in the same order as all_chunk_texts.\n",
    "- sentiment assignment loop: iterates documents and their chunks in all_chunks and assigns chunk['sentiment'] = all_sentiments[sentiment_idx], preserving original chunk ordering.\n",
    "\n",
    "Behavioral notes\n",
    "- Uses tqdm to show progress during sentiment inference.\n",
    "- Prints summary messages before and after processing.\n",
    "- Downstream code expects chunk['sentiment'] to contain a label and numeric confidence for storage, filtering, and UI display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6bff0002",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100,
     "referenced_widgets": [
      "6c1ecca5215d41d89a0f830cd8c25225",
      "100e97e7f6d84d0683a4c705e4fe2eff",
      "a903083c92cd4b788191bf6e9ee46695",
      "0025cf09b3e74298acf17160ed680e38",
      "3ee652ae942346528f4a06ee5ff37d6a",
      "af012e52c3ad4f539164a67364831a55",
      "84e01f7ee39844b796f03a11b9be1407",
      "7a995d4f93cb42378850d78897a9fe68",
      "39a63fff1694413b8ed1449d9e8d88df",
      "7aa4d76a95ad45b798bf831bbf1a59fa",
      "9c9293eca0e042458da526ea365549d4"
     ]
    },
    "id": "6bff0002",
    "outputId": "a7165d1f-5569-462d-afe2-0c9ce8689064"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying sentiment analysis to all chunks...\n",
      "Processing 2991 chunks across 41 files...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e1d54455e0d40eebf4f885bdffc2a3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sentiment analysis:   0%|          | 0/2991 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis complete for all 2991 chunks\n"
     ]
    }
   ],
   "source": [
    "# Apply sentiment analysis to all chunks\n",
    "print(\"Applying sentiment analysis to all chunks...\")\n",
    "\n",
    "# Collect all chunks from all documents for batch processing\n",
    "all_chunk_texts = []\n",
    "chunk_file_mapping = []  # Keep track of which file each chunk belongs to\n",
    "\n",
    "for name, doc in all_chunks.items():\n",
    "    if doc.get('chunks'):\n",
    "        for chunk in doc['chunks']:\n",
    "            all_chunk_texts.append(chunk['text'])\n",
    "            chunk_file_mapping.append(name)\n",
    "\n",
    "# Process all chunks with a single progress bar\n",
    "print(f\"Processing {len(all_chunk_texts)} chunks across {len(all_chunks)} files...\")\n",
    "all_sentiments = []\n",
    "for chunk_text in tqdm(all_chunk_texts, desc=\"Sentiment analysis\"):\n",
    "    sentiment = analyze_fin_sentiment(chunk_text, finbert)\n",
    "    all_sentiments.append(sentiment)\n",
    "\n",
    "# Assign sentiment results back to chunks\n",
    "sentiment_idx = 0\n",
    "for name, doc in all_chunks.items():\n",
    "    if doc.get('chunks'):\n",
    "        for chunk in doc['chunks']:\n",
    "            chunk['sentiment'] = all_sentiments[sentiment_idx]\n",
    "            sentiment_idx += 1\n",
    "\n",
    "print(f\"Sentiment analysis complete for all {len(all_chunk_texts)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1591de73",
   "metadata": {},
   "source": [
    "##### 2.9.2 Display sample chunk with sentiment\n",
    "\n",
    "Quick sanity check to confirm sentiment labels and confidences were attached to chunks and that chunk metadata and document metadata (including layout summaries) look correct before downstream storage or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa11bb4e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aa11bb4e",
    "outputId": "91b549b4-f6f9-41dc-d495-45d21d564d46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample chunk with sentiment from 'Market Risk_16-09-2025.pdf':\n",
      "============================================================\n",
      "\n",
      "Chunk ID: Market Risk_16-09-2025_0\n",
      "Text Length: 582 characters\n",
      "Sentiment: Neutral (confidence: 100.0%)\n",
      "Boundary Type: natural\n",
      "Offset: {'start': 0, 'end': 583}\n",
      "\n",
      "Text Sample:\n",
      "Prudential Regulation Authority Rulebook\n",
      "Part\n",
      "Market Risk\n",
      "Printed on: 16/09/2025\n",
      "Rulebook at: 16/09/2025\n",
      "Related links\n",
      "PS7/13 - Strengthening capital standar\n",
      "ds: implementing CRD IV, feedback an\n",
      "d final rules\n",
      "https://www.bankofengland.co.uk/prudential-regula\n",
      "tion/publication/2013/strengthening-capit...\n",
      "\n",
      "Document Metadata:\n",
      "   filename: Market Risk_16-09-2025.pdf\n",
      "   company: market risk\n",
      "   quarter: 16-09-2025\n",
      "   doc_type: financial_document\n",
      "   file_extension: .pdf\n",
      "   filename_parts: ['market risk', '16-09-2025']\n",
      "   extraction_source: content\n",
      "   company_filename: market risk\n",
      "   year_filename: 2025\n",
      "   years_filename: ['2025']\n",
      "   doc_type_filename: financial_document\n",
      "   content_dates: {'full_date_content': ['16/09/2025', '16/09/2025', '17/09/2021', '17/09/2021', '01/01/2014'], 'years_content': ['2025', '2021', '2019', '2016', '2015'], 'primary_year_content': '2021', 'quarters_content': ['3', '4', '1', '2']}\n",
      "   doc_type_content: policy_document\n",
      "   doc_type_scores: {'policy_document': {'score': 4, 'matched_patterns': ['policy', 'procedure']}}\n",
      "   doc_type_confidence: 0.4\n",
      "   extracted_entities: {'companies': ['Internal Capital'], 'people': [], 'organizations': ['Internal Models: Risk Capture', 'The Internal Capital Adequa', 'Title\\nIV', 'Regulated Activities Order', 'CRD IV', 'Supervisory Review and Evaluation\\nProcess', 'Prudential Regulation Authorityâ€™s', 'CRR', 'Prudential Regulation Authority Rulebook\\nPart\\nMarket Risk\\nPrinted'], 'money_amounts': [], 'percentages': ['100%'], 'locations': ['UK']}\n",
      "   entity_counts: {'companies': 1, 'people': 0, 'organizations': 9, 'money_amounts': 0, 'percentages': 1, 'locations': 1}\n",
      "   financial_terms: {'metrics': ['profit', 'loss'], 'instruments': ['bonds', 'equity'], 'regulations': ['pra']}\n",
      "   financial_term_frequencies: {'metrics': {'profit': 1, 'loss': 1}, 'instruments': {'bonds': 2, 'equity': 5}, 'regulations': {'pra': 2}}\n",
      "   financial_complexity_score: 0.25\n",
      "   total_financial_terms: 5\n",
      "   content_statistics: {'word_count': 707, 'sentence_count': 57, 'paragraph_count': 1, 'avg_words_per_sentence': 12.4, 'has_numbers': True, 'has_tables': False, 'has_financial_symbols': True}\n",
      "   doc_type_source: filename\n",
      "   company_source: content\n",
      "   all_companies: ['Internal Capital']\n",
      "   year: 2021\n",
      "   year_source: content\n",
      "   quarter_source: content\n",
      "   metadata_quality_score: 0.65\n",
      "   advanced_extraction_applied: True\n",
      "   layout: 204 layout spans\n",
      "   extraction_mode: layout_auto\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a sample chunk with sentiment for verification\n",
    "for name, doc in list(all_chunks.items())[:1]:\n",
    "    if doc.get('chunks'):\n",
    "        print(f\"\\nSample chunk with sentiment from '{Path(name).name}':\\n\" + \"=\"*60 + \"\\n\")\n",
    "        sample_chunk = doc['chunks'][0]\n",
    "        print(f\"Chunk ID: {sample_chunk['chunk_id']}\")\n",
    "        print(f\"Text Length: {len(sample_chunk['text'])} characters\")\n",
    "\n",
    "        # Check if sentiment data exists\n",
    "        sentiment_data = sample_chunk.get('sentiment', {})\n",
    "        if sentiment_data and 'sentiment' in sentiment_data:\n",
    "            sentiment_label = sentiment_data['sentiment'].title()\n",
    "            confidence = sentiment_data.get('confidence', 0.0)\n",
    "            print(f\"Sentiment: {sentiment_label} (confidence: {confidence:.1%})\")\n",
    "        else:\n",
    "            print(f\"Sentiment: Not available (run sentiment analysis first)\")\n",
    "\n",
    "        print(f\"Boundary Type: {sample_chunk.get('boundary_type', 'unknown')}\")\n",
    "        print(f\"Offset: {sample_chunk.get('offset', {})}\")\n",
    "        print(f\"\\nText Sample:\\n{sample_chunk['text'][:300]}{'...' if len(sample_chunk['text']) > 300 else ''}\\n\")\n",
    "        print(f\"Document Metadata:\")\n",
    "        if doc['metadata']:\n",
    "            for key, value in doc['metadata'].items():\n",
    "                if key == 'layout':\n",
    "                    print(f\"   {key}: {len(value)} layout spans\" if isinstance(value, list) else f\"   {key}: {value}\")\n",
    "                else:\n",
    "                    print(f\"   {key}: {value}\")\n",
    "        else:\n",
    "            print(\"   No metadata available\")\n",
    "        print(\"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50be522",
   "metadata": {},
   "source": [
    "##### 2.10.1 Generate Embeddings for all chunks\n",
    "\n",
    "Orchestrates flattening of all chunk texts, generates embeddings with the previously-loaded SentenceTransformer, and writes embeddings back into each chunk dict.\n",
    "\n",
    "\n",
    "- all_chunks: mapping of document path â†’ { \"text\", \"chunks\": [...], \"metadata\": ... } must be populated.\n",
    "- embedding_model: global SentenceTransformer instance loaded in Section 1.5 must be available.\n",
    "- Embedding generation\n",
    "  - Calls generate_embeddings(all_chunk_texts, embedding_model) in one shot.\n",
    "  - generate_embeddings performs model.encode(..., show_progress_bar=True, normalize_embeddings=True) and returns a numpy.ndarray (empty array on no input).\n",
    "  - Note: this is a single-call approach (no internal batching), so memory use scales with corpus size (This something to improve in the future).\n",
    "\n",
    "- Assignment back to chunks\n",
    "  - Iterates over all_chunks in original order and assigns chunk['embedding'] = all_embeddings[embedding_idx] while guarding index bounds.\n",
    "  - Tracks total_chunks_embedded and increments embedding_idx for each assigned embedding.\n",
    "  - Mutates all_chunks in-place so downstream steps can use chunk['embedding'].\n",
    "\n",
    "- Important properties / caveats\n",
    "  - Embeddings are normalized by the encoder (normalize_embeddings=True).\n",
    "  - If all_chunk_texts is empty, all_embeddings stays empty and no assignments occur.\n",
    "  - Single-call encoding may cause high memory use for large corpora; downstream code elsewhere provides batched alternatives.\n",
    "  - The parallel chunk_file_mapping is collected but not used in the assignment loop (provenance is preserved via chunk IDs and metadata).\n",
    "\n",
    "Effect\n",
    "- After the cell runs, each chunk dict in all_chunks that had a corresponding embedding receives an 'embedding' field (numpy array), enabling storage, similarity search and downstream ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5368f681",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100,
     "referenced_widgets": [
      "8b5327ec92eb4f87a93403538fa28fea",
      "66fab4aa7d894d1fac6a01ee3c49a941",
      "f612d957ea1c45ca8a3418128ed4d0bb",
      "5b11374aa95043498807e683c2c52398",
      "cb28e591587c4bd1b5e14c27d883b6f2",
      "f2ec9f58668b4f6dbdca6073ec463dde",
      "1763329f29de4e42b83c9517fe9499d1",
      "48b9e3ee3a274d26b6bef10cb805c412",
      "32d7a49e2aa24ac58ee4c3eff272e3e5",
      "50b9ed7391c44f229ae719bc984d2ae1",
      "833cd928eaec4488941db2214b1d3e4f"
     ]
    },
    "id": "5368f681",
    "outputId": "ef8fb016-7237-41ad-ec32-5ff984b9cdac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying embedding generation to all chunks...\n",
      "Processing 2991 chunks across 41 files...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cd371eea0ee41fa887aec124e2f6a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding generation complete for all 2991 chunks\n"
     ]
    }
   ],
   "source": [
    "# Apply embedding generation to all chunks\n",
    "print(\"Applying embedding generation to all chunks...\")\n",
    "\n",
    "# Collect all chunk texts from all documents for batch processing\n",
    "all_chunk_texts = []\n",
    "chunk_file_mapping = []  # Keep track of which file each chunk belongs to\n",
    "\n",
    "for name, doc in all_chunks.items():\n",
    "    if doc.get('chunks'):\n",
    "        for chunk in doc['chunks']:\n",
    "            all_chunk_texts.append(chunk['text'])\n",
    "            chunk_file_mapping.append(name)\n",
    "\n",
    "# Process all chunks with a single progress bar\n",
    "print(f\"Processing {len(all_chunk_texts)} chunks across {len(all_chunks)} files...\")\n",
    "all_embeddings = []\n",
    "\n",
    "\n",
    "# Generate embeddings for all chunks at once\n",
    "if all_chunk_texts:\n",
    "    all_embeddings = generate_embeddings(all_chunk_texts, embedding_model) #  From section 1.5 \n",
    "\n",
    "# Assign embedding results back to chunks\n",
    "embedding_idx = 0\n",
    "total_chunks_embedded = 0\n",
    "\n",
    "for name, doc in all_chunks.items():\n",
    "    if doc.get('chunks'):\n",
    "        for chunk in doc['chunks']:\n",
    "            if embedding_idx < len(all_embeddings):\n",
    "                chunk['embedding'] = all_embeddings[embedding_idx]\n",
    "                embedding_idx += 1\n",
    "                total_chunks_embedded += 1\n",
    "\n",
    "print(f\"Embedding generation complete for all {total_chunks_embedded} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c166ea2",
   "metadata": {},
   "source": [
    "##### 2.10.2 Display sample embedding\n",
    "\n",
    "Sanity check to verify that embeddings were attached to chunks and that embedding/sentiment fields have expected types and shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0fa7a51d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0fa7a51d",
    "outputId": "51704535-24b0-44ea-fb71-6767c66cf6f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample chunk with embeddings from 'Market Risk_16-09-2025.pdf':\n",
      "---\n",
      "\n",
      "Chunk ID: Market Risk_16-09-2025_0\n",
      "Text Length: 582 characters\n",
      "Text Sample: Prudential Regulation Authority Rulebook\n",
      "Part\n",
      "Market Risk\n",
      "Printed on: 16/09/2025\n",
      "Rulebook at: 16/09/2025\n",
      "Related links\n",
      "PS7/13 - Strengthening capital standar\n",
      "ds: implementing CRD IV, feedback an\n",
      "d fin...\n",
      "Embedding: numpy array of shape (1024,)\n",
      "   First 5 values: [-0.00994054 -0.00976403 -0.0113218   0.00842191  0.01860651]\n",
      "Sentiment: Neutral (confidence: 100.0%)\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a sample chunk with embeddings for verification\n",
    "for name, doc in list(all_chunks.items())[:1]:\n",
    "    if doc.get('chunks'):\n",
    "        print(f\"\\nSample chunk with embeddings from '{Path(name).name}':\\n---\\n\")\n",
    "        sample_chunk = doc['chunks'][0]\n",
    "        print(f\"Chunk ID: {sample_chunk['chunk_id']}\")\n",
    "        print(f\"Text Length: {len(sample_chunk['text'])} characters\")\n",
    "        print(f\"Text Sample: {sample_chunk['text'][:200]}...\")\n",
    "\n",
    "        embedding = sample_chunk.get('embedding')\n",
    "        if embedding is not None:\n",
    "            if isinstance(embedding, np.ndarray):\n",
    "                print(f\"Embedding: numpy array of shape {embedding.shape}\")\n",
    "                print(f\"   First 5 values: {embedding[:5]}\")\n",
    "            elif isinstance(embedding, list):\n",
    "                print(f\"Embedding: list of length {len(embedding)}\")\n",
    "                print(f\"   First 5 values: {embedding[:5]}\")\n",
    "            else:\n",
    "                print(f\"Embedding: {type(embedding)}\")\n",
    "        else:\n",
    "            print(\"No embedding found! (run embedding generation first)\")\n",
    "\n",
    "        # Check sentiment data\n",
    "        sentiment = sample_chunk.get('sentiment', {})\n",
    "        if sentiment and 'sentiment' in sentiment:\n",
    "            print(f\"Sentiment: {sentiment['sentiment'].title()} (confidence: {sentiment.get('confidence', 0.0):.1%})\")\n",
    "        else:\n",
    "            print(\"Sentiment: Not available (run sentiment analysis first)\")\n",
    "\n",
    "        print(\"---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff813d1",
   "metadata": {
    "id": "0ff813d1"
   },
   "source": [
    "## 3. Database Storage\n",
    "\n",
    "This section includes functions for creating and managing the vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8f7341",
   "metadata": {},
   "source": [
    "#### 3.1 Collection Management with Delete Functionality\n",
    "\n",
    "Provide an interactive UI (ipywidgets) to list/create/select/delete ChromaDB collections and to store processed document chunks into a selected collection. For new collection add new collection name, create and save the collection. For existing collection, select from dropdown menu, and stored processed document chunks into the selected collection.\n",
    "\n",
    "- create_enhanced_collection_manager_ui(client, all_chunks)\n",
    "  - Builds and returns the full widget layout; requires a chromadb client and the notebook-level all_chunks mapping.\n",
    "\n",
    "Key UI components\n",
    "- existing_dropdown: collection chooser populated from client via list_existing_collections.\n",
    "- new_name_input: text input for creating a new collection.\n",
    "- Buttons: Select, Create New, Delete, and Store Documents.\n",
    "- storage_progress: FloatProgress used during ingestion.\n",
    "- output: Output widget used for all console messages.\n",
    "\n",
    "- refresh_collections(): fetches current collection names from the client.\n",
    "- update_dropdown(): updates dropdown state and enables/disables buttons (delete, store) based on availability of collections and all_chunks.\n",
    "- on_select(btn): selects the dropdown value, assigns the global collection object (client.get_collection), prints basic collection info (via get_collection_info) and toggles store button.\n",
    "- on_create(btn): uses create_new_collection(client, name) to create or fetch a collection, refreshes UI, and enables storage when all_chunks is present.\n",
    "- on_delete(btn): shows a confirmation UI (Confirm Delete / Cancel). on_confirm calls client.delete_collection(name), clears global collection if it was selected, refreshes the dropdown and prints success/error. on_cancel simply dismisses the confirmation.\n",
    "- on_store_documents(btn): guarded ingestion routine:\n",
    "  - Validates a collection is selected and that all_chunks contains processed documents.\n",
    "  - Shows storage_progress, iterates all_chunks, and for each document calls store_enriched_chunks(collection, chunks, metadata).\n",
    "  - Tracks total_stored and failed_files, updates progress bar, prints a concise end summary (total stored and brief failure list), hides the progress bar, and attempts to print final collection.count().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b0d509d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced Collection Management UI with Document Storage ready\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Collection Management UI with Document Storage Integration\n",
    "def create_enhanced_collection_manager_ui(client, all_chunks):\n",
    "    \"\"\"Complete collection management UI with create, select, delete, and storage operations\"\"\"\n",
    "    \n",
    "    def refresh_collections():\n",
    "        \"\"\"Helper to get current collection list\"\"\"\n",
    "        return list_existing_collections(client)\n",
    "    \n",
    "    # UI Components\n",
    "    existing_dropdown = widgets.Dropdown(\n",
    "        options=['No collections found'],\n",
    "        description='Collection:',\n",
    "        disabled=True,\n",
    "        layout=widgets.Layout(width='400px')\n",
    "    )\n",
    "    \n",
    "    new_name_input = widgets.Text(\n",
    "        description='New Name:',\n",
    "        placeholder='Enter collection name',\n",
    "        layout=widgets.Layout(width='400px')\n",
    "    )\n",
    "    \n",
    "    # Buttons\n",
    "    select_btn = widgets.Button(\n",
    "        description='Select',\n",
    "        button_style='primary',\n",
    "        layout=widgets.Layout(width='120px')\n",
    "    )\n",
    "    \n",
    "    create_btn = widgets.Button(\n",
    "        description='Create New',\n",
    "        button_style='success',\n",
    "        layout=widgets.Layout(width='120px')\n",
    "    )\n",
    "    \n",
    "    delete_btn = widgets.Button(\n",
    "        description='Delete',\n",
    "        button_style='danger',\n",
    "        layout=widgets.Layout(width='120px'),\n",
    "        disabled=True\n",
    "    )\n",
    "    \n",
    "    # NEW: Store Documents Button\n",
    "    store_btn = widgets.Button(\n",
    "        description='ðŸ“¥ Store Documents',\n",
    "        button_style='info',\n",
    "        layout=widgets.Layout(width='150px'),\n",
    "        disabled=True,\n",
    "        tooltip='Store processed documents in selected collection'\n",
    "    )\n",
    "    \n",
    "    # NEW: Progress indicators for storage\n",
    "    storage_progress = widgets.FloatProgress(\n",
    "        value=0,\n",
    "        min=0,\n",
    "        max=100,\n",
    "        description='Storing:',\n",
    "        bar_style='info',\n",
    "        layout=widgets.Layout(width='400px', visibility='hidden')\n",
    "    )\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def update_dropdown():\n",
    "        \"\"\"Refresh dropdown with current collections\"\"\"\n",
    "        collections = refresh_collections()\n",
    "        if collections:\n",
    "            existing_dropdown.options = collections\n",
    "            existing_dropdown.disabled = False\n",
    "            existing_dropdown.value = collections[0]\n",
    "            delete_btn.disabled = False\n",
    "            # Enable store button if collection is selected and documents are available\n",
    "            store_btn.disabled = not (all_chunks and len(all_chunks) > 0)\n",
    "        else:\n",
    "            existing_dropdown.options = ['No collections found']\n",
    "            existing_dropdown.disabled = True\n",
    "            delete_btn.disabled = True\n",
    "            store_btn.disabled = True\n",
    "    \n",
    "    def on_select(btn):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            if existing_dropdown.disabled:\n",
    "                print(\"No collections available\")\n",
    "                return\n",
    "            \n",
    "            name = existing_dropdown.value\n",
    "            try:\n",
    "                global collection\n",
    "                collection = client.get_collection(name)\n",
    "                info = get_collection_info(collection)\n",
    "                print(f\"Selected: '{name}'\")\n",
    "                print(f\"   Documents: {info['document_count']:,}\")\n",
    "                \n",
    "                # Enable store button if documents are available\n",
    "                store_btn.disabled = not (all_chunks and len(all_chunks) > 0)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error selecting '{name}': {e}\")\n",
    "    \n",
    "    def on_create(btn):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            name = new_name_input.value.strip()\n",
    "            \n",
    "            if not name:\n",
    "                print(\"Enter collection name\")\n",
    "                return\n",
    "            \n",
    "            try:\n",
    "                global collection\n",
    "                collection = create_new_collection(client, name)\n",
    "                if collection:\n",
    "                    print(f\"Created: '{name}'\")\n",
    "                    new_name_input.value = ''\n",
    "                    update_dropdown()\n",
    "                    \n",
    "                    # Enable store button if documents are available\n",
    "                    store_btn.disabled = not (all_chunks and len(all_chunks) > 0)\n",
    "                    \n",
    "                    # Show helpful message about storing documents\n",
    "                    if all_chunks and len(all_chunks) > 0:\n",
    "                        print(f\"\\nReady to store documents!\")\n",
    "                        print(f\"   Click 'Store Documents' to save {len(all_chunks)} document(s)\")\n",
    "                    else:\n",
    "                        print(f\"\\nProcess documents first (Section 2) to enable storage\")\n",
    "                else:\n",
    "                    print(f\"Failed to create '{name}'\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "    \n",
    "    def on_delete(btn):\n",
    "        with output:\n",
    "            clear_output()\n",
    "            if existing_dropdown.disabled:\n",
    "                print(\"No collections to delete\")\n",
    "                return\n",
    "            \n",
    "            name = existing_dropdown.value\n",
    "            \n",
    "            try:\n",
    "                temp_collection = client.get_collection(name)\n",
    "                doc_count = temp_collection.count()\n",
    "                info_msg = f\"Delete collection '{name}'?\\n   This will permanently remove {doc_count:,} document chunks.\"\n",
    "            except:\n",
    "                info_msg = f\"Delete collection '{name}'?\\n   This will permanently remove all documents.\"\n",
    "            \n",
    "            print(info_msg)\n",
    "            \n",
    "            confirm_btn = widgets.Button(\n",
    "                description='Confirm Delete',\n",
    "                button_style='danger',\n",
    "                layout=widgets.Layout(width='140px')\n",
    "            )\n",
    "            \n",
    "            cancel_btn = widgets.Button(\n",
    "                description='Cancel',\n",
    "                button_style='',\n",
    "                layout=widgets.Layout(width='100px')\n",
    "            )\n",
    "            \n",
    "            def on_confirm(b):\n",
    "                with output:\n",
    "                    clear_output()\n",
    "                    try:\n",
    "                        client.delete_collection(name)\n",
    "                        \n",
    "                        global collection\n",
    "                        if collection and collection.name == name:\n",
    "                            collection = None\n",
    "                        \n",
    "                        print(f\"Deleted collection: '{name}'\")\n",
    "                        update_dropdown()\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error deleting '{name}': {e}\")\n",
    "                        update_dropdown()\n",
    "            \n",
    "            def on_cancel(b):\n",
    "                with output:\n",
    "                    clear_output()\n",
    "                    print(\"Deletion cancelled\")\n",
    "            \n",
    "            confirm_btn.on_click(on_confirm)\n",
    "            cancel_btn.on_click(on_cancel)\n",
    "            \n",
    "            display(widgets.HBox([confirm_btn, cancel_btn], layout=widgets.Layout(margin='10px 0')))\n",
    "    \n",
    "    def on_store_documents(btn):\n",
    "        \"\"\"Store processed documents in the selected collection\"\"\"\n",
    "        with output:\n",
    "            clear_output()\n",
    "            \n",
    "            # Check if collection is selected\n",
    "            if 'collection' not in globals() or collection is None:\n",
    "                print(\"No collection selected\")\n",
    "                print(\"   â†’ Select a collection first\")\n",
    "                return\n",
    "            \n",
    "            # Check if documents are available\n",
    "            if not all_chunks or len(all_chunks) == 0:\n",
    "                print(\"No processed documents found\")\n",
    "                print(\"   â†’ Run Section 2 to process documents first\")\n",
    "                return\n",
    "            \n",
    "            # Show progress bar\n",
    "            storage_progress.layout.visibility = 'visible'\n",
    "            storage_progress.value = 0\n",
    "            \n",
    "            total_stored = 0\n",
    "            failed_files = []\n",
    "            total_files = len(all_chunks)\n",
    "            \n",
    "            for idx, (filepath, doc_data) in enumerate(all_chunks.items(), 1):\n",
    "                chunks = doc_data.get('chunks', [])\n",
    "                metadata = doc_data.get('metadata', {})\n",
    "                filename = Path(filepath).name\n",
    "                \n",
    "                # Update progress\n",
    "                storage_progress.value = (idx / total_files) * 100\n",
    "                \n",
    "                if not chunks:\n",
    "                    print(f\"  {filename}: No chunks (skipped)\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    num_stored = store_enriched_chunks(collection, chunks, metadata)\n",
    "                    total_stored += num_stored\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    error_msg = str(e)[:100]\n",
    "                    print(f\"  {filename}: {error_msg}\")\n",
    "                    failed_files.append((filename, error_msg))\n",
    "            \n",
    "            print(\"=\" * 60)\n",
    "            print(f\"Storage complete: {total_stored:,} chunks stored\")\n",
    "            \n",
    "            if failed_files:\n",
    "                print(f\"\\n{len(failed_files)} file(s) failed:\")\n",
    "                for filename, error in failed_files[:5]:\n",
    "                    print(f\"   â€¢ {filename}: {error}\")\n",
    "            \n",
    "            # Hide progress bar\n",
    "            storage_progress.layout.visibility = 'hidden'\n",
    "            storage_progress.value = 0\n",
    "            \n",
    "            # Show collection info\n",
    "            try:\n",
    "                final_count = collection.count()\n",
    "                print(f\"\\nCollection '{collection.name}' now contains: {final_count:,} chunks\")\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Bind events\n",
    "    select_btn.on_click(on_select)\n",
    "    create_btn.on_click(on_create)\n",
    "    delete_btn.on_click(on_delete)\n",
    "    store_btn.on_click(on_store_documents)\n",
    "    \n",
    "    # Initial population\n",
    "    update_dropdown()\n",
    "    \n",
    "    # Build UI\n",
    "    ui = widgets.VBox([\n",
    "        widgets.HTML(\"<h4>ðŸ“‚ Collection Management</h4>\"),\n",
    "        widgets.HTML(\"<hr style='margin: 10px 0;'>\"),\n",
    "        \n",
    "        # Select existing\n",
    "        widgets.HTML(\"<b>Select Existing Collection:</b>\"),\n",
    "        existing_dropdown,\n",
    "        widgets.HBox([select_btn, delete_btn], layout=widgets.Layout(margin='5px 0')),\n",
    "        \n",
    "        widgets.HTML(\"<div style='height: 20px;'></div>\"),\n",
    "        \n",
    "        # Create new\n",
    "        widgets.HTML(\"<b>Create New Collection:</b>\"),\n",
    "        new_name_input,\n",
    "        widgets.HBox([create_btn], layout=widgets.Layout(margin='5px 0')),\n",
    "        \n",
    "        widgets.HTML(\"<div style='height: 20px;'></div>\"),\n",
    "        \n",
    "        # Store documents section\n",
    "        widgets.HTML(\"<b>Store Documents in Collection:</b>\"),\n",
    "        widgets.HBox([store_btn], layout=widgets.Layout(margin='5px 0')),\n",
    "        storage_progress,\n",
    "        \n",
    "        widgets.HTML(\"<div style='height: 15px;'></div>\"),\n",
    "        output\n",
    "    ], layout=widgets.Layout(\n",
    "        padding='15px',\n",
    "        border='1px solid #e2e8f0',\n",
    "        border_radius='8px',\n",
    "        background_color='#f8fafc',\n",
    "        width='550px'\n",
    "    ))\n",
    "    \n",
    "    return ui\n",
    "\n",
    "print(\"Enhanced Collection Management UI with Document Storage ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95511b7c",
   "metadata": {},
   "source": [
    "#### 3.2 Choose collection to Store Documents in ChromaDB\n",
    "\n",
    "Display the collection-management UI built in 3.1 so the user can select/create/delete collections and store processed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d62b506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d63b4ecb58d4433b9133affca8f21aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4>ðŸ“‚ Collection Management</h4>'), HTML(value=\"<hr style='margin: 10px 0;'>\"), HTMâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "41 document(s) ready for storage\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Collection Manager with Storage\n",
    "if 'client' in globals():\n",
    "    # Check if documents have been processed\n",
    "    if 'all_chunks' not in globals():\n",
    "        all_chunks = {}\n",
    "        print(\"No processed documents found\")\n",
    "        print(\"Documents from Section 2 will be available for storage once processed\")\n",
    "    \n",
    "    enhanced_collection_ui = create_enhanced_collection_manager_ui(client, all_chunks)\n",
    "    display(enhanced_collection_ui)\n",
    "    \n",
    "    if all_chunks and len(all_chunks) > 0:\n",
    "        print(f\"\\n{len(all_chunks)} document(s) ready for storage\")\n",
    "    else:\n",
    "        print(\"\\nProcess documents in Section 2 to enable storage\")\n",
    "else:\n",
    "    print(\"ChromaDB client not available. Run Section 1.6 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b719cd7",
   "metadata": {
    "id": "2b719cd7"
   },
   "source": [
    "#### 3.3 Verify collection readiness\n",
    "\n",
    "- verify_collection_ready(): confirm a selected ChromaDB collection is set and accessible.  \n",
    "- display_all_collections(): enumerate all collections in the ChromaDB client and show per-collection diagnostics.\n",
    "- Cell-level orchestration after definitions  \n",
    "  - Calls display_all_collections() to show database state.  \n",
    "  - Then checks if a global collection exists: if so, prints \"Selected Collection Status:\" and runs verify_collection_ready(); otherwise prints guidance to run Section 3.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e047a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available Collections:\n",
      "============================================================\n",
      "   â€¢ transcrips_barclasys: 5,591 chunks\n",
      "   â€¢ pra_rules: 2,991 chunks\n",
      "============================================================\n",
      "\n",
      "Selected Collection Status:\n",
      "Collection ready: 'pra_rules'\n",
      "   Documents: 2,991 chunks\n"
     ]
    }
   ],
   "source": [
    "# Verify Collection Readiness\n",
    "def verify_collection_ready() -> bool:\n",
    "    \"\"\"Verify a collection is selected and accessible\"\"\"\n",
    "    global collection\n",
    "    \n",
    "    if collection is None:\n",
    "        print(\"No collection selected\")\n",
    "        print(\"Run Section 3.2 to select/create a collection\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        count = collection.count()\n",
    "        print(f\"Collection ready: '{collection.name}'\")\n",
    "        print(f\"   Documents: {count:,} chunks\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Collection error: {e}\")\n",
    "        print(\"Re-run Section 3.2 to select a valid collection\")\n",
    "        return False\n",
    "\n",
    "def display_all_collections() -> None:\n",
    "    \"\"\"Display all available collections with document counts\"\"\"\n",
    "    print(\"\\nAvailable Collections:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        collections = client.list_collections()\n",
    "        \n",
    "        if not collections:\n",
    "            print(\"   No collections found\")\n",
    "            print(\"   Process documents (Section 2) to create collections\")\n",
    "            return\n",
    "        \n",
    "        for col in collections:\n",
    "            try:\n",
    "                col_obj = client.get_collection(col.name)\n",
    "                count = col_obj.count()\n",
    "                print(f\"   â€¢ {col.name}: {count:,} chunks\")\n",
    "            except Exception as e:\n",
    "                print(f\"   â€¢ {col.name}: Error accessing ({str(e)[:50]})\")\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to list collections: {e}\")\n",
    "\n",
    "# Display all collections\n",
    "display_all_collections()\n",
    "\n",
    "# Verify selected collection if one exists\n",
    "if 'collection' in globals() and collection is not None:\n",
    "    print(\"\\nSelected Collection Status:\")\n",
    "    verify_collection_ready()\n",
    "else:\n",
    "    print(\"\\nNo collection selected yet\")\n",
    "    print(\"Run Section 3.2 to select a collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f212af5",
   "metadata": {
    "id": "4f212af5"
   },
   "source": [
    "## 4. RAG System Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e5f053",
   "metadata": {},
   "source": [
    "#### 4.1.1 Enhanced Collections Manager with BM25 Indexing\n",
    "\n",
    "Management layer around ChromaDB collections that adds an on-disk, cached BM25 index per collection and unified retrieval helpers (BM25 and semantic). It centralizes collection access, index creation/loading, tokenization logic and result formatting.\n",
    "\n",
    "Key components and behavior\n",
    "- Initialization\n",
    "  - Accepts a chromadb client and store directory.\n",
    "  - Prepares a bm25 storage directory and in-memory caches:\n",
    "    - self.bm25_indices: in-memory BM25 objects keyed by collection name\n",
    "    - self.collections_cache: cached chromadb.Collection objects\n",
    "\n",
    "- get_collection(name)\n",
    "  - Returns cached collection if present; otherwise attempts client.get_collection and caches the result.\n",
    "  - Defensive: returns None on failure.\n",
    "\n",
    "- get_available_collections()\n",
    "  - Returns collection names from client.list_collections(), safe-fails to [].\n",
    "\n",
    "- _get_index_path(collection_name)\n",
    "  - Deterministic path for storing the pickled BM25 index per collection.\n",
    "\n",
    "- _tokenize_documents(documents)\n",
    "  - Normalizes and tokenizes texts for BM25 (lowercase, removes non-alphanumeric/dots, filters short tokens).\n",
    "  - Encapsulates tokenization policy so BM25 behavior is consistent.\n",
    "\n",
    "- build_bm25_index(collection_name)\n",
    "  - Loads documents from the collection (via collection.get include=['documents']), tokenizes them, constructs BM25Okapi, caches it and persists it to disk with pickle.\n",
    "  - Returns success boolean and prints/logs failures.\n",
    "\n",
    "- load_bm25_index(collection_name)\n",
    "  - Loads a previously saved BM25 object from disk into the in-memory cache; returns boolean success.\n",
    "\n",
    "- search_bm25(query, collection_name, top_k)\n",
    "  - Ensures BM25 is available (load or build on demand).\n",
    "  - Tokenizes query with the same tokenizer, gets BM25 scores, selects top indices and fetches corresponding documents/metadatas from the collection.\n",
    "  - Returns a list of result dicts containing: text, metadata, bm25_score, collection and search_method.\n",
    "  - Skips items with non-positive score; defensive around missing collection/data.\n",
    "\n",
    "- semantic_search(query, collection_name, top_k)\n",
    "  - Uses the global embedding model if available to encode the query, otherwise queries ChromaDB with query_texts.\n",
    "  - Calls collection.query(include=['documents','metadatas','distances']) and converts distances to a semantic score (1 - distance).\n",
    "  - Returns formatted results with semantic_score, distance, metadata and collection.\n",
    "\n",
    "Design notes (concise)\n",
    "- Persistent BM25 indices: pickled to disk for reuse and loaded into memory on demand.\n",
    "- Consistent tokenization: ensures BM25 and query tokens share the same pre-processing rules.\n",
    "- Caching: reduces repeated ChromaDB calls and re-build overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bc09c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… EnhancedCollectionsManager ready\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Collections Manager\n",
    "class EnhancedCollectionsManager:\n",
    "    \"\"\"Manages ChromaDB collections with BM25 indexing\"\"\"\n",
    "    \n",
    "    def __init__(self, client: chromadb.Client, store_dir: Path):\n",
    "        self.client = client\n",
    "        self.store_dir = Path(store_dir)\n",
    "        self.bm25_dir = self.store_dir / \"bm25_indices\"\n",
    "        self.bm25_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Caches\n",
    "        self.bm25_indices: Dict[str, BM25Okapi] = {}\n",
    "        self.collections_cache: Dict[str, chromadb.Collection] = {}\n",
    "    \n",
    "    def get_collection(self, name: str) -> Optional[chromadb.Collection]:\n",
    "        \"\"\"Get collection from cache or client\"\"\"\n",
    "        if not name:\n",
    "            return None\n",
    "        if name not in self.collections_cache:\n",
    "            try:\n",
    "                self.collections_cache[name] = self.client.get_collection(name)\n",
    "            except:\n",
    "                return None\n",
    "        return self.collections_cache[name]\n",
    "    \n",
    "    def get_available_collections(self) -> List[str]:\n",
    "        \"\"\"List all collection names\"\"\"\n",
    "        try:\n",
    "            return [col.name for col in self.client.list_collections()]\n",
    "        except:\n",
    "            return []\n",
    "    \n",
    "    def _get_index_path(self, collection_name: str) -> Path:\n",
    "        \"\"\"Get BM25 index file path\"\"\"\n",
    "        return self.bm25_dir / f\"{collection_name}_bm25.pkl\"\n",
    "    \n",
    "    def _tokenize_documents(self, documents: List[str]) -> List[List[str]]:\n",
    "        \"\"\"Tokenize documents for BM25\"\"\"\n",
    "        tokenized = []\n",
    "        for doc in documents:\n",
    "            # Keep alphanumeric and dots, remove other punctuation\n",
    "            clean = re.sub(r\"[^0-9a-zA-Z\\.\\s]\", \" \", doc.lower())\n",
    "            tokens = [t for t in clean.split() if len(t) > 1]\n",
    "            tokenized.append(tokens)\n",
    "        return tokenized\n",
    "    \n",
    "    def build_bm25_index(self, collection_name: str) -> bool:\n",
    "        \"\"\"Build and save BM25 index for collection\"\"\"\n",
    "        collection = self.get_collection(collection_name)\n",
    "        if not collection:\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            # Get documents\n",
    "            data = collection.get(include=['documents'])\n",
    "            documents = data.get('documents', [])\n",
    "            if not documents:\n",
    "                return False\n",
    "            \n",
    "            # Tokenize and build index\n",
    "            tokenized = self._tokenize_documents(documents)\n",
    "            bm25 = BM25Okapi(tokenized, k1=1.2, b=0.75)\n",
    "            \n",
    "            # Cache and save\n",
    "            self.bm25_indices[collection_name] = bm25\n",
    "            with open(self._get_index_path(collection_name), 'wb') as f:\n",
    "                pickle.dump(bm25, f)\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to build BM25 index for {collection_name}: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def load_bm25_index(self, collection_name: str) -> bool:\n",
    "        \"\"\"Load pre-built BM25 index from disk\"\"\"\n",
    "        if collection_name in self.bm25_indices:\n",
    "            return True\n",
    "        \n",
    "        index_path = self._get_index_path(collection_name)\n",
    "        if not index_path.exists():\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            with open(index_path, 'rb') as f:\n",
    "                self.bm25_indices[collection_name] = pickle.load(f)\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def search_bm25(self, query: str, collection_name: str, top_k: int = 10) -> List[Dict]:\n",
    "        \"\"\"Search using BM25 index\"\"\"\n",
    "        # Ensure index is loaded\n",
    "        if collection_name not in self.bm25_indices:\n",
    "            if not self.load_bm25_index(collection_name):\n",
    "                if not self.build_bm25_index(collection_name):\n",
    "                    return []\n",
    "        \n",
    "        bm25 = self.bm25_indices[collection_name]\n",
    "        \n",
    "        # Tokenize query\n",
    "        query_tokens = [t for t in re.sub(r'[^\\w\\s\\.]', ' ', query.lower()).split() if len(t) > 1]\n",
    "        if not query_tokens:\n",
    "            return []\n",
    "        \n",
    "        # Get scores\n",
    "        scores = bm25.get_scores(query_tokens)\n",
    "        top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]\n",
    "        \n",
    "        # Get documents\n",
    "        collection = self.get_collection(collection_name)\n",
    "        if not collection:\n",
    "            return []\n",
    "        \n",
    "        data = collection.get(include=['documents', 'metadatas'])\n",
    "        documents = data.get('documents', [])\n",
    "        metadatas = data.get('metadatas', [])\n",
    "        \n",
    "        # Build results\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            if idx < len(documents) and scores[idx] > 0:\n",
    "                results.append({\n",
    "                    'text': documents[idx],\n",
    "                    'metadata': metadatas[idx] if idx < len(metadatas) else {},\n",
    "                    'bm25_score': float(scores[idx]),\n",
    "                    'collection': collection_name,\n",
    "                    'search_method': 'bm25'\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def semantic_search(self, query: str, collection_name: str, top_k: int = 10) -> List[Dict]:\n",
    "        \"\"\"Search using semantic similarity\"\"\"\n",
    "        collection = self.get_collection(collection_name)\n",
    "        if not collection:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # Use global embedding model if available\n",
    "            if 'embedding_model' in globals():\n",
    "                query_emb = embedding_model.encode([query])\n",
    "                results = collection.query(\n",
    "                    query_embeddings=query_emb.tolist(),\n",
    "                    n_results=top_k,\n",
    "                    include=['documents', 'metadatas', 'distances']\n",
    "                )\n",
    "            else:\n",
    "                results = collection.query(\n",
    "                    query_texts=[query],\n",
    "                    n_results=top_k,\n",
    "                    include=['documents', 'metadatas', 'distances']\n",
    "                )\n",
    "            \n",
    "            # Format results\n",
    "            formatted = []\n",
    "            docs = results.get('documents', [[]])[0]\n",
    "            metas = results.get('metadatas', [[]])[0]\n",
    "            dists = results.get('distances', [[]])[0]\n",
    "            \n",
    "            for i, doc in enumerate(docs):\n",
    "                dist = dists[i] if i < len(dists) else 1.0\n",
    "                formatted.append({\n",
    "                    'text': doc,\n",
    "                    'metadata': metas[i] if i < len(metas) else {},\n",
    "                    'semantic_score': 1.0 - dist,\n",
    "                    'distance': dist,\n",
    "                    'collection': collection_name,\n",
    "                    'search_method': 'semantic'\n",
    "                })\n",
    "            \n",
    "            return formatted\n",
    "        except Exception as e:\n",
    "            print(f\"Semantic search error: {e}\")\n",
    "            return []\n",
    "\n",
    "print(\"âœ… EnhancedCollectionsManager ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa970b1",
   "metadata": {},
   "source": [
    "#### 4.1.2 Initialize BM25 Index\n",
    "\n",
    "Instantiates the EnhancedCollectionsManager and verify BM25 index storage readiness for the running environment.\n",
    "\n",
    "- Guard: checks that a running Chroma client (\"client\") and STORE_DIR are available in globals(); otherwise prints a short error and does not proceed.\n",
    "- Initialization: creates collections_manager = EnhancedCollectionsManager(client, STORE_DIR). That constructor creates the bm25_indices directory (bm25_dir), initializes in-memory caches (bm25_indices, collections_cache) and prepares tokenization/indexing helpers defined earlier.\n",
    "- Status output: prints confirmation plus the path to the bm25 directory and the list of currently available collections (via collections_manager.get_available_collections()).\n",
    "- Side effects: ensures bm25 storage directory exists on disk and exposes the manager instance for downstream operations (build/load/search BM25 indices).\n",
    "\n",
    "Notes\n",
    "- This cell does not build indexes itself, it only prepares the manager and directory; index building/loading is handled by subsequent prebuilder code (BM25IndexPreBuilder) or manager methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a18182fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Collections Manager initialized\n",
      "   BM25 directory: /Users/rodrigograndy/Desktop/coding_projects/posts_website_drafts/rag_boe/chroma_store/bm25_indices\n",
      "   Collections: ['transcrips_barclasys', 'pra_rules']\n"
     ]
    }
   ],
   "source": [
    "# Initialize Collections Manager\n",
    "\n",
    "if 'client' in globals() and 'STORE_DIR' in globals():\n",
    "    collections_manager = EnhancedCollectionsManager(client, STORE_DIR)\n",
    "    print(\"âœ… Collections Manager initialized\")\n",
    "    print(f\"   BM25 directory: {collections_manager.bm25_dir}\")\n",
    "    print(f\"   Collections: {collections_manager.get_available_collections()}\")\n",
    "else:\n",
    "    print(\"âŒ Missing client or STORE_DIR - run Section 1 first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8d0794",
   "metadata": {},
   "source": [
    "#### 4.2.1 BM25 Index Building Function\n",
    "\n",
    "Pre-build or load BM25 indices for all ChromaDB collections via the EnhancedCollectionsManager.\n",
    "\n",
    "Key class: BM25IndexPreBuilder\n",
    "\n",
    "- check_index_status(collection_name) -> Dict\n",
    "  - Verifies the collection exists (via manager.get_collection).\n",
    "  - Computes the expected index file path (manager._get_index_path) and collection.count().\n",
    "  - Returns a status dict containing: collection name, document_count, index_path, and status ('missing' or 'exists').\n",
    "  - If the index file exists it also reports file size (MB) and last_modified timestamp.\n",
    "  - Used to decide whether to load, skip, or rebuild an index.\n",
    "\n",
    "- prebuild_all_indices(force_rebuild: bool = False) -> Dict\n",
    "  - Orchestrates index preparation for every collection returned by manager.get_available_collections().\n",
    "  - Short-circuits if no collections found (returns {'status': 'no_collections', 'results': {}}).\n",
    "  - For each collection:\n",
    "    - Calls check_index_status to inspect current state.\n",
    "    - If an index exists and force_rebuild is False: attempts to load it via manager.load_bm25_index and records action='loaded_existing'.\n",
    "    - Otherwise: builds index via manager.build_bm25_index and records action='built_new'.\n",
    "  - Uses tqdm for progress, tracks timing and counts successful builds/loads.\n",
    "  - Returns a summary dict with overall status, total_collections, successful count, total_time and per-collection results (boolean success + action)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3845543f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… BM25IndexPreBuilder ready\n"
     ]
    }
   ],
   "source": [
    "# BM25 Index Pre-Builder\n",
    "class BM25IndexPreBuilder:\n",
    "    \"\"\"Pre-build BM25 indices for all collections\"\"\"\n",
    "    \n",
    "    def __init__(self, manager: EnhancedCollectionsManager):\n",
    "        self.manager = manager\n",
    "    \n",
    "    def check_index_status(self, collection_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Check if index exists and get stats\"\"\"\n",
    "        collection = self.manager.get_collection(collection_name)\n",
    "        if not collection:\n",
    "            return {'status': 'collection_not_found'}\n",
    "        \n",
    "        index_path = self.manager._get_index_path(collection_name)\n",
    "        document_count = collection.count()\n",
    "        \n",
    "        result = {\n",
    "            'collection': collection_name,\n",
    "            'document_count': document_count,\n",
    "            'index_path': str(index_path),\n",
    "            'status': 'missing'\n",
    "        }\n",
    "        \n",
    "        if index_path.exists():\n",
    "            stat = index_path.stat()\n",
    "            result.update({\n",
    "                'status': 'exists',\n",
    "                'index_size_mb': stat.st_size / (1024 * 1024),\n",
    "                'last_modified': time.strftime('%Y-%m-%d %H:%M:%S', \n",
    "                                              time.localtime(stat.st_mtime))\n",
    "            })\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def prebuild_all_indices(self, force_rebuild: bool = False) -> Dict[str, Any]:\n",
    "        \"\"\"Build indices for all collections\"\"\"\n",
    "        collections = self.manager.get_available_collections()\n",
    "        \n",
    "        if not collections:\n",
    "            return {'status': 'no_collections', 'results': {}}\n",
    "        \n",
    "        print(f\"Building BM25 indices for {len(collections)} collections...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        results = {}\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for collection_name in tqdm(collections, desc=\"Building indices\"):\n",
    "            # Check if rebuild needed\n",
    "            status = self.check_index_status(collection_name)\n",
    "            \n",
    "            if status['status'] == 'exists' and not force_rebuild:\n",
    "                results[collection_name] = {\n",
    "                    'action': 'loaded_existing',\n",
    "                    'success': self.manager.load_bm25_index(collection_name)\n",
    "                }\n",
    "            else:\n",
    "                results[collection_name] = {\n",
    "                    'action': 'built_new',\n",
    "                    'success': self.manager.build_bm25_index(collection_name)\n",
    "                }\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        successful = sum(1 for r in results.values() if r['success'])\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(f\"âœ… Complete: {successful}/{len(collections)} indices ready\")\n",
    "        print(f\"â±ï¸  Total time: {total_time:.1f}s\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return {\n",
    "            'status': 'complete',\n",
    "            'total_collections': len(collections),\n",
    "            'successful': successful,\n",
    "            'total_time': total_time,\n",
    "            'results': results\n",
    "        }\n",
    "\n",
    "print(\"âœ… BM25IndexPreBuilder ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eae50e9",
   "metadata": {},
   "source": [
    "#### 4.2.2 Initialize BM25 Index\n",
    "\n",
    "Prepares the BM25 pre-builder utility so index building/loading can be triggered later.\n",
    "\n",
    "- Checks that collections_manager exists in globals() (requires Chroma client + STORE_DIR initialized).\n",
    "- Instantiates bm25_prebuilder = BM25IndexPreBuilder(collections_manager).\n",
    "\n",
    "- Side effects / observable output\n",
    "  - Ensures the pre-builder is available as a global object for downstream use.\n",
    "  - Prints confirmation, the bm25 storage directory path (collections_manager.bm25_dir), and short usage hints:\n",
    "    - bm25_prebuilder.prebuild_all_indices()\n",
    "    - bm25_prebuilder.prebuild_all_indices(force_rebuild=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7be6832c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… BM25 Pre-Builder initialized\n",
      "   Storage: /Users/rodrigograndy/Desktop/coding_projects/posts_website_drafts/rag_boe/chroma_store/bm25_indices\n",
      "\n",
      "Usage:\n",
      "  â€¢ bm25_prebuilder.prebuild_all_indices()\n",
      "  â€¢ bm25_prebuilder.prebuild_all_indices(force_rebuild=True)\n"
     ]
    }
   ],
   "source": [
    "# Initialize Pre-Builder\n",
    "\n",
    "if 'collections_manager' in globals():\n",
    "    bm25_prebuilder = BM25IndexPreBuilder(collections_manager)\n",
    "    print(\"âœ… BM25 Pre-Builder initialized\")\n",
    "    print(f\"   Storage: {collections_manager.bm25_dir}\")\n",
    "    print(\"\\nUsage:\")\n",
    "    print(\"  â€¢ bm25_prebuilder.prebuild_all_indices()\")\n",
    "    print(\"  â€¢ bm25_prebuilder.prebuild_all_indices(force_rebuild=True)\")\n",
    "else:\n",
    "    print(\"âŒ Collections manager not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "face109c",
   "metadata": {},
   "source": [
    "#### 4.2.3 BM25 Index Version Check\n",
    "\n",
    "Provides a lightweight health/version/status checker for BM25 indices managed by BM25IndexPreBuilder.\n",
    "\n",
    "Key class: BM25IndexChecker\n",
    "\n",
    "- check_all_indices() -> Dict[str, Any]\n",
    "  - Iterates all collection names from prebuilder.manager.get_available_collections().\n",
    "  - Calls prebuilder.check_index_status(collection_name) for each collection and aggregates results.\n",
    "  - Produces and returns a summary dict with:\n",
    "    - total (number of collections),\n",
    "    - ready (count of indices with status 'exists'),\n",
    "    - missing (count of indices not present),\n",
    "    - details (per-collection status dicts returned by check_index_status).\n",
    "  - Does not modify on-disk indices; purely diagnostic.\n",
    "\n",
    "Initialization behavior\n",
    "- If bm25_prebuilder is present in globals(), the notebook creates index_checker = BM25IndexChecker(bm25_prebuilder) and prints readiness.\n",
    "- If prebuilder is missing, it prints an error and does not create the checker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b000e75",
   "metadata": {
    "id": "2b000e75",
    "outputId": "ceec0d56-dea8-4691-9c6e-54abeeef7998"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index Checker ready\n"
     ]
    }
   ],
   "source": [
    "# Section 4.2.3: Index Status Checker\n",
    "class BM25IndexChecker:\n",
    "    \"\"\"Check BM25 index health and version\"\"\"\n",
    "    \n",
    "    def __init__(self, prebuilder: BM25IndexPreBuilder):\n",
    "        self.prebuilder = prebuilder\n",
    "    \n",
    "    def check_all_indices(self) -> Dict[str, Any]:\n",
    "        \"\"\"Check status of all indices\"\"\"\n",
    "        collections = self.prebuilder.manager.get_available_collections()\n",
    "        \n",
    "        summary = {\n",
    "            'total': len(collections),\n",
    "            'ready': 0,\n",
    "            'missing': 0,\n",
    "            'details': {}\n",
    "        }\n",
    "        \n",
    "        for collection_name in collections:\n",
    "            status = self.prebuilder.check_index_status(collection_name)\n",
    "            summary['details'][collection_name] = status\n",
    "            \n",
    "            if status['status'] == 'exists':\n",
    "                summary['ready'] += 1\n",
    "            else:\n",
    "                summary['missing'] += 1\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def print_status(self):\n",
    "        \"\"\"Print formatted status report\"\"\"\n",
    "        summary = self.check_all_indices()\n",
    "        \n",
    "        print(\"\\nBM25 Index Status\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Total collections: {summary['total']}\")\n",
    "        print(f\"Indices ready: {summary['ready']}\")\n",
    "        print(f\"Indices missing: {summary['missing']}\")\n",
    "        \n",
    "        if summary['details']:\n",
    "            print(\"\\nDetails:\")\n",
    "            for name, status in summary['details'].items():\n",
    "                if status['status'] == 'exists':\n",
    "                    size = status.get('index_size_mb', 0)\n",
    "                    print(f\"  {name}: {size:.2f} MB\")\n",
    "                else:\n",
    "                    print(f\"  {name}: missing\")\n",
    "\n",
    "# Initialize checker\n",
    "if 'bm25_prebuilder' in globals():\n",
    "    index_checker = BM25IndexChecker(bm25_prebuilder)\n",
    "    print(\"Index Checker ready\")\n",
    "else:\n",
    "    print(\"Pre-builder not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80fb7dc",
   "metadata": {},
   "source": [
    "#### 4.2.4 Execute BM25 Index building\n",
    "\n",
    "Orchestrates the on-demand pre-building/loading of BM25 indices for all collections using the BM25IndexPreBuilder and reports high-level progress/results.\n",
    "\n",
    "- Trigger build:\n",
    "  - Calls bm25_prebuilder.prebuild_all_indices(force_rebuild=False).\n",
    "    - This method iterates available collections, calls BM25IndexPreBuilder.check_index_status for each, and either loads an existing on-disk index (manager.load_bm25_index) or builds a new BM25 index (manager.build_bm25_index). It returns a summary dict with per-collection outcomes and overall counts/timing.\n",
    "\n",
    "- Post-run reporting:\n",
    "  - Inspects the returned summary (results) and prints a final success message when all indices succeeded, otherwise prints a warning showing how many indices were built vs total.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11d87d95",
   "metadata": {
    "id": "11d87d95",
    "outputId": "2aaf3e4f-524e-4cfc-ee03-92b61ef9a283"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting BM25 index pre-building...\n",
      "\n",
      "BM25 Index Status\n",
      "============================================================\n",
      "Total collections: 2\n",
      "Indices ready: 2\n",
      "Indices missing: 0\n",
      "\n",
      "Details:\n",
      "  transcrips_barclasys: 3.70 MB\n",
      "  pra_rules: 1.63 MB\n",
      "Building BM25 indices for 2 collections...\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6528eb58b6eb4b919899ae7245bb7ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building indices:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "âœ… Complete: 2/2 indices ready\n",
      "â±ï¸  Total time: 0.1s\n",
      "============================================================\n",
      "\n",
      "âœ… All indices ready - system optimized!\n"
     ]
    }
   ],
   "source": [
    "# Execute Index Building\n",
    "if 'bm25_prebuilder' in globals():\n",
    "    print(\"ðŸš€ Starting BM25 index pre-building...\")\n",
    "    \n",
    "    # Check current status\n",
    "    if 'index_checker' in globals():\n",
    "        index_checker.print_status()\n",
    "    \n",
    "    # Build indices\n",
    "    results = bm25_prebuilder.prebuild_all_indices(\n",
    "        force_rebuild=False  # Change to True to force rebuild\n",
    "    )\n",
    "    \n",
    "    # Show final status\n",
    "    if results['successful'] == results['total_collections']:\n",
    "        print(\"\\nâœ… All indices ready - system optimized!\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸ {results['successful']}/{results['total_collections']} indices built\")\n",
    "else:\n",
    "    print(\"âŒ Pre-builder not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450f78fa",
   "metadata": {},
   "source": [
    "#### 4.3.1 Score Normalization for Hybrid Search Engine with RRF\n",
    "\n",
    "Provides deterministic, reusable normalization routines so BM25 and semantic scores are on comparable scales before fusion (used by RRFFusionEngine._add_normalized_scores).\n",
    "\n",
    "- normalize_bm25(scores: List[float]) -> List[float]: Compress extreme BM25 values and scale into [0,1] for robust fusion.\n",
    "    - Early-return if input empty or all zeros.\n",
    "    - Compute 95th percentile (clip_val) and clip scores to [0, clip_val].\n",
    "    - If clip_val > 0, divide clipped values by clip_val to produce normalized scores in [0,1]; otherwise return zeros.\n",
    "  - Notes: percentile-based clipping limits the influence of outliers while preserving relative ordering below the clip threshold.\n",
    "\n",
    "- normalize_semantic(scores: List[float]) -> List[float]: Convert semantic similarity/distance values into a smooth [0,1] score distribution suitable for combination with BM25.\n",
    "    - Early-return if input empty or all zeros.\n",
    "    - Compute mean and standard deviation (with small epsilon to avoid div-by-zero).\n",
    "    - Convert to z-scores, scale z by factor 2, then map through a sigmoid 1/(1+exp(-z*2)) to push values into (0,1).\n",
    "  - Notes: Z-score + sigmoid centers and compresses the semantic distribution, handling both similarity scores and distance-derived inputs; small std epsilon stabilizes near-constant inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2598880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ScoreNormalizer ready\n"
     ]
    }
   ],
   "source": [
    "# Score Normalization (Standalone utility)\n",
    "class ScoreNormalizer:\n",
    "    \"\"\"Normalize BM25 and semantic scores for fair fusion\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_bm25(scores: List[float]) -> List[float]:\n",
    "        \"\"\"Clip and normalize BM25 scores (percentile-based)\"\"\"\n",
    "        if not scores or all(s == 0 for s in scores):\n",
    "            return scores\n",
    "        \n",
    "        arr = np.array(scores)\n",
    "        clip_val = np.percentile(arr, 95)\n",
    "        clipped = np.clip(arr, 0, clip_val)\n",
    "        \n",
    "        if clip_val > 0:\n",
    "            return (clipped / clip_val).tolist()\n",
    "        return [0.0] * len(scores)\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_semantic(scores: List[float]) -> List[float]:\n",
    "        \"\"\"Normalize semantic scores using sigmoid (handles distance-based scores)\"\"\"\n",
    "        if not scores or all(s == 0 for s in scores):\n",
    "            return scores\n",
    "        \n",
    "        arr = np.array(scores)\n",
    "        mean = np.mean(arr)\n",
    "        std = np.std(arr) + 1e-8\n",
    "        \n",
    "        z_scores = (arr - mean) / std\n",
    "        normalized = 1 / (1 + np.exp(-z_scores * 2))\n",
    "        return normalized.tolist()\n",
    "\n",
    "print(\"âœ… ScoreNormalizer ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b149d58a",
   "metadata": {},
   "source": [
    "#### 4.3.2 Hybrid Search (BM25 + Embeddings)\n",
    "\n",
    "Orchestrates retrieval from two modalities (BM25 and semantic/embedding search) and fuses them with an enhanced Reciprocal Rank Fusion (RRF) implementation.\n",
    "\n",
    "- RRFFusionEngine: combines two ranked result lists (semantic, BM25) into a single, scored ranking using RRF plus simple confidence blending and cross-method boosting.\n",
    "  - Important state: params dict (k, semantic_weight, bm25_weight, cross_boost, decay_factor) controls RRF formula, relative weighting, boosting for docs found in both sets, and exponential decay per rank.\n",
    "  - Methods:\n",
    "    - fuse(semantic_results, bm25_results) -> List[Dict]: Filters out entries with empty text.\n",
    "      - Calls _add_normalized_scores on both lists (uses ScoreNormalizer).\n",
    "      - Iterates semantic results, assigns initial RRF contributions by rank; then iterates BM25 results, adds RRF contributions and applies cross_boost when doc already present.\n",
    "      - Builds final result dicts that include rrf_score, a computed confidence (mean of normalized factors when present), and final_score = rrf_score + (confidence * 0.05). Sorts by final_score descending.\n",
    "    - _add_normalized_scores(results, score_type, normalizer) -> results\n",
    "      - Extracts raw scores, uses ScoreNormalizer.normalize_bm25 or .normalize_semantic to add e.g. 'semantic_score_normalized' or 'bm25_score_normalized' fields to each result.\n",
    "    - _calculate_rrf(rank, weight) -> float\n",
    "      - Implements RRF contribution with decay: (weight / (rank + 1 + k)) * decay_factor**rank.\n",
    "    - _get_doc_id(result) -> str\n",
    "      - Stable doc id extraction: prefers result['id'] then metadata.chunk_id, otherwise fallback hash(filename+text snippet).\n",
    "\n",
    "\n",
    "- HybridSearchEngine: fetches candidate lists from BM25 and semantic backends, call fusion engine, provide single-collection and multi-collection search entry points.\n",
    "  - Methods:\n",
    "    - search(query, collection_name, top_k=20) -> List[Dict]\n",
    "      - Ensures BM25 index is loaded (loads/builds via collections_manager as needed).\n",
    "      - Expands retrieval depth (expanded_k = int(top_k * 2.5)) to collect richer candidate pools.\n",
    "      - Calls collections_manager.search_bm25 and .semantic_search with expanded_k.\n",
    "      - Calls RRFFusionEngine.fuse(semantic_results, bm25_results) and returns top_k of fused results.\n",
    "      - Defensive: returns [] if both retrievals are empty.\n",
    "    - search_multi(query, collection_names, top_k=10) -> List[Dict]\n",
    "      - Runs search over each collection (calling search), tags results with source_collection, aggregates.\n",
    "      - Applies _promote_diversity when searching multiple collections, re-sorts and truncates to requested volume.\n",
    "    - _promote_diversity(results, collections) -> List[Dict]\n",
    "      - Simple per-collection boosting to favor under-represented collections (adds/subtracts small constants to final_score based on current counts).\n",
    "  - Interaction: relies on collections_manager for tokenization, BM25 index management, and Chroma semantic queries; relies on ScoreNormalizer via RRFFusionEngine for comparable scores.\n",
    "\n",
    "Operational/edge behaviors\n",
    "- Normalization: semantic and BM25 scores are normalized before fusion (ScoreNormalizer percentile clipping + sigmoid z-score).\n",
    "- Candidate expansion: uses a larger retrieval window to let fusion and reranking identify cross-method matches.\n",
    "- Robustness: filters empty texts, loads or builds BM25 indices on demand, and uses simple fallbacks for stable IDs.\n",
    "\n",
    "Result fields produced (typical)\n",
    "- final_score, rrf_score, confidence, semantic_rank, bm25_rank, in_both_sets, fusion_method, plus original result fields and normalized score fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "801ef757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… RRFFusionEngine ready\n"
     ]
    }
   ],
   "source": [
    "# RRF Fusion Engine\n",
    "class RRFFusionEngine:\n",
    "    \"\"\"Reciprocal Rank Fusion with enhanced scoring\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.params = {\n",
    "            'k': 60,\n",
    "            'semantic_weight': 0.6,\n",
    "            'bm25_weight': 0.4,\n",
    "            'cross_boost': 1.15,\n",
    "            'decay_factor': 0.95\n",
    "        }\n",
    "    \n",
    "    def fuse(self, semantic_results: List[Dict], bm25_results: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Fuse semantic and BM25 results using RRF\"\"\"\n",
    "        # Validate inputs\n",
    "        semantic_results = [r for r in semantic_results if r.get('text')]\n",
    "        bm25_results = [r for r in bm25_results if r.get('text')]\n",
    "        \n",
    "        if not semantic_results and not bm25_results:\n",
    "            return []\n",
    "        \n",
    "        # Normalize scores\n",
    "        normalizer = ScoreNormalizer()\n",
    "        semantic_results = self._add_normalized_scores(semantic_results, 'semantic', normalizer)\n",
    "        bm25_results = self._add_normalized_scores(bm25_results, 'bm25', normalizer)\n",
    "        \n",
    "        # Build combined scores\n",
    "        combined = {}\n",
    "        \n",
    "        # Process semantic results\n",
    "        for rank, result in enumerate(semantic_results):\n",
    "            doc_id = self._get_doc_id(result)\n",
    "            rrf_score = self._calculate_rrf(rank, self.params['semantic_weight'])\n",
    "            \n",
    "            combined[doc_id] = {\n",
    "                'result': result,\n",
    "                'semantic_rank': rank + 1,\n",
    "                'bm25_rank': None,\n",
    "                'rrf_score': rrf_score,\n",
    "                'in_both': False\n",
    "            }\n",
    "        \n",
    "        # Process BM25 results\n",
    "        for rank, result in enumerate(bm25_results):\n",
    "            doc_id = self._get_doc_id(result)\n",
    "            rrf_score = self._calculate_rrf(rank, self.params['bm25_weight'])\n",
    "            \n",
    "            if doc_id in combined:\n",
    "                # Document in both sets - boost score\n",
    "                combined[doc_id]['rrf_score'] += rrf_score\n",
    "                combined[doc_id]['rrf_score'] *= self.params['cross_boost']\n",
    "                combined[doc_id]['bm25_rank'] = rank + 1\n",
    "                combined[doc_id]['in_both'] = True\n",
    "            else:\n",
    "                combined[doc_id] = {\n",
    "                    'result': result,\n",
    "                    'semantic_rank': None,\n",
    "                    'bm25_rank': rank + 1,\n",
    "                    'rrf_score': rrf_score,\n",
    "                    'in_both': False\n",
    "                }\n",
    "        \n",
    "        # Build final results\n",
    "        final_results = []\n",
    "        for doc_id, data in combined.items():\n",
    "            result = data['result'].copy()\n",
    "            \n",
    "            # Calculate confidence\n",
    "            factors = []\n",
    "            if result.get('semantic_score_normalized'):\n",
    "                factors.append(result['semantic_score_normalized'])\n",
    "            if result.get('bm25_score_normalized'):\n",
    "                factors.append(result['bm25_score_normalized'])\n",
    "            \n",
    "            confidence = np.mean(factors) if factors else 0.5\n",
    "            \n",
    "            # Final score\n",
    "            final_score = data['rrf_score'] + (confidence * 0.05)\n",
    "            \n",
    "            result.update({\n",
    "                'final_score': final_score,\n",
    "                'rrf_score': data['rrf_score'],\n",
    "                'confidence': confidence,\n",
    "                'semantic_rank': data['semantic_rank'],\n",
    "                'bm25_rank': data['bm25_rank'],\n",
    "                'in_both_sets': data['in_both'],\n",
    "                'fusion_method': 'Enhanced_RRF'\n",
    "            })\n",
    "            \n",
    "            final_results.append(result)\n",
    "        \n",
    "        # Sort by final score\n",
    "        final_results.sort(key=lambda x: x['final_score'], reverse=True)\n",
    "        return final_results\n",
    "    \n",
    "    def _add_normalized_scores(self, results: List[Dict], score_type: str, normalizer: ScoreNormalizer) -> List[Dict]:\n",
    "        \"\"\"Add normalized scores to results\"\"\"\n",
    "        scores = [r.get(f'{score_type}_score', 0) for r in results]\n",
    "        \n",
    "        if score_type == 'bm25':\n",
    "            normalized = normalizer.normalize_bm25(scores)\n",
    "        else:\n",
    "            normalized = normalizer.normalize_semantic(scores)\n",
    "        \n",
    "        for i, result in enumerate(results):\n",
    "            result[f'{score_type}_score_normalized'] = normalized[i]\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _calculate_rrf(self, rank: int, weight: float) -> float:\n",
    "        \"\"\"Calculate RRF score with decay\"\"\"\n",
    "        k = self.params['k']\n",
    "        decay = self.params['decay_factor'] ** rank\n",
    "        return (weight / (rank + 1 + k)) * decay\n",
    "    \n",
    "    def _get_doc_id(self, result: Dict) -> str:\n",
    "        \"\"\"Get consistent document ID\"\"\"\n",
    "        if result.get('id'):\n",
    "            return result['id']\n",
    "        \n",
    "        chunk_id = result.get('metadata', {}).get('chunk_id')\n",
    "        if chunk_id:\n",
    "            return chunk_id\n",
    "        \n",
    "        # Fallback: hash of text + filename\n",
    "        text = result.get('text', '')[:100]\n",
    "        filename = result.get('metadata', {}).get('filename', 'unknown')\n",
    "        return f\"doc_{hash(f'{filename}_{text}')}\"\n",
    "\n",
    "print(\"âœ… RRFFusionEngine ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b51298",
   "metadata": {},
   "source": [
    "#### 4.3.3 Hybrid Search Engine (Orchestrator)\n",
    "\n",
    "Orchestrates hybrid retrieval by calling BM25 and semantic backends (via the collections manager), expanding candidate pools, and fusing results with the RRFFusionEngine to produce a single ranked result list.\n",
    "\n",
    "- search(query, collection_name, top_k=20) -> List[Dict]: Ensures a BM25 index is present for the collection: attempts to load from cache, otherwise triggers build.\n",
    "  - Expands retrieval depth (expanded_k = int(top_k * 2.5)) to collect larger candidate sets for both modalities.\n",
    "  - Calls collections_manager.search_bm25(...) and collections_manager.semantic_search(...) with expanded_k.\n",
    "  - If both retrievals are empty returns [].\n",
    "  - Passes results to fusion_engine.fuse(semantic_results, bm25_results) and returns the top_k fused entries.\n",
    "  - Defensive: handles index loading/building on demand and avoids failing when one modality yields no results.\n",
    "\n",
    "- search_multi(query, collection_names, top_k=10) -> List[Dict]\n",
    "  - Iterates over collection_names, calling search(...) for each collection.\n",
    "  - Tags each returned result with source_collection.\n",
    "  - Aggregates per-collection results, applies diversity promotion when multiple collections are involved, re-sorts by final_score and truncates to a sensible output size (top_k * len(collection_names)).\n",
    "  - Catches and logs per-collection exceptions to allow partial results.\n",
    "\n",
    "- _promote_diversity(results, collections) -> List[Dict]\n",
    "  - Simple post-hoc balancing mechanism: computes per-collection counts and target_per_collection, then nudges final_score with small positive boosts for under-represented collections and small penalties for over-represented ones (constants used: +0.05 / -0.02).\n",
    "  - Mutates final_score directly to favor distributional diversity across collections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86edb69a",
   "metadata": {
    "id": "86edb69a",
    "outputId": "900887e9-b7ce-4126-ee73-5d796cab4cc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… HybridSearchEngine ready\n"
     ]
    }
   ],
   "source": [
    "# Hybrid Search Engine (Orchestrator)\n",
    "class HybridSearchEngine:\n",
    "    \"\"\"Main hybrid search orchestrator - uses collections manager for BM25/semantic search\"\"\"\n",
    "    \n",
    "    def __init__(self, collections_manager):\n",
    "        self.manager = collections_manager\n",
    "        self.fusion_engine = RRFFusionEngine()\n",
    "    \n",
    "    def search(self, query: str, collection_name: str, top_k: int = 20) -> List[Dict]:\n",
    "        \"\"\"Perform hybrid search on single collection\"\"\"\n",
    "        # Ensure BM25 index loaded\n",
    "        if collection_name not in self.manager.bm25_indices:\n",
    "            if not self.manager.load_bm25_index(collection_name):\n",
    "                self.manager.build_bm25_index(collection_name)\n",
    "        \n",
    "        # Expand retrieval for better fusion\n",
    "        expanded_k = int(top_k * 2.5)\n",
    "        \n",
    "        # Get results from both methods\n",
    "        bm25_results = self.manager.search_bm25(query, collection_name, expanded_k)\n",
    "        semantic_results = self.manager.semantic_search(query, collection_name, expanded_k)\n",
    "        \n",
    "        if not bm25_results and not semantic_results:\n",
    "            return []\n",
    "        \n",
    "        # Fuse results\n",
    "        fused = self.fusion_engine.fuse(semantic_results, bm25_results)\n",
    "        return fused[:top_k]\n",
    "    \n",
    "    def search_multi(self, query: str, collection_names: List[str], top_k: int = 10) -> List[Dict]:\n",
    "        \"\"\"Search across multiple collections\"\"\"\n",
    "        all_results = []\n",
    "        \n",
    "        for collection_name in collection_names:\n",
    "            try:\n",
    "                results = self.search(query, collection_name, top_k)\n",
    "                \n",
    "                # Tag with source collection\n",
    "                for result in results:\n",
    "                    result['source_collection'] = collection_name\n",
    "                \n",
    "                all_results.extend(results)\n",
    "            except Exception as e:\n",
    "                print(f\"Error searching {collection_name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Apply diversity if multi-collection\n",
    "        if len(collection_names) > 1:\n",
    "            all_results = self._promote_diversity(all_results, collection_names)\n",
    "        \n",
    "        # Re-sort and limit\n",
    "        all_results.sort(key=lambda x: x.get('final_score', 0), reverse=True)\n",
    "        return all_results[:top_k * len(collection_names)]\n",
    "    \n",
    "    def _promote_diversity(self, results: List[Dict], collections: List[str]) -> List[Dict]:\n",
    "        \"\"\"Promote collection diversity in results\"\"\"\n",
    "        collection_counts = {name: 0 for name in collections}\n",
    "        target_per_collection = len(results) / len(collections)\n",
    "        \n",
    "        for result in results:\n",
    "            collection = result.get('source_collection', 'unknown')\n",
    "            if collection not in collection_counts:\n",
    "                continue\n",
    "            \n",
    "            count = collection_counts[collection]\n",
    "            \n",
    "            # Boost under-represented collections\n",
    "            if count < target_per_collection:\n",
    "                result['final_score'] = result.get('final_score', 0) + 0.05\n",
    "            else:\n",
    "                result['final_score'] = result.get('final_score', 0) - 0.02\n",
    "            \n",
    "            collection_counts[collection] += 1\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"âœ… HybridSearchEngine ready\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a0b498",
   "metadata": {},
   "source": [
    "#### 4.3.4 Integration and Verification \n",
    "\n",
    "Wires the HybridSearchEngine into the running system and verify integration points so downstream orchestration uses the fused BM25+semantic search.\n",
    "- Instantiates hybrid_search = HybridSearchEngine(collections_manager).\n",
    "- Updates existing orchestrator objects by attaching the new hybrid search instance:\n",
    "  - If rag_orchestrator exists, sets rag_orchestrator.hybrid_search = hybrid_search and prints confirmation.\n",
    "  - If spec_rag exists, sets spec_rag.hybrid_search = hybrid_search and prints confirmation.\n",
    "- Emits a summary printout confirming initialization and high-level features (BM25+semantic fusion, normalization, cross-boost, multi-collection diversity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5842ed43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Hybrid Search Engine initialized!\n",
      "   â€¢ BM25 + Semantic fusion with RRF\n",
      "   â€¢ Score normalization (percentile + sigmoid)\n",
      "   â€¢ Cross-method boost for documents in both sets\n",
      "   â€¢ Multi-collection search with diversity\n"
     ]
    }
   ],
   "source": [
    "# Initialize Hybrid Search Engine\n",
    "if 'collections_manager' in globals():\n",
    "    hybrid_search = HybridSearchEngine(collections_manager)\n",
    "    \n",
    "    # Update global references\n",
    "    if 'rag_orchestrator' in globals():\n",
    "        rag_orchestrator.hybrid_search = hybrid_search\n",
    "        print(\"âœ… Updated rag_orchestrator\")\n",
    "    \n",
    "    if 'spec_rag' in globals():\n",
    "        spec_rag.hybrid_search = hybrid_search\n",
    "        print(\"âœ… Updated spec_rag\")\n",
    "    \n",
    "    print(\"\\nâœ… Hybrid Search Engine initialized!\")\n",
    "    print(\"   â€¢ BM25 + Semantic fusion with RRF\")\n",
    "    print(\"   â€¢ Score normalization (percentile + sigmoid)\")\n",
    "    print(\"   â€¢ Cross-method boost for documents in both sets\")\n",
    "    print(\"   â€¢ Multi-collection search with diversity\")\n",
    "else:\n",
    "    print(\"âŒ collections_manager not available - run Section 4.1 first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb478dc2",
   "metadata": {},
   "source": [
    "#### 4.4 Cross-Encoder Reranking \n",
    "\n",
    "Provides a required crossâ€‘encoder component that scores query/document pairs and reorders fused search results to improve final relevance.\n",
    "\n",
    "- rerank(query: str, results: List[Dict], top_k: int = 10) -> List[Dict]\n",
    "  - Inputs: original query string and a list of candidate result dicts (each expected to contain 'text' and optional existing scores).\n",
    "  - Prepares (query, document_text) pairs, truncating document text to ~512 chars for efficiency.\n",
    "  - Calls the cross-encoder model to predict a scalar relevance score per pair.\n",
    "  - Side effects on each result dict:\n",
    "    - Adds 'rerank_score' (float).\n",
    "    - Updates 'final_score' by blending existing final_score (if present) and rerank_score with weights 60% original / 40% rerank; if no existing final_score, sets final_score = rerank_score.\n",
    "  - Returns results sorted by the blended final_score, truncated to top_k.\n",
    "  - Operates in-place (mutates provided result objects).\n",
    "\n",
    "Operational notes\n",
    "- Default cross-encoder model: cross-encoder/ms-marco-MiniLM-L-6-v2.\n",
    "- Designed for efficiency: truncation of document text and batch prediction via CrossEncoder.predict.\n",
    "- Integration behavior: the cell treats the cross-encoder as REQUIRED â€” initialization failures are escalated so downstream components expect reranking to be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "333a8602",
   "metadata": {
    "id": "333a8602",
    "outputId": "7bd3be1a-f905-4dea-b2c7-d988b0b2b4d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Cross-encoder loaded: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "\n",
      "âœ… Cross-Encoder Reranking initialized!\n",
      "   â€¢ Model: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "   â€¢ Blends scores: 60% original + 40% rerank\n",
      "   â€¢ Status: REQUIRED component (system will not work without it)\n"
     ]
    }
   ],
   "source": [
    "# Cross-Encoder Reranking\n",
    "class CrossEncoderReranker:\n",
    "    \"\"\"Cross-encoder reranking for improved relevance - REQUIRED component\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"):\n",
    "        try:\n",
    "            from sentence_transformers import CrossEncoder\n",
    "            self.model = CrossEncoder(model_name)\n",
    "            self.available = True\n",
    "            print(f\"âœ… Cross-encoder loaded: {model_name}\")\n",
    "        except ImportError as e:\n",
    "            raise ImportError(\n",
    "                \"Cross-encoder unavailable: sentence-transformers not installed.\\n\"\n",
    "                \"Install with: pip install sentence-transformers\\n\"\n",
    "                f\"   Error: {e}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                f\"Failed to load cross-encoder model: {model_name}\\n\"\n",
    "                f\"Error: {e}\\n\"\n",
    "                \"Ensure model is available or check network connection.\"\n",
    "            )\n",
    "    \n",
    "    def rerank(self, query: str, results: List[Dict], top_k: int = 10) -> List[Dict]:\n",
    "        \"\"\"Rerank results using cross-encoder scores\"\"\"\n",
    "        if not results:\n",
    "            return []\n",
    "        \n",
    "        # Prepare query-document pairs (limit text for efficiency)\n",
    "        pairs = [(query, result.get('text', '')[:512]) for result in results]\n",
    "        \n",
    "        # Get cross-encoder scores\n",
    "        scores = self.model.predict(pairs)\n",
    "        \n",
    "        # Add scores to results\n",
    "        for i, result in enumerate(results):\n",
    "            result['rerank_score'] = float(scores[i])\n",
    "            \n",
    "            # Blend with existing score if present\n",
    "            if 'final_score' in result:\n",
    "                result['final_score'] = (\n",
    "                    result['final_score'] * 0.6 +\n",
    "                    result['rerank_score'] * 0.4\n",
    "                )\n",
    "            else:\n",
    "                result['final_score'] = result['rerank_score']\n",
    "        \n",
    "        # Sort by blended score\n",
    "        reranked = sorted(results, key=lambda x: x.get('final_score', 0), reverse=True)\n",
    "        return reranked[:top_k]\n",
    "\n",
    "# Initialize cross-encoder (MANDATORY - will raise exception if unavailable)\n",
    "try:\n",
    "    cross_encoder = CrossEncoderReranker()\n",
    "    \n",
    "    print(\"\\nâœ… Cross-Encoder Reranking initialized!\")\n",
    "    print(\"   â€¢ Model: cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "    print(\"   â€¢ Blends scores: 60% original + 40% rerank\")\n",
    "    print(\"   â€¢ Status: REQUIRED component (system will not work without it)\")\n",
    "    \n",
    "except (ImportError, RuntimeError) as e:\n",
    "    print(f\"\\n{str(e)}\")\n",
    "    print(\"\\nCRITICAL: Cross-encoder is a REQUIRED component.\")\n",
    "    print(\"The RAG system cannot function without reranking capability.\")\n",
    "    print(\"Please install dependencies and re-run this section.\")\n",
    "    cross_encoder = None\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b2f351",
   "metadata": {
    "id": "01b2f351"
   },
   "source": [
    "## 5. Intelligent Query Processing and LLM Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a535cd",
   "metadata": {},
   "source": [
    "#### 5.2 Query Classification and Routing \n",
    "\n",
    "LLM-first classifier that converts free-form queries into a small, validated routing object. Its main responsibilities are prompt construction, robust JSON extraction/parsing, basic schema validation, and returning normalized fields (type, confidence, subtype, params) for the RAG pipeline to route to metadata queries, summarization, comparisons, or search.\n",
    "\n",
    "- classify_query(query, collections): Public entrypoint that returns a structured routing object for downstream handlers. It delegates to the LLM-based classifier (`_llm_classify`) and returns the normalized result; if the LLM-based call fails or returns no usable output it surfaces an error (no silent fallback in the implementation).\n",
    "\n",
    "- _llm_classify(query, collections): Core LLM-driven classifier:\n",
    "  - Builds a concise instruction prompt that requests a single JSON object describing query_type, confidence, reasoning, and an extracted_params block (e.g., target_document, word_count).\n",
    "  - Calls llm.generate_response(...) to get a textual response.\n",
    "  - Extracts JSON from the LLM output using a regex, parses it with json.loads, validates query_type against allowed types, and converts results into the internal normalized dictionary form.\n",
    "  - Returns None on parsing/validation errors so callers can detect failure.\n",
    "\n",
    "- _determine_subtype(query_type, query): Post-processing helper that derives a finer-grained subtype from the high-level query_type and the raw query text (e.g., map database queries to metadata_count vs metadata_list, or classify summarization as document_summary vs general_summary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "836049e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Enhanced Query Classifier with LLM ready\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Query Classification with LLM\n",
    "class EnhancedQueryClassifier:\n",
    "    \"\"\"Robust LLM-based query classifier with fallback patterns\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_engine: LocalLLMEngine):\n",
    "        if not llm_engine:\n",
    "            raise ValueError(\"LLM engine is required for EnhancedQueryClassifier\")\n",
    "        if not getattr(llm_engine, \"available\", False):\n",
    "            raise ValueError(\"LLM engine must be marked as available\")\n",
    "        \n",
    "        self.llm = llm_engine\n",
    "        print(f\"âœ… Enhanced Query Classifier initialized with LLM engine: {type(llm_engine).__name__}\")\n",
    "    \n",
    "    def classify_query(self, query: str, collections: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Classify query using LLM with structured output\"\"\"\n",
    "        \n",
    "        if not self.llm:\n",
    "            raise RuntimeError(\"LLM engine not available\")\n",
    "        \n",
    "        if not getattr(self.llm, \"available\", False):\n",
    "            raise RuntimeError(\"LLM engine is not marked as available\")\n",
    "        \n",
    "        result = self._llm_classify(query, collections)\n",
    "        \n",
    "        if not result:\n",
    "            raise RuntimeError(\"LLM classification failed - no valid response\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _llm_classify(self, query: str, collections: List[str]) -> Optional[Dict]:\n",
    "        \"\"\"LLM-based classification with explicit examples\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"Classify this RAG system query into ONE type. Respond with JSON only.\n",
    "\n",
    "Available collections: {', '.join(collections)}\n",
    "\n",
    "QUERY TYPES:\n",
    "1. \"database_query\" - Asking about files/documents in database (NOT content)\n",
    "2. \"summarization\" - Requesting summary of document content\n",
    "3. \"comparison\" - Comparing documents or checking compliance\n",
    "4. \"search\" - General content search/questions\n",
    "5. \"question_answering\" - Direct Q&A from documents.\n",
    "\n",
    "CRITICAL RULES:\n",
    "- \"list files\", \"count documents\", \"show available files\" â†’ database_query\n",
    "- \"what does X say about Y\" â†’ search (asking about CONTENT)\n",
    "- \"summarize document.pdf\" â†’ summarization\n",
    "- \"compare X against Y\" â†’ comparison\n",
    "- \"Identify compliance\" â†’ comparison\n",
    "- \"Identify misalignment\" â†’ comparison\n",
    "\n",
    "Query: \"{query}\"\n",
    "\n",
    "Respond with JSON:\n",
    "{{\"query_type\": \"database_query|summarization|comparison|search\",\n",
    "  \"confidence\": 0.0-1.0,\n",
    "  \"reasoning\": \"brief explanation\",\n",
    "  \"extracted_params\": {{\n",
    "    \"target_document\": \"filename or null\",\n",
    "    \"reference_collection\": \"collection name or null\",\n",
    "    \"word_count\": 300\n",
    "  }}\n",
    "}}\n",
    "\n",
    "JSON:\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.llm.generate_response(prompt, temperature=0, max_tokens=200)\n",
    "            print(f\"\\nðŸ” LLM Response for '{query}':\")\n",
    "            print(f\"Raw response: {response[:200]}...\")\n",
    "            \n",
    "            # Extract JSON\n",
    "            json_match = re.search(r'\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}', response, re.DOTALL)\n",
    "            if not json_match:\n",
    "                print(f\"âŒ No JSON found in LLM response for query: {query}\")\n",
    "                return None\n",
    "            \n",
    "            data = json.loads(json_match.group(0))\n",
    "            \n",
    "            # Validate\n",
    "            valid_types = {'database_query', 'summarization', 'comparison', 'search', 'question_answering'}\n",
    "            if data.get('query_type') not in valid_types:\n",
    "                return None\n",
    "            \n",
    "            # Build result\n",
    "            result = {\n",
    "                'type': data['query_type'],\n",
    "                'confidence': float(data.get('confidence', 0.7)),\n",
    "                'reasoning': data.get('reasoning', ''),\n",
    "                'subtype': self._determine_subtype(data['query_type'], query),\n",
    "                **data.get('extracted_params', {})\n",
    "            }\n",
    "            return result\n",
    "        \n",
    "        except (json.JSONDecodeError, ValueError) as e:\n",
    "            print(f\"LLM classification failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _determine_subtype(self, query_type: str, query: str) -> str:\n",
    "        \"\"\"Determine subtype based on query type\"\"\"\n",
    "        if query_type == 'database_query':\n",
    "            if 'count' in query.lower():\n",
    "                return 'metadata_count'\n",
    "            elif 'list' in query.lower() or 'show' in query.lower():\n",
    "                return 'metadata_list'\n",
    "            return 'metadata_query'\n",
    "        \n",
    "        elif query_type == 'summarization':\n",
    "            return 'document_summary' if re.search(r'\\w+\\.pdf', query) else 'general_summary'\n",
    "        \n",
    "        elif query_type == 'comparison':\n",
    "            return 'cross_reference_comparison' if 'compliance' in query.lower() else 'document_comparison'\n",
    "        \n",
    "        elif query_type == 'question_answering':\n",
    "            return 'direct_qa'\n",
    "        \n",
    "        return 'general_search'\n",
    "    \n",
    "    \n",
    "\n",
    "print(\"âœ… Enhanced Query Classifier with LLM ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce973d7",
   "metadata": {},
   "source": [
    "#### 5.3 Enhanced LLM Query Expansion with Financial Domain Optimization\n",
    "\n",
    "Augments user queries for the financial RAG pipeline by combining a small deterministic domain lexicon with a short, LLM-driven contextual expansion step to improve recall for hybrid search.\n",
    "\n",
    "- Constructor (__init__(llm_engine)) :  \n",
    "  - Requires an LLM engine (`llm_engine.available == True`) and raises immediately if missing.  \n",
    "  - Initializes a compact, curated `financial_synonyms` map and an `abbreviations` map used as deterministic seed expansions.\n",
    "    - Purpose of the maps:\n",
    "      - Seed, not ceiling: They give deterministic hints so expansions include high-value domain terms. The LLM still generates extra terms based on the full prompt/context.\n",
    "      - Control noise: Small curated lists reduce irrelevant expansions and help keep the downstream search precise.\n",
    "      - When to expand the maps:\n",
    "        - Add terms if you find repeated misses for domain-specific language (industry jargon, ticker conventions, local regulation names).\n",
    "        - Donâ€™t bloat if the LLM already supplies good contextual terms â€” very large lexicons add maintenance cost and can increase noisy expansions.\n",
    "\n",
    "- Main operation (expand(query, max_terms=8)) :  \n",
    "  - Validates `query` is non-empty.  \n",
    "  - Seeds expansions deterministically by matching tokens from `financial_synonyms` and `abbreviations` found in the query (collects short synonym lists).  \n",
    "  - Calls the LLM (via `llm.generate_response`) to request a concise, comma-separated list of additional financial terms; parsing is robust to newlines and commas.  \n",
    "  - Normalizes, deduplicates, and truncates combined terms to `max_terms`.  \n",
    "  - Returns a structured dict with keys: `original`, `expanded` (original query + appended terms), `matches` (seeded synonyms/abbreviations), `llm_terms` (terms returned by the LLM), and `expansion_count`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "290ea59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced LLM Query Expander \n",
    "class FinancialQueryExpander:\n",
    "    \"\"\"\n",
    "    Expand financial queries using domain synonyms + LLM contextual terms.\n",
    "    Requires a ready `llm_engine` with `generate_response(prompt, temperature, max_tokens)`.\n",
    "    If LLM is not available this will raise ValueError so the caller can fix it.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, llm_engine):\n",
    "        if not llm_engine or not getattr(llm_engine, \"available\", False):\n",
    "            raise ValueError(\"FinancialQueryExpander requires an available LLM engine (llm_engine.available=True).\")\n",
    "        self.llm = llm_engine\n",
    "\n",
    "        # Small curated domain lexicon to seed expansions (keeps output focused)\n",
    "        self.financial_synonyms = {\n",
    "            \"revenue\": [\"income\", \"sales\", \"turnover\", \"earnings\"],\n",
    "            \"profit\": [\"earnings\", \"net income\", \"margin\"],\n",
    "            \"loss\": [\"deficit\", \"decline\"],\n",
    "            \"compliance\": [\"regulatory\", \"requirements\", \"governance\"],\n",
    "            \"risk\": [\"exposure\", \"liability\", \"uncertainty\", \"misalignment\"],\n",
    "            \"cash\": [\"liquidity\", \"working capital\", \"cash flow\"],\n",
    "            \"debt\": [\"liability\", \"borrowings\", \"loan\"],\n",
    "            \"margin\": [\"profitability\", \"spread\"],\n",
    "            \"growth\": [\"expansion\", \"increase\"]\n",
    "        }\n",
    "\n",
    "        self.abbreviations = {\n",
    "            \"EBITDA\": \"earnings before interest tax depreciation amortization\",\n",
    "            \"ROI\": \"return on investment\",\n",
    "            \"CAPEX\": \"capital expenditure\",\n",
    "            \"OPEX\": \"operating expenditure\",\n",
    "            \"YoY\": \"year over year\",\n",
    "            \"QoQ\": \"quarter over quarter\",\n",
    "            \"PRA\": \"prudential regulatory authority\"\n",
    "        }\n",
    "\n",
    "    def expand(self, query: str, max_terms: int = 8) -> Dict:\n",
    "        \"\"\"\n",
    "        Expand `query` with domain synonyms and LLM-provided terms.\n",
    "        Returns dict: { original, expanded, matches, llm_terms, expansion_count }\n",
    "        \"\"\"\n",
    "        if not isinstance(query, str) or not query.strip():\n",
    "            raise ValueError(\"Query must be a non-empty string.\")\n",
    "\n",
    "        original = query.strip()\n",
    "        query_lower = original.lower()\n",
    "\n",
    "        # 1) Seed expansions from curated lexicon (deterministic)\n",
    "        matches = {}\n",
    "        seed_terms = []\n",
    "        for word, syns in self.financial_synonyms.items():\n",
    "            if word in query_lower:\n",
    "                matches[word] = syns[:3]\n",
    "                seed_terms.extend(syns[:3])\n",
    "\n",
    "        for abbr, full in self.abbreviations.items():\n",
    "            if abbr.lower() in query_lower or abbr.lower() in original.lower():\n",
    "                matches[abbr] = full\n",
    "                seed_terms.extend(full.split())\n",
    "\n",
    "        # 2) Always call LLM for contextual expansion (per requirements)\n",
    "        # Request a short, comma-separated list of related financial terms (3-6)\n",
    "        needed = max(0, max_terms - len(seed_terms))\n",
    "        llm_terms = []\n",
    "        try:\n",
    "            prompt = (\n",
    "                f\"Provide {max(3, needed)} concise financial query expansion terms for:\\n\\n\"\n",
    "                f\"\\\"{original}\\\"\\n\\n\"\n",
    "                \"Respond with a single line containing comma-separated terms only (no explanations).\"\n",
    "            )\n",
    "            resp = self.llm.generate_response(prompt, temperature=0, max_tokens=60)\n",
    "            if not resp or not isinstance(resp, str):\n",
    "                raise RuntimeError(\"LLM returned empty or invalid response for expansion.\")\n",
    "            # Parse comma-separated terms robustly\n",
    "            parts = [p.strip().lower() for p in resp.replace(\"\\n\", \",\").split(\",\")]\n",
    "            # Filter out empties and duplicates, keep short tokens/phrases\n",
    "            llm_terms = []\n",
    "            for p in parts:\n",
    "                if not p:\n",
    "                    continue\n",
    "                if p in llm_terms:\n",
    "                    continue\n",
    "                if len(p) < 2:\n",
    "                    continue\n",
    "                llm_terms.append(p)\n",
    "                if len(llm_terms) >= max(3, needed):\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            # Per requirements: do NOT fallback silently; surface a clear error\n",
    "            raise RuntimeError(f\"LLM expansion failed: {e}\")\n",
    "\n",
    "        # 3) Combine seeds + llm_terms while deduping and limiting\n",
    "        combined = []\n",
    "        for t in seed_terms + llm_terms:\n",
    "            norm = t.strip()\n",
    "            if not norm:\n",
    "                continue\n",
    "            if norm in combined:\n",
    "                continue\n",
    "            combined.append(norm)\n",
    "            if len(combined) >= max_terms:\n",
    "                break\n",
    "\n",
    "        expanded_query = original + (\" \" + \" \".join(combined) if combined else \"\")\n",
    "        return {\n",
    "            \"original\": original,\n",
    "            \"expanded\": expanded_query,\n",
    "            \"matches\": matches,\n",
    "            \"llm_terms\": llm_terms,\n",
    "            \"expansion_count\": len(combined)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b27df6",
   "metadata": {},
   "source": [
    "#### 5.4 Document Finder and Reference Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "19eb057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Finder & Reference Extractor â€” uses hybrid_search and LLM\n",
    "class DocumentFinderAndRefExtractor:\n",
    "    \"\"\"\n",
    "    LLM-driven finder that uses the notebook's `hybrid_search` for retrieval and the provided\n",
    "    `llm_engine` for claim extraction and alignment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, llm_engine, hybrid_search):\n",
    "        if not llm_engine or not getattr(llm_engine, \"available\", False):\n",
    "            raise ValueError(\"LLM engine required and must be available (llm_engine.available=True).\")\n",
    "        if not hybrid_search or not hasattr(hybrid_search, \"search\"):\n",
    "            raise ValueError(\"hybrid_search required and must implement .search(query, collection, top_k).\")\n",
    "        self.llm = llm_engine\n",
    "        self.hybrid = hybrid_search\n",
    "\n",
    "    def _extract_json_from_text(self, text: str) -> Any:\n",
    "        m = re.search(r\"(\\{.*\\}|\\[.*\\])\", text, re.S)\n",
    "        if m:\n",
    "            try:\n",
    "                return json.loads(m.group(1))\n",
    "            except Exception:\n",
    "                pass\n",
    "        try:\n",
    "            return json.loads(text)\n",
    "        except Exception:\n",
    "            raise ValueError(\"Could not parse JSON from LLM response.\")\n",
    "\n",
    "    def find_documents(self, query: str, collections: List[str], top_k: int = 10) -> List[Dict]:\n",
    "        \"\"\"Search across collections using hybrid_search and return aggregated ranked results.\"\"\"\n",
    "        results = []\n",
    "        for col in collections:\n",
    "            results.extend(self.hybrid.search(query, col, top_k))\n",
    "        results.sort(key=lambda r: r.get(\"final_score\", r.get(\"score\", 0)), reverse=True)\n",
    "        return results[: top_k * len(collections)]\n",
    "\n",
    "    def extract_claims(self, document_text: str, max_claims: int = 8) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Ask the LLM to extract concise factual claims. Returns list of {'id','text'}.\n",
    "        Raises on parsing or LLM errors.\n",
    "        \"\"\"\n",
    "        prompt = (\n",
    "            f\"Extract up to {max_claims} concise factual claims from the document below.\\n\"\n",
    "            \"Return a JSON array of objects with keys: id, text. No extra commentary.\\n\\n\"\n",
    "            f\"DOCUMENT:\\n{document_text[:8000]}\"\n",
    "        )\n",
    "        try:\n",
    "            resp = self.llm.generate_response(prompt, temperature=0.0, max_tokens=600)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"LLM claim extraction failed: {e}\")\n",
    "        parsed = self._extract_json_from_text(resp)\n",
    "        if not isinstance(parsed, list):\n",
    "            raise ValueError(\"LLM did not return a JSON array of claims.\")\n",
    "        claims = []\n",
    "        for i, item in enumerate(parsed[:max_claims]):\n",
    "            text = item.get(\"text\") if isinstance(item, dict) else str(item)\n",
    "            claims.append({\"id\": f\"claim_{i+1}\", \"text\": text.strip()})\n",
    "        return claims\n",
    "\n",
    "    def retrieve_references(self, claim_text: str, ref_collection: str, top_k: int = 5) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Retrieve reference passages using existing hybrid_search (BM25+semantic+fusion).\n",
    "        Returns list of dicts with 'text' and 'metadata'.\n",
    "        \"\"\"\n",
    "        return self.hybrid.search(claim_text, ref_collection, top_k)\n",
    "\n",
    "    def analyze_claim_against_refs(self, claim: Dict, refs: List[Dict], target_doc: str, ref_collection: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Use LLM to determine alignment. Returns:\n",
    "         { claim_id, claim_text, has_misalignment, severity, explanation, references }\n",
    "        \"\"\"\n",
    "        refs_text = \"\\n\\n\".join(\n",
    "            f\"Ref {i+1} ({r.get('metadata', {}).get('filename','unknown')}): {r.get('text','')[:800]}\"\n",
    "            for i, r in enumerate(refs[:3])\n",
    "        ) or \"No references found.\"\n",
    "        prompt = (\n",
    "            \"Compare the TARGET statement to the REFERENCES. Return only JSON with keys:\\n\"\n",
    "            \"status: 'ALIGNED' or 'MISALIGNED', severity: 'high'|'medium'|'low'|null, explanation: string.\\n\\n\"\n",
    "            f\"TARGET DOCUMENT: {target_doc}\\n\\nSTATEMENT: {claim['text'][:1200]}\\n\\n\"\n",
    "            f\"REFERENCES ({ref_collection}):\\n{refs_text}\"\n",
    "        )\n",
    "        try:\n",
    "            resp = self.llm.generate_response(prompt, temperature=0.0, max_tokens=500)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"LLM alignment analysis failed: {e}\")\n",
    "        parsed = self._extract_json_from_text(resp)\n",
    "        if not isinstance(parsed, dict):\n",
    "            raise ValueError(\"LLM alignment response not a JSON object.\")\n",
    "        status = parsed.get(\"status\", \"\").upper()\n",
    "        has_misalignment = status != \"ALIGNED\"\n",
    "        severity = parsed.get(\"severity\") or ((\"medium\") if has_misalignment else None)\n",
    "        explanation = parsed.get(\"explanation\", \"\").strip()\n",
    "        return {\n",
    "            \"claim_id\": claim.get(\"id\"),\n",
    "            \"claim_text\": claim.get(\"text\"),\n",
    "            \"has_misalignment\": bool(has_misalignment),\n",
    "            \"severity\": severity,\n",
    "            \"explanation\": explanation,\n",
    "            \"references\": [\n",
    "                {\"filename\": r.get(\"metadata\", {}).get(\"filename\", \"unknown\"),\n",
    "                 \"snippet\": r.get(\"text\", \"\")[:400]}\n",
    "                for r in refs[:3]\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    def compare_document_to_collection(self, document_text: str, target_doc_name: str,\n",
    "                                       ref_collection: str, max_claims: int = 8, top_k_refs: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Full pipeline:\n",
    "         - extract claims (LLM)\n",
    "         - for each claim retrieve refs (hybrid_search)\n",
    "         - analyze each claim vs refs (LLM)\n",
    "        Returns summary dict with analyses and confidence.\n",
    "        \"\"\"\n",
    "        claims = self.extract_claims(document_text, max_claims=max_claims)\n",
    "        analyses = []\n",
    "        for claim in claims:\n",
    "            refs = self.retrieve_references(claim[\"text\"], ref_collection, top_k_refs)\n",
    "            analysis = self.analyze_claim_against_refs(claim, refs, target_doc_name, ref_collection)\n",
    "            analyses.append(analysis)\n",
    "        misalignments = [a for a in analyses if a.get(\"has_misalignment\")]\n",
    "        confidence = round(max(0.0, 1.0 - (len(misalignments) / max(1, len(claims)))), 3)\n",
    "        return {\n",
    "            \"target_document\": target_doc_name,\n",
    "            \"reference_collection\": ref_collection,\n",
    "            \"total_claims\": len(claims),\n",
    "            \"misalignments\": misalignments,\n",
    "            \"analyses\": analyses,\n",
    "            \"confidence\": confidence\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e6f2ef93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM-driven, verified document filter (uses hybrid_search + verification)\n",
    "import json, re\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def _extract_json_from_text(text: str) -> Any:\n",
    "    m = re.search(r\"(\\{.*\\}|\\[.*\\])\", text, re.S)\n",
    "    if m:\n",
    "        try:\n",
    "            return json.loads(m.group(1))\n",
    "        except Exception:\n",
    "            pass\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Could not parse JSON from LLM response: {e}\\nRaw: {text[:400]}\")\n",
    "\n",
    "def _metadata_year_matches(meta: dict, year: int) -> bool:\n",
    "    if not meta:\n",
    "        return False\n",
    "    # look for explicit numeric year fields and date-like strings\n",
    "    for k, v in meta.items():\n",
    "        s = str(v or \"\").lower()\n",
    "        if re.search(rf\"\\b{year}\\b\", s):\n",
    "            return True\n",
    "        # common date patterns: YYYY-MM-DD etc\n",
    "        if re.search(rf\"{year}[-/]\", s) or re.search(rf\"[-/]{year}\", s):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def find_documents_with_verified_filters(llm_engine, hybrid_search, user_query: str,\n",
    "                                         collections: List[str], year_hint: int = None,\n",
    "                                         candidate_k: int = 50) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Hybrid retrieval -> LLM filter -> deterministic verification -> (optional) LLM recheck.\n",
    "    Returns structured dict with filters, matches and verification details.\n",
    "    \"\"\"\n",
    "    if not llm_engine or not getattr(llm_engine, \"available\", False):\n",
    "        raise ValueError(\"LLM engine required and not available (llm_engine.available=True).\")\n",
    "    if not hybrid_search or not hasattr(hybrid_search, \"search\"):\n",
    "        raise ValueError(\"hybrid_search required and must implement .search(query, collection, top_k).\")\n",
    "\n",
    "    # 1) get candidates\n",
    "    candidates = []\n",
    "    for col in collections:\n",
    "        candidates.extend(hybrid_search.search(user_query, col, candidate_k))\n",
    "    if not candidates:\n",
    "        return {\"filters\": {}, \"candidates\": [], \"matched\": [], \"llm_assessment\": {\"found\": False}, \"verification\": [], \"message\": \"No candidates returned by hybrid_search.\"}\n",
    "\n",
    "    # 2) prepare compact candidate info including relevant metadata fields\n",
    "    items = []\n",
    "    for i, c in enumerate(candidates[:candidate_k]):\n",
    "        meta = c.get(\"metadata\") or {}\n",
    "        filename = meta.get(\"filename\") or meta.get(\"source\") or meta.get(\"doc_id\") or \"<unknown>\"\n",
    "        # try to pull an explicit year-like field if present\n",
    "        year_field = None\n",
    "        for k in (\"year\", \"file_year\", \"date\", \"file_date\"):\n",
    "            if k in meta:\n",
    "                year_field = meta[k]\n",
    "                break\n",
    "        snippet = (c.get(\"text\") or \"\")[:600].replace(\"\\n\", \" \").strip()\n",
    "        items.append({\"i\": i, \"filename\": str(filename), \"year_meta\": str(year_field) if year_field is not None else None, \"snippet\": snippet})\n",
    "\n",
    "    items_json = json.dumps(items, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # 3) prompt LLM to extract filters and pick matching indexes, and to state evidence per index\n",
    "    prompt = (\n",
    "        \"You are given a user query and a list of candidate documents (index, filename, year_meta, snippet).\\n\"\n",
    "        f\"USER QUERY: {user_query}\\n\\n\"\n",
    "        \"Task (strict JSON only):\\n\"\n",
    "        \"  1) Extract filters implied by the query (year, exact_title, title_keywords, author, company, other).\\n\"\n",
    "        \"  2) From the candidates, identify which documents match those filters.\\n\"\n",
    "        \"  3) For each selected index return the evidence used (one of: 'metadata_year','filename','snippet','other') and the metadata value used.\\n\\n\"\n",
    "        \"Return JSON: { filters: {...}, found: bool, indexes: [ints], evidence: {index: {evidence: 'metadata_year'|'filename'|'snippet'|'other', value: '...'}}, explanation: '...'}\\n\\n\"\n",
    "        \"Candidates:\\n\" + items_json + \"\\n\\nRespond with JSON only.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        resp = llm_engine.generate_response(prompt, temperature=0.0, max_tokens=700)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"LLM call failed: {e}\")\n",
    "\n",
    "    assessment = _extract_json_from_text(resp)\n",
    "    if not isinstance(assessment, dict):\n",
    "        raise ValueError(\"LLM response is not a JSON object.\")\n",
    "\n",
    "    # normalization\n",
    "    idxs = []\n",
    "    for v in assessment.get(\"indexes\", []):\n",
    "        try:\n",
    "            idxs.append(int(v))\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # 4) deterministic verification: check metadata/filename/snippet for year if year filter present\n",
    "    verification = []\n",
    "    target_year = assessment.get(\"filters\", {}).get(\"year\") or year_hint\n",
    "    for idx in idxs:\n",
    "        entry = {\"index\": idx, \"llm_evidence\": assessment.get(\"evidence\", {}).get(str(idx)) or assessment.get(\"evidence\", {}).get(idx)}\n",
    "        meta = candidates[idx].get(\"metadata\") or {}\n",
    "        filename = str(meta.get(\"filename\") or meta.get(\"source\") or \"\")\n",
    "        md_match = _metadata_year_matches(meta, target_year) if target_year else False\n",
    "        filename_match = bool(re.search(rf\"\\b{target_year}\\b\", filename)) if target_year else False\n",
    "        snippet_match = bool(re.search(rf\"\\b{target_year}\\b\", (candidates[idx].get(\"text\") or \"\"))) if target_year else False\n",
    "        verification.append({\n",
    "            \"index\": idx,\n",
    "            \"metadata_year_present\": bool(md_match),\n",
    "            \"filename_contains_year\": bool(filename_match),\n",
    "            \"snippet_contains_year\": bool(snippet_match),\n",
    "            \"llm_evidence\": entry.get(\"llm_evidence\")\n",
    "        })\n",
    "\n",
    "    # 5) If LLM said found=True but verification shows none of the selected indexes have year evidence,\n",
    "    #    re-prompt LLM with the verification table and ask it to reconsider.\n",
    "    verified_matches = [v for v in verification if v[\"metadata_year_present\"] or v[\"filename_contains_year\"] or v[\"snippet_contains_year\"]]\n",
    "    if assessment.get(\"found\") and not verified_matches and target_year:\n",
    "        followup_prompt = (\n",
    "            \"You previously identified candidate indexes as matching the user's filters. However, deterministic checks show none of the selected candidates contain the requested year in metadata, filename, or snippet.\\n\"\n",
    "            \"Please re-evaluate the same candidate list and return corrected JSON with the same schema (filters, found, indexes, evidence, explanation). Use metadata year preferentially; if no reliable matches exist, set found:false and explain.\\n\\n\"\n",
    "            \"Original USER QUERY: \" + user_query + \"\\n\\n\"\n",
    "            \"Candidates (index, filename, year_meta, snippet):\\n\" + items_json + \"\\n\\n\"\n",
    "            \"Deterministic verification results:\\n\" + json.dumps(verification, ensure_ascii=False, indent=2) + \"\\n\\nRespond with JSON only.\"\n",
    "        )\n",
    "        try:\n",
    "            resp2 = llm_engine.generate_response(followup_prompt, temperature=0.0, max_tokens=700)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"LLM follow-up call failed: {e}\")\n",
    "        assessment2 = _extract_json_from_text(resp2)\n",
    "        if not isinstance(assessment2, dict):\n",
    "            raise ValueError(\"LLM follow-up response is not a JSON object.\")\n",
    "        assessment = assessment2\n",
    "        # recompute idxs and verification based on new assessment\n",
    "        idxs = []\n",
    "        for v in assessment.get(\"indexes\", []):\n",
    "            try:\n",
    "                idxs.append(int(v))\n",
    "            except Exception:\n",
    "                continue\n",
    "        verification = []\n",
    "        for idx in idxs:\n",
    "            meta = candidates[idx].get(\"metadata\") or {}\n",
    "            filename = str(meta.get(\"filename\") or meta.get(\"source\") or \"\")\n",
    "            md_match = _metadata_year_matches(meta, target_year) if target_year else False\n",
    "            filename_match = bool(re.search(rf\"\\b{target_year}\\b\", filename)) if target_year else False\n",
    "            snippet_match = bool(re.search(rf\"\\b{target_year}\\b\", (candidates[idx].get(\"text\") or \"\"))) if target_year else False\n",
    "            verification.append({\n",
    "                \"index\": idx,\n",
    "                \"metadata_year_present\": bool(md_match),\n",
    "                \"filename_contains_year\": bool(filename_match),\n",
    "                \"snippet_contains_year\": bool(snippet_match),\n",
    "                \"llm_evidence\": assessment.get(\"evidence\", {}).get(str(idx)) or assessment.get(\"evidence\", {}).get(idx)\n",
    "            })\n",
    "\n",
    "    # 6) finalize matched list (only indexes that exist in candidates)\n",
    "    matched = []\n",
    "    for idx in idxs:\n",
    "        if 0 <= idx < len(candidates):\n",
    "            matched.append(candidates[idx])\n",
    "\n",
    "    message = assessment.get(\"explanation\", \"\")\n",
    "    if assessment.get(\"found\") and not matched:\n",
    "        message = (message or \"\") + \" (LLM indicated matches but no valid indexes or verification failed.)\"\n",
    "\n",
    "    return {\n",
    "        \"filters\": assessment.get(\"filters\", {}),\n",
    "        \"candidates\": candidates[:candidate_k],\n",
    "        \"matched\": matched,\n",
    "        \"llm_assessment\": assessment,\n",
    "        \"verification\": verification,\n",
    "        \"message\": message or (\"Found matches\" if matched else \"No reliable matches found\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dcb6dcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using models_dir: /Users/rodrigograndy/Desktop/coding_projects/posts_website_drafts/rag_boe/models\n",
      "Loading MLX model from: /Users/rodrigograndy/Desktop/coding_projects/posts_website_drafts/rag_boe/models/Llama-3.1-8B-Instruct-4bit ...\n",
      "âœ… MLX Model loaded: Llama-3.1-8B-Instruct-4bit (resolved: /Users/rodrigograndy/Desktop/coding_projects/posts_website_drafts/rag_boe/models/Llama-3.1-8B-Instruct-4bit)\n",
      "LLM loaded. Resolved path: /Users/rodrigograndy/Desktop/coding_projects/posts_website_drafts/rag_boe/models/Llama-3.1-8B-Instruct-4bit\n"
     ]
    }
   ],
   "source": [
    "models_dir = Path(MODELS_DIR) if 'MODELS_DIR' in globals() and MODELS_DIR else Path.cwd() / \"models\"\n",
    "print(\"Using models_dir:\", models_dir)\n",
    "chosen_model_dirname = \"Llama-3.1-8B-Instruct-4bit\"\n",
    "\n",
    "llm_engine = LocalLLMEngine(models_dir, chosen_model_dirname)\n",
    "print(\"LLM loaded. Resolved path:\", llm_engine.model_info.get('resolved_path', llm_engine.model_info.get('name')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "611e2100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DocumentFinderAndRefExtractor created successfully.\n",
      "Parsed filters: {'year': '2022', 'exact_title': False, 'title_keywords': False, 'author': False, 'company': False, 'other': False}\n",
      "LLM assessment: {'filters': {'year': '2022', 'exact_title': False, 'title_keywords': False, 'author': False, 'company': False, 'other': False}, 'found': True, 'indexes': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], 'evidence': {'0': {'evidence': 'metadata_year', 'value': '2025'}, '1': {'evidence': 'metadata_year', 'value': '2025'}, '2': {'evidence': 'metadata_year', 'value': '2025'}, '3': {'evidence': 'metadata_year', 'value': '2025'}, '4': {'evidence': 'metadata_year', 'value': '2025'}, '5': {'evidence': 'metadata_year', 'value': '2025'}, '6': {'evidence': 'metadata_year', 'value': '2025'}, '7': {'evidence': 'metadata_year', 'value': '2025'}, '8': {'evidence': 'metadata_year', 'value': '2025'}, '9': {'evidence': 'metadata_year', 'value': '2020'}, '10': {'evidence': 'metadata_year', 'value': '2025'}, '11': {'evidence': 'metadata_year', 'value': '2023'}, '12': {'evidence': 'metadata_year', 'value': '2025'}, '13': {'evidence': 'metadata_year', 'value': '2025'}, '14': {'evidence': 'metadata_year', 'value': '2025'}, '15': {'evidence': 'metadata_year', 'value': '2025'}, '16': {'evidence': 'metadata_year', 'value': '2000'}, '17': {'evidence': 'metadata_year', 'value': '2025'}, '18': {'evidence': 'metadata_year', 'value': '2025'}, '19': {'evidence': 'metadata_year', 'value': '2025'}}, 'explanation': \"The query 'list files for 2022 in collections' implies a filter on the year metadata, which matches 20 documents.\"}\n",
      "Matches: 20\n",
      "1. Reporting (CRR)_16-09-2025.pdf  score:0.05928657472389136\n",
      "2. Reporting (CRR)_16-09-2025.pdf  score:0.0584128046821227\n",
      "3. Reporting (CRR)_16-09-2025.pdf  score:0.057689304677356744\n",
      "4. Disclosure (CRR)_16-09-2025.pdf  score:0.05655737704918033\n",
      "5. Reporting (CRR)_16-09-2025.pdf  score:0.056287636400233945\n",
      "6. Regulatory Reporting_16-09-2025.pdf  score:0.056129032258064517\n",
      "7. Securitisation_16-09-2025.pdf  score:0.055730158730158734\n",
      "8. Reporting (CRR)_16-09-2025.pdf  score:0.05515927622857132\n",
      "9. Securitisation_16-09-2025.pdf  score:0.0550275401308279\n",
      "10. Regulatory Reporting_16-09-2025.pdf  score:0.054414636583914565\n",
      "11. Reporting (CRR)_16-09-2025.pdf  score:0.053907155518845645\n",
      "12. SDDT Regime â€“ General Application_16-09-2025.pdf  score:0.052590829339994635\n",
      "13. Regulatory Reporting_16-09-2025.pdf  score:0.052514473893380535\n",
      "14. Reporting (CRR)_16-09-2025.pdf  score:0.05083409273067937\n",
      "15. Resolution Assessment_16-09-2025.pdf  score:0.05013281333178869\n",
      "16. Disclosure (CRR)_16-09-2025.pdf  score:0.049433715198165445\n",
      "17. Definition of Capital_16-09-2025.pdf  score:0.0482894870233637\n",
      "18. Disclosure (CRR)_16-09-2025.pdf  score:0.047681995619653784\n",
      "19. Regulatory Reporting_16-09-2025.pdf  score:0.04728276399189404\n",
      "20. General Organisational Requirements_16-09-2025.pdf  score:0.04646871137262858\n"
     ]
    }
   ],
   "source": [
    "finder = DocumentFinderAndRefExtractor(llm_engine, globals()['hybrid_search'])\n",
    "print(\"DocumentFinderAndRefExtractor created successfully.\")\n",
    "\n",
    "query = \"list files for 2022 in collections\"\n",
    "\n",
    "res = find_documents_with_verified_filters(\n",
    "    llm_engine=llm_engine,\n",
    "    hybrid_search=hybrid_search,\n",
    "    user_query=query,\n",
    "    collections=[\"pra_rules\"],  # choose relevant collections\n",
    "    candidate_k=20\n",
    ")\n",
    "\n",
    "print(\"Parsed filters:\", res[\"filters\"])\n",
    "print(\"LLM assessment:\", res[\"llm_assessment\"])\n",
    "print(\"Matches:\", len(res[\"matched\"]))\n",
    "for i, r in enumerate(res[\"matched\"], 1):\n",
    "    meta = r.get(\"metadata\", {}) or {}\n",
    "    print(f\"{i}. {meta.get('filename','<unknown>')}  score:{r.get('final_score', r.get('score',0))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a04666f",
   "metadata": {},
   "source": [
    "#### 5.5 Results Processor with Confidence Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8a52af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ResultsProcessor ready\n",
      "   â€¢ Multi-factor confidence scoring\n",
      "   â€¢ Cross-encoder reranking integration\n",
      "   â€¢ Text processing and relevance indicators\n",
      "   â€¢ Confidence-based filtering\n"
     ]
    }
   ],
   "source": [
    "# Results Processor with Confidence Scoring\n",
    "\n",
    "class ResultsProcessor:\n",
    "    \"\"\"Process and score search results with confidence metrics\"\"\"\n",
    "    \n",
    "    def __init__(self, cross_encoder=None):\n",
    "        self.cross_encoder = cross_encoder\n",
    "        self.confidence_thresholds = {'high': 0.8, 'medium': 0.5, 'low': 0.3}\n",
    "    \n",
    "    def process_results(self, results: List[Dict], query: str,\n",
    "                       apply_reranking: bool = True,\n",
    "                       min_confidence: float = 0.3) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process and enhance search results with confidence scoring.\n",
    "        \n",
    "        Args:\n",
    "            results: Raw search results with text, scores, metadata\n",
    "            query: Original query string\n",
    "            apply_reranking: Whether to apply cross-encoder reranking\n",
    "            min_confidence: Minimum confidence threshold for filtering\n",
    "        \n",
    "        Returns:\n",
    "            Dict with processed results, statistics, and average confidence\n",
    "        \"\"\"\n",
    "        if not results:\n",
    "            return self._empty_results(query)\n",
    "        \n",
    "        processed_results = []\n",
    "        \n",
    "        for result in results:\n",
    "            confidence = self._calculate_confidence(result)\n",
    "            \n",
    "            # Filter low-confidence results\n",
    "            if confidence < min_confidence:\n",
    "                continue\n",
    "            \n",
    "            # Enhance with metadata\n",
    "            enhanced_result = {\n",
    "                **result,\n",
    "                'confidence': confidence,\n",
    "                'confidence_level': self._get_confidence_level(confidence),\n",
    "                'processed_text': self._process_text_snippet(result.get('text', '')),\n",
    "                'relevance_indicators': self._extract_relevance_indicators(result, query)\n",
    "            }\n",
    "            \n",
    "            processed_results.append(enhanced_result)\n",
    "        \n",
    "        # Apply cross-encoder reranking if available\n",
    "        if apply_reranking and self.cross_encoder and processed_results:\n",
    "            processed_results = self._apply_reranking(query, processed_results)\n",
    "        \n",
    "        # Sort by combined score\n",
    "        processed_results.sort(\n",
    "            key=lambda x: (x.get('final_score', 0) + x.get('confidence', 0)) / 2,\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        avg_confidence = (\n",
    "            sum(r['confidence'] for r in processed_results) / len(processed_results)\n",
    "            if processed_results else 0.0\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'results': processed_results,\n",
    "            'total_found': len(results),\n",
    "            'total_processed': len(processed_results),\n",
    "            'average_confidence': avg_confidence,\n",
    "            'query': query\n",
    "        }\n",
    "    \n",
    "    def _calculate_confidence(self, result: Dict) -> float:\n",
    "        \"\"\"Calculate multi-factor confidence score\"\"\"\n",
    "        factors = []\n",
    "        \n",
    "        # Score-based factors\n",
    "        if 'final_score' in result:\n",
    "            factors.append(min(result['final_score'] * 2, 1.0))\n",
    "        if 'semantic_score' in result:\n",
    "            factors.append(result['semantic_score'])\n",
    "        if 'bm25_score' in result:\n",
    "            factors.append(min(result['bm25_score'] / 10, 1.0))\n",
    "        \n",
    "        # Text length factor (optimal: 200-1000 chars)\n",
    "        text_len = len(result.get('text', ''))\n",
    "        if 200 <= text_len <= 1000:\n",
    "            factors.append(0.8)\n",
    "        elif 100 <= text_len <= 1500:\n",
    "            factors.append(0.6)\n",
    "        else:\n",
    "            factors.append(0.4)\n",
    "        \n",
    "        # Metadata completeness\n",
    "        metadata = result.get('metadata', {})\n",
    "        if metadata.get('filename') and metadata.get('chunk_id'):\n",
    "            factors.append(0.7)\n",
    "        \n",
    "        return np.mean(factors) if factors else 0.5\n",
    "    \n",
    "    def _get_confidence_level(self, confidence: float) -> str:\n",
    "        \"\"\"Map confidence score to level label\"\"\"\n",
    "        if confidence >= self.confidence_thresholds['high']:\n",
    "            return 'high'\n",
    "        elif confidence >= self.confidence_thresholds['medium']:\n",
    "            return 'medium'\n",
    "        return 'low'\n",
    "    \n",
    "    def _process_text_snippet(self, text: str, max_length: int = 300) -> str:\n",
    "        \"\"\"Clean and truncate text for display\"\"\"\n",
    "        if not text:\n",
    "            return \"\"\n",
    "        \n",
    "        # Normalize whitespace\n",
    "        clean_text = re.sub(r'\\s+', ' ', text.strip())\n",
    "        \n",
    "        if len(clean_text) <= max_length:\n",
    "            return clean_text\n",
    "        \n",
    "        # Break at sentence boundary if possible\n",
    "        truncated = clean_text[:max_length]\n",
    "        last_period = truncated.rfind('.')\n",
    "        last_space = truncated.rfind(' ')\n",
    "        \n",
    "        if last_period > max_length - 50:\n",
    "            return truncated[:last_period + 1]\n",
    "        elif last_space > max_length - 20:\n",
    "            return truncated[:last_space] + \"...\"\n",
    "        return truncated + \"...\"\n",
    "    \n",
    "    def _extract_relevance_indicators(self, result: Dict, query: str) -> List[str]:\n",
    "        \"\"\"Extract why this result is relevant\"\"\"\n",
    "        indicators = []\n",
    "        text_lower = result.get('text', '').lower()\n",
    "        query_terms = query.lower().split()\n",
    "        \n",
    "        # Direct term matches\n",
    "        matches = [term for term in query_terms if term in text_lower]\n",
    "        if matches:\n",
    "            indicators.append(f\"Contains: {', '.join(matches[:3])}\")\n",
    "        \n",
    "        # Score indicators\n",
    "        if result.get('semantic_score', 0) > 0.7:\n",
    "            indicators.append(\"High semantic similarity\")\n",
    "        if result.get('bm25_score', 0) > 5:\n",
    "            indicators.append(\"Strong keyword match\")\n",
    "        \n",
    "        # Metadata indicators\n",
    "        metadata = result.get('metadata', {})\n",
    "        if metadata.get('doc_type') == 'financial_document':\n",
    "            indicators.append(\"Financial document\")\n",
    "        \n",
    "        return indicators\n",
    "    \n",
    "    def _apply_reranking(self, query: str, results: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Apply cross-encoder reranking if available\"\"\"\n",
    "        if not self.cross_encoder or not hasattr(self.cross_encoder, 'predict'):\n",
    "            return results\n",
    "        \n",
    "        try:\n",
    "            # Prepare pairs (truncate for efficiency)\n",
    "            pairs = [(query, result.get('text', '')[:500]) for result in results]\n",
    "            \n",
    "            # Get reranking scores\n",
    "            rerank_scores = self.cross_encoder.predict(pairs)\n",
    "            \n",
    "            # Update results with blended scores\n",
    "            for i, result in enumerate(results):\n",
    "                result['rerank_score'] = float(rerank_scores[i])\n",
    "                result['final_score'] = (\n",
    "                    result.get('final_score', 0) * 0.7 +\n",
    "                    result['rerank_score'] * 0.3\n",
    "                )\n",
    "            \n",
    "            return results\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Reranking failed: {e}\")\n",
    "            return results\n",
    "    \n",
    "    def _empty_results(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Return empty results structure\"\"\"\n",
    "        return {\n",
    "            'results': [],\n",
    "            'total_found': 0,\n",
    "            'total_processed': 0,\n",
    "            'average_confidence': 0.0,\n",
    "            'query': query\n",
    "        }\n",
    "\n",
    "print(\"âœ… ResultsProcessor ready\")\n",
    "print(\"   â€¢ Multi-factor confidence scoring\")\n",
    "print(\"   â€¢ Cross-encoder reranking integration\")\n",
    "print(\"   â€¢ Text processing and relevance indicators\")\n",
    "print(\"   â€¢ Confidence-based filtering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b20b3b",
   "metadata": {},
   "source": [
    "#### 5.6 Initialize LLM Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b177a29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ Found 1 model(s):\n",
      "   â€¢ mistral-7b-4bit_mlx: 7B (4bit mlx, ~3.9GB)\n",
      "     Path: /Users/rodrigograndy/Desktop/coding_projects/posts_website_drafts/rag_boe/models/mistral-7b-instruct-v0.3-4bit\n",
      "\n",
      "ðŸŽ¯ Selected: mistral-7b-4bit_mlx\n",
      "Loading MLX model: mistral-7b-instruct-v0.3-4bit...\n",
      "âœ… MLX Model loaded: mistral-7b-instruct-v0.3-4bit\n",
      "   Parameters: 7B, Quantization: 4bit mlx\n",
      "   Location: /Users/rodrigograndy/Desktop/coding_projects/posts_website_drafts/rag_boe/models/mistral-7b-instruct-v0.3-4bit\n",
      "   ðŸš€ Optimized for Apple Silicon\n",
      "âœ… Model loaded: mistral-7b-instruct-v0.3-4bit\n",
      "   Parameters: 7B, Quantization: 4bit mlx\n",
      "   Location: /Users/rodrigograndy/Desktop/coding_projects/posts_website_drafts/rag_boe/models/mistral-7b-instruct-v0.3-4bit\n",
      "\n",
      "âœ… Section 5 components initialized:\n",
      "   â€¢ LocalLLMEngine\n",
      "   â€¢ EnhancedQueryClassifier\n",
      "   â€¢ DocumentFinder\n",
      "   â€¢ FinancialQueryExpander\n",
      "   â€¢ ResultsProcessor\n",
      "\n",
      "ðŸ§ª Testing LLM...\n",
      "   âœ… LLM responding: OK. How can I\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM engine with model detection\n",
    "if 'MODELS_DIR' not in globals():\n",
    "    print(\"âš ï¸ MODELS_DIR not defined - run Section 1.4 first\")\n",
    "    llm_engine = None\n",
    "else:\n",
    "    models_path = Path(MODELS_DIR)\n",
    "    \n",
    "    # Check for model DIRECTORIES (not just files)\n",
    "    model_dirs = {}\n",
    "    for key, info in LocalLLMEngine.RECOMMENDED_MODELS.items():\n",
    "        # Try multiple name variants for directories\n",
    "        candidates = [\n",
    "            models_path / info['name'],                                    # Exact name\n",
    "            models_path / info['name'].replace('.4bit', '-4bit'),          # Dash variant\n",
    "            models_path / info['name'].replace('.4bit', '-4bit').replace('.', '-')  # Full dash\n",
    "        ]\n",
    "        \n",
    "        for candidate in candidates:\n",
    "            if candidate.exists() and candidate.is_dir():  # âœ… Check if it's a directory\n",
    "                model_dirs[key] = {\n",
    "                    'key': key,\n",
    "                    'name': info['name'],\n",
    "                    'params': info['params'],\n",
    "                    'quant': info['quant'],\n",
    "                    'size_gb': info['size_gb'],\n",
    "                    'path': str(candidate)\n",
    "                }\n",
    "                break\n",
    "    \n",
    "    if not model_dirs:\n",
    "        print(f\"âš ï¸ No models found in {models_path}\")\n",
    "        print(\"\\nSearched for:\")\n",
    "        for key, info in LocalLLMEngine.RECOMMENDED_MODELS.items():\n",
    "            print(f\"   â€¢ {info['name']} ({key})\")\n",
    "        \n",
    "        # Show what IS in the directory\n",
    "        if models_path.exists():\n",
    "            existing = list(models_path.iterdir())\n",
    "            if existing:\n",
    "                print(f\"\\nFound in models directory:\")\n",
    "                for item in existing[:5]:\n",
    "                    item_type = \"dir\" if item.is_dir() else \"file\"\n",
    "                    print(f\"   â€¢ {item.name} ({item_type})\")\n",
    "        \n",
    "        llm_engine = None\n",
    "    else:\n",
    "        # Display available models\n",
    "        print(f\"ðŸ“¦ Found {len(model_dirs)} model(s):\")\n",
    "        for model_info in model_dirs.values():\n",
    "            print(f\"   â€¢ {model_info['key']}: {model_info['params']} ({model_info['quant']}, ~{model_info['size_gb']}GB)\")\n",
    "            print(f\"     Path: {model_info['path']}\")\n",
    "        \n",
    "        # Select model (prefer llama3.1-8b-4bit_mlx, fallback to first available)\n",
    "        model_keys = list(model_dirs.keys())\n",
    "        selected_key = 'llama3.1-8b-4bit_mlx' if 'llama3.1-8b-4bit_mlx' in model_keys else model_keys[0]\n",
    "        \n",
    "        print(f\"\\nðŸŽ¯ Selected: {selected_key}\")\n",
    "        \n",
    "        # Initialize LLM engine with detected directory path\n",
    "        try:\n",
    "            selected_info = model_dirs[selected_key]\n",
    "            llm_engine = LocalLLMEngine(models_path, selected_key)\n",
    "            \n",
    "            if llm_engine.available:\n",
    "                print(f\"âœ… Model loaded: {llm_engine.model_info['name']}\")\n",
    "                print(f\"   Parameters: {llm_engine.model_info['params']}, Quantization: {llm_engine.model_info['quant']}\")\n",
    "                print(f\"   Location: {llm_engine.model_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to initialize: {e}\")\n",
    "            llm_engine = None\n",
    "\n",
    "# Initialize other Section 5 components if LLM engine available\n",
    "if llm_engine and llm_engine.available:\n",
    "    query_classifier = EnhancedQueryClassifier(llm_engine)\n",
    "    doc_finder = DocumentFinder(collections_manager) if 'collections_manager' in globals() else None\n",
    "    query_expander = FinancialQueryExpander(llm_engine)\n",
    "    results_processor = ResultsProcessor(cross_encoder if 'cross_encoder' in globals() else None)\n",
    "    \n",
    "    print(\"\\nâœ… Section 5 components initialized:\")\n",
    "    print(\"   â€¢ LocalLLMEngine\")\n",
    "    print(\"   â€¢ EnhancedQueryClassifier\")\n",
    "    print(\"   â€¢ DocumentFinder\")\n",
    "    print(\"   â€¢ FinancialQueryExpander\")\n",
    "    print(\"   â€¢ ResultsProcessor\")\n",
    "    \n",
    "    # Quick LLM test\n",
    "    print(\"\\nðŸ§ª Testing LLM...\")\n",
    "    test_response = llm_engine.generate_response(\"Say 'OK' in one word.\", temperature=0.0, max_tokens=5)\n",
    "    \n",
    "    if test_response and not test_response.startswith(\"Error\"):\n",
    "        print(f\"   âœ… LLM responding: {test_response[:20]}\")\n",
    "    else:\n",
    "        print(f\"   âš ï¸ Test response: {test_response}\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ LLM engine not available - Section 5 components not initialized\")\n",
    "    print(\"   Some RAG features will be limited without LLM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3739472",
   "metadata": {
    "id": "e3739472"
   },
   "source": [
    "## 6. Enhanced User Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1344dbd0",
   "metadata": {
    "id": "1344dbd0"
   },
   "source": [
    "#### 6.0 Enhanced Citation and Cross-Document Analysis Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2dc90b8f",
   "metadata": {
    "id": "2dc90b8f",
    "outputId": "61626290-2eff-4237-f650-71e609d7dfaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Citation and Cross-Document Analysis Systems initialized\n",
      "   â€¢ EnhancedCitationSystem: Generate structured citations\n",
      "   â€¢ CrossDocumentAnalyzer: Assess coverage and consistency\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Citation and Cross-Document Analysis Systems\n",
    "class EnhancedCitationSystem:\n",
    "    \"\"\"Generate precise citations with confidence scores from search results\"\"\"\n",
    "    \n",
    "    def __init__(self, collections_manager):\n",
    "        self.manager = collections_manager\n",
    "    \n",
    "    def generate_precise_citations(self, references: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Create structured citations from search results\"\"\"\n",
    "        citations = []\n",
    "        \n",
    "        for i, ref in enumerate(references):\n",
    "            if not ref.get('text'):\n",
    "                continue\n",
    "            \n",
    "            metadata = ref.get('metadata', {})\n",
    "            confidence = ref.get('confidence', 0.0)\n",
    "            \n",
    "            citation = {\n",
    "                'id': f\"cite_{i+1}\",\n",
    "                'filename': metadata.get('filename', 'Unknown'),\n",
    "                'collection': ref.get('collection', metadata.get('collection', 'Unknown')),\n",
    "                'chunk_id': metadata.get('chunk_id'),\n",
    "                'page_num': metadata.get('page_num'),\n",
    "                'confidence_score': confidence,\n",
    "                'relevance_score': ref.get('final_score', 0.0),\n",
    "                'text_snippet': ref['text'][:150] + '...' if len(ref['text']) > 150 else ref['text'],\n",
    "                'source_type': 'document_chunk',\n",
    "                'verification_status': 'verified' if confidence > 0.7 else 'low_confidence'\n",
    "            }\n",
    "            \n",
    "            citations.append(citation)\n",
    "        \n",
    "        return citations\n",
    "    \n",
    "    def format_citation_text(self, citation: Dict) -> str:\n",
    "        \"\"\"Format single citation for inline display\"\"\"\n",
    "        return (f\"[{citation['id']}: {citation['filename']} \"\n",
    "                f\"from {citation['collection']}, \"\n",
    "                f\"confidence: {citation['confidence_score']:.2f}]\")\n",
    "\n",
    "\n",
    "class CrossDocumentAnalyzer:\n",
    "    \"\"\"Analyze patterns and consistency across multiple document sources\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_engine, collections_manager):\n",
    "        self.llm_engine = llm_engine\n",
    "        self.manager = collections_manager\n",
    "    \n",
    "    def analyze_cross_document_patterns(self, references: List[Dict], query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze document coverage and consistency across sources\"\"\"\n",
    "        \n",
    "        # Group references by document\n",
    "        docs = {}\n",
    "        for ref in references:\n",
    "            filename = ref.get('metadata', {}).get('filename', 'Unknown')\n",
    "            if filename not in docs:\n",
    "                docs[filename] = {\n",
    "                    'chunks': [],\n",
    "                    'confidence_sum': 0,\n",
    "                    'collection': ref.get('collection', 'Unknown')\n",
    "                }\n",
    "            \n",
    "            docs[filename]['chunks'].append(ref)\n",
    "            docs[filename]['confidence_sum'] += ref.get('confidence', 0)\n",
    "        \n",
    "        # Calculate per-document insights\n",
    "        insights = []\n",
    "        for filename, data in docs.items():\n",
    "            chunk_count = len(data['chunks'])\n",
    "            avg_confidence = data['confidence_sum'] / chunk_count if chunk_count else 0\n",
    "            \n",
    "            # Simple coverage assessment\n",
    "            if chunk_count > 2:\n",
    "                coverage = 'high'\n",
    "            elif chunk_count > 1:\n",
    "                coverage = 'medium'\n",
    "            else:\n",
    "                coverage = 'low'\n",
    "            \n",
    "            insights.append({\n",
    "                'filename': filename,\n",
    "                'collection': data['collection'],\n",
    "                'chunk_count': chunk_count,\n",
    "                'average_confidence': avg_confidence,\n",
    "                'coverage_assessment': coverage\n",
    "            })\n",
    "        \n",
    "        # Assess overall consistency\n",
    "        consistency = self._assess_consistency(references)\n",
    "        \n",
    "        # Determine information completeness\n",
    "        total_refs = len(references)\n",
    "        if total_refs > 5:\n",
    "            completeness = 'high'\n",
    "        elif total_refs > 2:\n",
    "            completeness = 'medium'\n",
    "        else:\n",
    "            completeness = 'low'\n",
    "        \n",
    "        return {\n",
    "            'total_documents': len(docs),\n",
    "            'document_insights': insights,\n",
    "            'cross_document_consistency': consistency,\n",
    "            'information_completeness': completeness\n",
    "        }\n",
    "    \n",
    "    def _assess_consistency(self, references: List[Dict]) -> str:\n",
    "        \"\"\"Simple consistency check based on confidence variance\"\"\"\n",
    "        if len(references) < 2:\n",
    "            return 'insufficient_data'\n",
    "        \n",
    "        confidences = [ref.get('confidence', 0) for ref in references]\n",
    "        avg_conf = np.mean(confidences)\n",
    "        var_conf = np.var(confidences)\n",
    "        \n",
    "        # High consistency: high average, low variance\n",
    "        if avg_conf > 0.7 and var_conf < 0.1:\n",
    "            return 'high'\n",
    "        elif avg_conf > 0.5:\n",
    "            return 'medium'\n",
    "        else:\n",
    "            return 'low'\n",
    "\n",
    "\n",
    "print(\"âœ… Citation and Cross-Document Analysis Systems initialized\")\n",
    "print(\"   â€¢ EnhancedCitationSystem: Generate structured citations\")\n",
    "print(\"   â€¢ CrossDocumentAnalyzer: Assess coverage and consistency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1458e23",
   "metadata": {
    "id": "c1458e23"
   },
   "source": [
    "#### 6.0.1 Cross-Reference Comparison System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1780998e",
   "metadata": {
    "id": "1780998e",
    "outputId": "be504afc-686f-4ecd-b1d8-2da54043854e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Cross-Reference Comparison System initialized!\n",
      "   â€¢ Document-to-collection comparison\n",
      "   â€¢ Misalignment detection with severity scoring\n",
      "   â€¢ Query intent detection\n",
      "   â€¢ Confidence-scored reports\n"
     ]
    }
   ],
   "source": [
    "# Cross-Reference Comparison System\n",
    "\n",
    "class CrossReferenceComparator:\n",
    "    \"\"\"\n",
    "    Compare target documents against reference collections (e.g., regulations).\n",
    "    Identifies misalignments and compliance issues.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm_engine, collections_manager, embedding_model, hybrid_search=None):\n",
    "        self.llm_engine = llm_engine\n",
    "        self.manager = collections_manager\n",
    "        self.embedding_model = embedding_model\n",
    "        self.hybrid_search = hybrid_search\n",
    "    \n",
    "    def compare_documents(self, target_doc: str, reference_collection: str,\n",
    "                         target_collection: str = None, top_k: int = 20) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Main comparison entry point.\n",
    "        Returns comprehensive report with misalignments and confidence score.\n",
    "        \"\"\"\n",
    "        # Step 1: Get target document content\n",
    "        target_content = self._get_document_content(target_doc, target_collection)\n",
    "        if not target_content:\n",
    "            return {\n",
    "                'status': 'error',\n",
    "                'message': f\"Document not found: {target_doc}\",\n",
    "                'target_doc': target_doc,\n",
    "                'reference_collection': reference_collection\n",
    "            }\n",
    "        \n",
    "        # Step 2: Generate document overview\n",
    "        overview = self._generate_overview(target_doc, target_content)\n",
    "        \n",
    "        # Step 3: Extract and analyze key claims\n",
    "        claims = self._extract_claims(target_content, max_claims=15)\n",
    "        misalignments = []\n",
    "        \n",
    "        for claim in claims:\n",
    "            # Get relevant reference passages\n",
    "            refs = self._retrieve_references(claim['text'], reference_collection, top_k)\n",
    "            \n",
    "            # Check alignment\n",
    "            analysis = self._check_alignment(claim, refs, target_doc, reference_collection)\n",
    "            if analysis['has_misalignment']:\n",
    "                misalignments.append(analysis)\n",
    "        \n",
    "        # Step 4: Build final report\n",
    "        confidence = self._calculate_confidence(misalignments, len(claims))\n",
    "        return self._build_report(target_doc, reference_collection, overview, \n",
    "                                  misalignments, confidence, len(claims))\n",
    "    \n",
    "    def detect_query_intent(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Detect if query requests cross-reference comparison.\"\"\"\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        # Check for comparison patterns\n",
    "        comparison_verbs = ['compare', 'check', 'analyze', 'review', 'verify', 'assess']\n",
    "        comparison_indicators = ['against', 'with', 'compliance', 'conflict', 'misalign']\n",
    "        \n",
    "        has_verb = any(verb in query_lower for verb in comparison_verbs)\n",
    "        has_indicator = any(ind in query_lower for ind in comparison_indicators)\n",
    "        \n",
    "        if not (has_verb or has_indicator):\n",
    "            return {'is_comparison_query': False}\n",
    "        \n",
    "        # Extract target document and reference collection\n",
    "        intent = {\n",
    "            'is_comparison_query': True,\n",
    "            'target_document': None,\n",
    "            'reference_collection': None,\n",
    "            'original_query': query\n",
    "        }\n",
    "        \n",
    "        # Pattern matching for \"compare X against Y\"\n",
    "        patterns = [\n",
    "            r'(?:compare|check|analyze)\\s+([^\\s]+(?:\\s+[^\\s]+){0,5}?)\\s+(?:against|with)\\s+(?:the\\s+)?([^\\s]+)',\n",
    "            r'(?:does|is)\\s+([^\\s]+(?:\\s+[^\\s]+){0,5}?)\\s+(?:align|comply)\\s+(?:with|to)\\s+([^\\s]+)'\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, query_lower)\n",
    "            if match:\n",
    "                intent['target_document'] = match.group(1).strip()\n",
    "                intent['reference_collection'] = self._map_collection(match.group(2).strip())\n",
    "                break\n",
    "        \n",
    "        return intent\n",
    "    \n",
    "    def _get_document_content(self, doc_name: str, collection_name: str = None) -> List[Dict]:\n",
    "        \"\"\"Retrieve all chunks for specified document.\"\"\"\n",
    "        collections = [collection_name] if collection_name else self.manager.get_available_collections()\n",
    "        \n",
    "        all_chunks = []\n",
    "        for coll_name in collections:\n",
    "            collection = self.manager.get_collection(coll_name)\n",
    "            if not collection:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                results = collection.get()\n",
    "                for i, metadata in enumerate(results.get('metadatas', [])):\n",
    "                    filename = metadata.get('filename', '')\n",
    "                    if doc_name.lower() in filename.lower():\n",
    "                        all_chunks.append({\n",
    "                            'text': results['documents'][i],\n",
    "                            'metadata': metadata,\n",
    "                            'collection': coll_name\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                print(f\"Error retrieving from {coll_name}: {e}\")\n",
    "        \n",
    "        # Sort by chunk offset\n",
    "        all_chunks.sort(key=lambda x: x['metadata'].get('start_offset', 0))\n",
    "        return all_chunks\n",
    "    \n",
    "    def _generate_overview(self, doc_name: str, chunks: List[Dict]) -> str:\n",
    "        \"\"\"Generate document overview from representative samples.\"\"\"\n",
    "        if not chunks:\n",
    "            return \"No content available.\"\n",
    "        \n",
    "        # Sample beginning, middle, end for coverage\n",
    "        total = len(chunks)\n",
    "        if total <= 10:\n",
    "            samples = chunks\n",
    "        else:\n",
    "            samples = chunks[:3] + chunks[total//3:total//3+3] + chunks[-3:]\n",
    "        \n",
    "        sample_text = \" \".join([c['text'] for c in samples])[:5000]\n",
    "        \n",
    "        prompt = f\"\"\"Provide a 5-6 sentence overview of this document.\n",
    "\n",
    "Document: {doc_name}\n",
    "Content: {sample_text}\n",
    "\n",
    "Requirements:\n",
    "- Be factual and specific\n",
    "- Cover main topics and scope\n",
    "- Use information from content only\n",
    "- No subjective interpretation\n",
    "\n",
    "Overview:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            return self.llm_engine.generate_response(prompt, temperature=0.1, max_tokens=500).strip()\n",
    "        except:\n",
    "            return f\"Document contains {len(chunks)} sections covering multiple topics.\"\n",
    "    \n",
    "    def _extract_claims(self, chunks: List[Dict], max_claims: int = 15) -> List[Dict]:\n",
    "        \"\"\"Extract key claims from document chunks.\"\"\"\n",
    "        claims = []\n",
    "        for i, chunk in enumerate(chunks[:max_claims]):\n",
    "            text = chunk['text']\n",
    "            \n",
    "            # Extend context if chunk is short\n",
    "            if len(text) < 500 and i + 1 < len(chunks):\n",
    "                text += \" \" + chunks[i + 1]['text'][:500]\n",
    "            \n",
    "            claims.append({\n",
    "                'id': f\"claim_{i+1}\",\n",
    "                'text': text[:3000],\n",
    "                'metadata': chunk['metadata'],\n",
    "                'chunk_index': i\n",
    "            })\n",
    "        \n",
    "        return claims\n",
    "    \n",
    "    def _retrieve_references(self, claim_text: str, ref_collection: str, top_k: int) -> List[Dict]:\n",
    "        \"\"\"Get relevant reference passages for comparison.\"\"\"\n",
    "        try:\n",
    "            if self.hybrid_search:\n",
    "                return self.hybrid_search.search(claim_text, ref_collection, top_k)\n",
    "            \n",
    "            # Fallback to semantic search\n",
    "            collection = self.manager.get_collection(ref_collection)\n",
    "            if not collection:\n",
    "                return []\n",
    "            \n",
    "            query_emb = self.embedding_model.encode([claim_text]).tolist()\n",
    "            results = collection.query(query_embeddings=query_emb, n_results=top_k)\n",
    "            \n",
    "            return [{\n",
    "                'text': results['documents'][0][i],\n",
    "                'metadata': results['metadatas'][0][i],\n",
    "                'score': 1.0 - results['distances'][0][i]\n",
    "            } for i in range(len(results['documents'][0]))]\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Reference retrieval error: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _check_alignment(self, claim: Dict, refs: List[Dict], \n",
    "                        target_doc: str, ref_collection: str) -> Dict[str, Any]:\n",
    "        \"\"\"Check if claim aligns with or conflicts with references.\"\"\"\n",
    "        if not refs:\n",
    "            return {\n",
    "                'has_misalignment': True,\n",
    "                'claim_id': claim['id'],\n",
    "                'claim_text': claim['text'][:1000],\n",
    "                'misalignment_type': 'no_reference_found',\n",
    "                'explanation': 'No relevant reference material found.',\n",
    "                'severity': 'medium',\n",
    "                'references': []\n",
    "            }\n",
    "        \n",
    "        # Build reference context\n",
    "        ref_context = \"\\n\\n\".join([\n",
    "            f\"Ref {i+1} ({ref['metadata'].get('filename', 'Unknown')}, \"\n",
    "            f\"Page {ref['metadata'].get('page_num', 'N/A')}): {ref['text'][:800]}\"\n",
    "            for i, ref in enumerate(refs[:3])\n",
    "        ])\n",
    "        \n",
    "        # LLM analysis\n",
    "        prompt = f\"\"\"Compare target statement against reference regulations.\n",
    "\n",
    "TARGET: {target_doc}\n",
    "STATEMENT: {claim['text'][:2000]}\n",
    "\n",
    "REFERENCES ({ref_collection}):\n",
    "{ref_context}\n",
    "\n",
    "Task: Identify MISALIGNMENTS or CONFLICTS only. If aligned, respond \"ALIGNED\".\n",
    "\n",
    "Requirements:\n",
    "- Base analysis ONLY on provided references\n",
    "- Quote specific phrases from both target and references\n",
    "- Focus on substance, not just wording\n",
    "- State clearly if insufficient context\n",
    "\n",
    "Format:\n",
    "STATUS: [ALIGNED or MISALIGNED]\n",
    "SEVERITY: [high/medium/low - if MISALIGNED]\n",
    "EXPLANATION: [Detailed if MISALIGNED, else \"No misalignment detected.\"]\n",
    "\n",
    "Response:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.llm_engine.generate_response(prompt, temperature=0.1)\n",
    "            \n",
    "            if 'status: aligned' in response.lower():\n",
    "                return {'has_misalignment': False}\n",
    "            \n",
    "            # Extract severity\n",
    "            severity = 'medium'\n",
    "            if 'severity: high' in response.lower():\n",
    "                severity = 'high'\n",
    "            elif 'severity: low' in response.lower():\n",
    "                severity = 'low'\n",
    "            \n",
    "            # Extract explanation\n",
    "            explanation_match = re.search(r'explanation[:\\s]+(.*)', response, re.IGNORECASE | re.DOTALL)\n",
    "            explanation = explanation_match.group(1).strip() if explanation_match else response.strip()\n",
    "            \n",
    "            # Format references\n",
    "            formatted_refs = [{\n",
    "                'filename': ref['metadata'].get('filename', 'Unknown'),\n",
    "                'page_num': ref['metadata'].get('page_num', 'N/A'),\n",
    "                'collection': ref_collection,\n",
    "                'text_snippet': ref['text'][:600]\n",
    "            } for ref in refs[:3]]\n",
    "            \n",
    "            return {\n",
    "                'has_misalignment': True,\n",
    "                'claim_id': claim['id'],\n",
    "                'claim_text': claim['text'][:1000],\n",
    "                'claim_source': {\n",
    "                    'filename': target_doc,\n",
    "                    'page_num': claim['metadata'].get('page_num', 'N/A'),\n",
    "                    'chunk_id': claim['metadata'].get('chunk_id', 'N/A')\n",
    "                },\n",
    "                'misalignment_type': 'regulatory_conflict',\n",
    "                'severity': severity,\n",
    "                'explanation': explanation,\n",
    "                'references': formatted_refs\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Alignment analysis error: {e}\")\n",
    "            return {'has_misalignment': False}\n",
    "    \n",
    "    def _calculate_confidence(self, misalignments: List[Dict], total_claims: int) -> float:\n",
    "        \"\"\"Calculate overall analysis confidence.\"\"\"\n",
    "        if total_claims == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        coverage_score = min(total_claims / 15, 1.0)\n",
    "        detection_score = 0.7 if misalignments else 0.5\n",
    "        \n",
    "        if misalignments:\n",
    "            severity_map = {'high': 0.9, 'medium': 0.7, 'low': 0.6}\n",
    "            avg_severity = np.mean([severity_map.get(m.get('severity', 'medium'), 0.7) \n",
    "                                   for m in misalignments])\n",
    "        else:\n",
    "            avg_severity = 0.8\n",
    "        \n",
    "        return round(coverage_score * 0.3 + detection_score * 0.3 + avg_severity * 0.4, 3)\n",
    "    \n",
    "    def _build_report(self, target_doc: str, ref_collection: str, overview: str,\n",
    "                     misalignments: List[Dict], confidence: float, \n",
    "                     total_claims: int) -> Dict[str, Any]:\n",
    "        \"\"\"Build final comparison report.\"\"\"\n",
    "        report = {\n",
    "            'title': f\"Cross-Reference Comparison: {target_doc} vs {ref_collection}\",\n",
    "            'target_document': target_doc,\n",
    "            'reference_collection': ref_collection,\n",
    "            'overview': overview,\n",
    "            'analysis_summary': {\n",
    "                'total_sections_analyzed': total_claims,\n",
    "                'misalignments_found': len(misalignments),\n",
    "                'confidence_score': confidence,\n",
    "                'confidence_level': 'High' if confidence > 0.75 else 'Medium' if confidence > 0.5 else 'Low'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if misalignments:\n",
    "            report['results'] = [{\n",
    "                'section': f\"Misalignment #{i+1}\",\n",
    "                'severity': m.get('severity', 'medium').upper(),\n",
    "                'target_location': m['claim_source'],\n",
    "                'target_statement': m['claim_text'],\n",
    "                'explanation': m['explanation'],\n",
    "                'reference_citations': m.get('references', [])\n",
    "            } for i, m in enumerate(misalignments)]\n",
    "        else:\n",
    "            report['results'] = [{\n",
    "                'section': 'Overall Assessment',\n",
    "                'message': 'No areas of misalignment detected',\n",
    "                'details': f'Analyzed {total_claims} sections. All content aligns with {ref_collection}.'\n",
    "            }]\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def _map_collection(self, reference_text: str) -> str:\n",
    "        \"\"\"Map natural language reference to collection name.\"\"\"\n",
    "        mappings = {\n",
    "            'pra': 'pra_rules',\n",
    "            'rules': 'pra_rules',\n",
    "            'regulations': 'pra_rules',\n",
    "            'earnings': 'earnings_transcripts',\n",
    "            'transcript': 'earnings_transcripts'\n",
    "        }\n",
    "        \n",
    "        ref_lower = reference_text.lower()\n",
    "        for key, collection in mappings.items():\n",
    "            if key in ref_lower:\n",
    "                return collection\n",
    "        \n",
    "        return reference_text\n",
    "\n",
    "print(\"âœ… Cross-Reference Comparison System initialized!\")\n",
    "print(\"   â€¢ Document-to-collection comparison\")\n",
    "print(\"   â€¢ Misalignment detection with severity scoring\")\n",
    "print(\"   â€¢ Query intent detection\")\n",
    "print(\"   â€¢ Confidence-scored reports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b34cae",
   "metadata": {
    "id": "01b34cae"
   },
   "source": [
    "#### 6.1 Enhanced Metadata Search and Summarization Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "050e4fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… EnhancedMetadataSearch and EnhancedSummarizer defined!\n",
      "   â€¢ Metadata queries (count, list, stats)\n",
      "   â€¢ Document summarization with citations\n",
      "   â€¢ Query-focused summaries\n",
      "âœ… AdvancedHybridSearchEngine defined\n"
     ]
    }
   ],
   "source": [
    "# Advanced Hybrid Search Engine with BM25 Index Management\n",
    "class AdvancedHybridSearchEngine:\n",
    "    \"\"\"Enhanced hybrid search with automatic BM25 index loading\"\"\"\n",
    "    \n",
    "    def __init__(self, collections_manager):\n",
    "        self.manager = collections_manager\n",
    "        self.fusion_engine = RRFFusionEngine()\n",
    "        self.loaded_collections = set()\n",
    "    \n",
    "    def ensure_bm25_indices_loaded(self, collection_names: List[str]):\n",
    "        \"\"\"Ensure BM25 indices are loaded for specified collections\"\"\"\n",
    "        for col_name in collection_names:\n",
    "            if col_name not in self.loaded_collections:\n",
    "                # Try to load existing index\n",
    "                if not self.manager.load_bm25_index(col_name):\n",
    "                    # Build new index if load fails\n",
    "                    print(f\"Building BM25 index for {col_name}...\")\n",
    "                    self.manager.build_bm25_index(col_name)\n",
    "                self.loaded_collections.add(col_name)\n",
    "    \n",
    "    def multi_collection_search(self, query: str, collection_names: List[str], \n",
    "                               top_k: int = 10) -> List[Dict]:\n",
    "        \"\"\"Search across multiple collections with fusion\"\"\"\n",
    "        # Ensure indices loaded\n",
    "        self.ensure_bm25_indices_loaded(collection_names)\n",
    "        \n",
    "        all_results = []\n",
    "        expanded_k = int(top_k * 2.5)\n",
    "        \n",
    "        for col_name in collection_names:\n",
    "            # Get BM25 results\n",
    "            bm25_results = self.manager.search_bm25(query, col_name, expanded_k)\n",
    "            \n",
    "            # Get semantic results\n",
    "            semantic_results = self.manager.semantic_search(query, col_name, expanded_k)\n",
    "            \n",
    "            # Fuse results\n",
    "            if bm25_results or semantic_results:\n",
    "                fused = self.fusion_engine.fuse(semantic_results, bm25_results)\n",
    "                \n",
    "                # Tag with source collection\n",
    "                for result in fused:\n",
    "                    result['source_collection'] = col_name\n",
    "                \n",
    "                all_results.extend(fused)\n",
    "        \n",
    "        # Re-sort all results\n",
    "        all_results.sort(key=lambda x: x.get('final_score', 0), reverse=True)\n",
    "        \n",
    "        return all_results[:top_k * len(collection_names)]\n",
    "\n",
    "# Add this cell before Section 6.4.4 (where SpecCompliantRAGSystem is defined)\n",
    "\n",
    "# ============================================================================\n",
    "# Enhanced Metadata Search System\n",
    "# ============================================================================\n",
    "\n",
    "class EnhancedMetadataSearch:\n",
    "    \"\"\"Handle metadata queries about files and collections\"\"\"\n",
    "    \n",
    "    def __init__(self, collections_manager, llm_engine):\n",
    "        self.manager = collections_manager\n",
    "        self.llm_engine = llm_engine\n",
    "    \n",
    "    def execute_metadata_query(self, query: str, collections: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Execute metadata query and return formatted results\"\"\"\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        # Detect query type\n",
    "        if any(word in query_lower for word in ['count', 'how many']):\n",
    "            return self._count_files(query, collections)\n",
    "        elif any(word in query_lower for word in ['list', 'show', 'display']):\n",
    "            return self._list_files(query, collections)\n",
    "        else:\n",
    "            return self._general_metadata_query(query, collections)\n",
    "    \n",
    "    def _count_files(self, query: str, collections: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Count files matching criteria\"\"\"\n",
    "        results = []\n",
    "        total_files = 0\n",
    "        \n",
    "        for col_name in collections:\n",
    "            collection = self.manager.get_collection(col_name)\n",
    "            if not collection:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                data = collection.get(include=['metadatas'])\n",
    "                filenames = set()\n",
    "                \n",
    "                for metadata in data.get('metadatas', []):\n",
    "                    if metadata and metadata.get('filename'):\n",
    "                        filenames.add(metadata['filename'])\n",
    "                \n",
    "                count = len(filenames)\n",
    "                total_files += count\n",
    "                results.append(f\"â€¢ {col_name}: {count} files\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                results.append(f\"â€¢ {col_name}: Error ({str(e)[:50]})\")\n",
    "        \n",
    "        answer = f\"Found {total_files} unique files across {len(collections)} collection(s):\\n\\n\" + \"\\n\".join(results)\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'search_type': 'Metadata Count Query',\n",
    "            'answer': answer,\n",
    "            'references': [],\n",
    "            'confidence': 1.0,\n",
    "            'collections_searched': collections\n",
    "        }\n",
    "    \n",
    "    def _list_files(self, query: str, collections: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"List files in collections\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for col_name in collections:\n",
    "            collection = self.manager.get_collection(col_name)\n",
    "            if not collection:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                data = collection.get(include=['metadatas'])\n",
    "                filenames = sorted(set(\n",
    "                    m.get('filename') for m in data.get('metadatas', [])\n",
    "                    if m and m.get('filename')\n",
    "                ))\n",
    "                \n",
    "                if filenames:\n",
    "                    results.append(f\"\\n{col_name} ({len(filenames)} files):\")\n",
    "                    for i, filename in enumerate(filenames, 1):\n",
    "                        results.append(f\"  {i}. {filename}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                results.append(f\"\\n{col_name}: Error - {str(e)[:50]}\")\n",
    "        \n",
    "        answer = \"Files in selected collections:\\n\" + \"\\n\".join(results) if results else \"No files found\"\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'search_type': 'File List Query',\n",
    "            'answer': answer,\n",
    "            'references': [],\n",
    "            'confidence': 1.0,\n",
    "            'collections_searched': collections\n",
    "        }\n",
    "    \n",
    "    def _general_metadata_query(self, query: str, collections: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Handle general metadata questions\"\"\"\n",
    "        # Gather metadata statistics\n",
    "        stats = []\n",
    "        \n",
    "        for col_name in collections:\n",
    "            collection = self.manager.get_collection(col_name)\n",
    "            if not collection:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                data = collection.get(include=['metadatas'])\n",
    "                metadatas = data.get('metadatas', [])\n",
    "                \n",
    "                filenames = set(m.get('filename') for m in metadatas if m and m.get('filename'))\n",
    "                doc_types = set(m.get('doc_type') for m in metadatas if m and m.get('doc_type'))\n",
    "                \n",
    "                stats.append({\n",
    "                    'collection': col_name,\n",
    "                    'total_chunks': len(metadatas),\n",
    "                    'unique_files': len(filenames),\n",
    "                    'doc_types': list(doc_types)\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                stats.append({'collection': col_name, 'error': str(e)})\n",
    "        \n",
    "        # Format answer\n",
    "        answer_lines = [\"Database Statistics:\\n\"]\n",
    "        for stat in stats:\n",
    "            if 'error' in stat:\n",
    "                answer_lines.append(f\"â€¢ {stat['collection']}: Error\")\n",
    "            else:\n",
    "                answer_lines.append(f\"â€¢ {stat['collection']}:\")\n",
    "                answer_lines.append(f\"  - Files: {stat['unique_files']}\")\n",
    "                answer_lines.append(f\"  - Chunks: {stat['total_chunks']}\")\n",
    "                answer_lines.append(f\"  - Types: {', '.join(stat['doc_types'])}\")\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'search_type': 'Metadata Query',\n",
    "            'answer': \"\\n\".join(answer_lines),\n",
    "            'references': [],\n",
    "            'confidence': 1.0,\n",
    "            'collections_searched': collections\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Enhanced Summarizer System\n",
    "# ============================================================================\n",
    "\n",
    "class EnhancedSummarizer:\n",
    "    \"\"\"Generate focused summaries with citation tracking\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_engine, citation_system):\n",
    "        self.llm_engine = llm_engine\n",
    "        self.citation_system = citation_system\n",
    "    \n",
    "    def generate_summary(self, search_results: List[Dict], query: str, \n",
    "                        word_count: int = 300) -> Dict[str, Any]:\n",
    "        \"\"\"Generate summary from search results with citations\"\"\"\n",
    "        \n",
    "        if not search_results:\n",
    "            return {\n",
    "                'summary': 'No content available to summarize.',\n",
    "                'confidence': 0.0,\n",
    "                'word_count': 0,\n",
    "                'target_words': word_count,\n",
    "                'citations_used': [],\n",
    "                'sources_count': 0\n",
    "            }\n",
    "        \n",
    "        # Generate citations\n",
    "        citations = self.citation_system.generate_precise_citations(search_results[:10])\n",
    "        \n",
    "        # Build context from results\n",
    "        context_parts = []\n",
    "        for i, result in enumerate(search_results[:10]):\n",
    "            if not result.get('text'):\n",
    "                continue\n",
    "            \n",
    "            citation = citations[i] if i < len(citations) else {'id': f'cite_{i+1}'}\n",
    "            filename = citation.get('filename', 'Unknown')\n",
    "            page = citation.get('page_num', 'N/A')\n",
    "            \n",
    "            context_parts.append(\n",
    "                f\"[{citation['id']} - {filename}, Page {page}]:\\n{result['text'][:800]}\"\n",
    "            )\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)[:5000]\n",
    "        \n",
    "        # Generate summary\n",
    "        prompt = f\"\"\"Provide a {word_count}-word summary of the following content.\n",
    "\n",
    "CONTENT:\n",
    "{context}\n",
    "\n",
    "QUERY FOCUS: {query}\n",
    "\n",
    "REQUIREMENTS:\n",
    "- Target length: {word_count} words (Â±20%)\n",
    "- Use citation format: (cite_X)\n",
    "- Focus on query-relevant information\n",
    "- Use professional, clear language\n",
    "- Cite sources for key facts\n",
    "\n",
    "SUMMARY:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            summary = self.llm_engine.generate_response(prompt, temperature=0.1, max_tokens=word_count*2)\n",
    "            \n",
    "            # Calculate statistics\n",
    "            actual_words = len(summary.split())\n",
    "            confidence = min(actual_words / word_count, 1.0) if word_count > 0 else 0.5\n",
    "            \n",
    "            return {\n",
    "                'summary': summary,\n",
    "                'confidence': confidence,\n",
    "                'word_count': actual_words,\n",
    "                'target_words': word_count,\n",
    "                'citations_used': citations,\n",
    "                'sources_count': len(search_results),\n",
    "                'query_focus': query\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'summary': f'Error generating summary: {str(e)}',\n",
    "                'confidence': 0.0,\n",
    "                'word_count': 0,\n",
    "                'target_words': word_count,\n",
    "                'citations_used': [],\n",
    "                'sources_count': 0\n",
    "            }\n",
    "\n",
    "\n",
    "print(\"âœ… EnhancedMetadataSearch and EnhancedSummarizer defined!\")\n",
    "print(\"   â€¢ Metadata queries (count, list, stats)\")\n",
    "print(\"   â€¢ Document summarization with citations\")\n",
    "print(\"   â€¢ Query-focused summaries\")\n",
    "\n",
    "print(\"âœ… AdvancedHybridSearchEngine defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190b5c27",
   "metadata": {
    "id": "190b5c27"
   },
   "source": [
    "#### 6.2 Reliability Enforcement System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c3cd35cd",
   "metadata": {
    "id": "c3cd35cd",
    "outputId": "8fee83c0-1874-48bd-d4d5-aaf06b17fef4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Reliability Enforcement System initialized!\n",
      "   â€¢ Semantic similarity-based grounding validation\n",
      "   â€¢ Fallback to word overlap if needed\n",
      "   â€¢ Claim extraction with transition phrase filtering\n"
     ]
    }
   ],
   "source": [
    "# Reliability Enforcement System\n",
    "\n",
    "class ReliabilityEnforcer:\n",
    "    \"\"\"Validate response grounding using semantic similarity\"\"\"\n",
    "\n",
    "    def __init__(self, llm_engine):\n",
    "        self.llm_engine = llm_engine\n",
    "\n",
    "    def validate_response_grounding(self, query: str, answer: str, \n",
    "                                   references: List[Dict]) -> Dict[str, Any]:\n",
    "        \"\"\"Validate that response is grounded in source materials\"\"\"\n",
    "\n",
    "        if not references:\n",
    "            return {\n",
    "                'is_grounded': False,\n",
    "                'grounding_score': 0.0,\n",
    "                'issues': ['No source references provided'],\n",
    "                'confidence': 0.0,\n",
    "                'total_claims_analyzed': 0\n",
    "            }\n",
    "\n",
    "        # Extract claims\n",
    "        claims = self._extract_key_claims(answer)\n",
    "        if not claims:\n",
    "            return {\n",
    "                'is_grounded': True,\n",
    "                'grounding_score': 1.0,\n",
    "                'issues': [],\n",
    "                'confidence': 1.0,\n",
    "                'total_claims_analyzed': 0\n",
    "            }\n",
    "\n",
    "        # Check grounding for each claim\n",
    "        grounding_scores = []\n",
    "        issues = []\n",
    "\n",
    "        for claim in claims:\n",
    "            result = self._check_claim_grounding(claim, references)\n",
    "            score = result['grounding_score']\n",
    "            grounding_scores.append(score)\n",
    "\n",
    "            if score < 0.5:\n",
    "                issues.append(f\"Weak grounding: {claim[:60]}...\")\n",
    "\n",
    "        # Calculate overall score\n",
    "        overall_score = np.mean(grounding_scores) if grounding_scores else 0.0\n",
    "        is_grounded = overall_score >= 0.6 and len(issues) == 0\n",
    "\n",
    "        return {\n",
    "            'is_grounded': is_grounded,\n",
    "            'grounding_score': overall_score,\n",
    "            'issues': issues,\n",
    "            'confidence': min(overall_score, 1.0),\n",
    "            'total_claims_analyzed': len(claims)\n",
    "        }\n",
    "\n",
    "    def _extract_key_claims(self, answer: str) -> List[str]:\n",
    "        \"\"\"Extract key factual claims from answer\"\"\"\n",
    "        # Split on sentence boundaries with lookbehind/lookahead\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', answer)\n",
    "\n",
    "        claims = []\n",
    "        transition_phrases = ('however', 'therefore', 'moreover', 'furthermore', \n",
    "                            'additionally', 'in conclusion', 'as a result')\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if (len(sentence) > 25 and \n",
    "                not sentence.lower().startswith(transition_phrases)):\n",
    "                claims.append(sentence)\n",
    "\n",
    "        return claims[:5]  # Limit to top 5 for performance\n",
    "\n",
    "    def _check_claim_grounding(self, claim: str, references: List[Dict]) -> Dict[str, Any]:\n",
    "        \"\"\"Check if claim is grounded using semantic similarity\"\"\"\n",
    "        \n",
    "        # Try semantic similarity first\n",
    "        if 'embedding_model' in globals():\n",
    "            try:\n",
    "                return self._semantic_grounding(claim, references)\n",
    "            except Exception:\n",
    "                pass  # Fall through to word overlap\n",
    "        \n",
    "        # Fallback: word overlap\n",
    "        return self._word_overlap_grounding(claim, references)\n",
    "    \n",
    "    def _semantic_grounding(self, claim: str, references: List[Dict]) -> Dict[str, Any]:\n",
    "        \"\"\"Semantic similarity-based grounding check\"\"\"\n",
    "        claim_emb = embedding_model.encode([claim])[0]\n",
    "        \n",
    "        max_similarity = 0.0\n",
    "        best_ref = None\n",
    "        \n",
    "        for ref in references:\n",
    "            if 'text' not in ref or not ref.get('text'):\n",
    "                continue\n",
    "            \n",
    "            ref_emb = embedding_model.encode([ref['text'][:500]])[0]\n",
    "            similarity = np.dot(claim_emb, ref_emb) / (\n",
    "                np.linalg.norm(claim_emb) * np.linalg.norm(ref_emb)\n",
    "            )\n",
    "            \n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                best_ref = ref\n",
    "        \n",
    "        return {\n",
    "            'claim': claim,\n",
    "            'grounding_score': float(max_similarity),\n",
    "            'best_reference': best_ref.get('metadata', {}).get('filename', 'Unknown') if best_ref else None\n",
    "        }\n",
    "    \n",
    "    def _word_overlap_grounding(self, claim: str, references: List[Dict]) -> Dict[str, Any]:\n",
    "        \"\"\"Fallback word overlap grounding check\"\"\"\n",
    "        claim_words = set(claim.lower().split())\n",
    "        \n",
    "        max_overlap = 0.0\n",
    "        best_ref = None\n",
    "        \n",
    "        for ref in references:\n",
    "            if 'text' not in ref or not ref.get('text'):\n",
    "                continue\n",
    "            \n",
    "            ref_words = set(ref['text'].lower().split())\n",
    "            overlap = len(claim_words.intersection(ref_words))\n",
    "            overlap_ratio = overlap / len(claim_words) if claim_words else 0\n",
    "            \n",
    "            if overlap_ratio > max_overlap:\n",
    "                max_overlap = overlap_ratio\n",
    "                best_ref = ref\n",
    "        \n",
    "        return {\n",
    "            'claim': claim,\n",
    "            'grounding_score': max_overlap,\n",
    "            'best_reference': best_ref.get('metadata', {}).get('filename', 'Unknown') if best_ref else None\n",
    "        }\n",
    "\n",
    "print(\"âœ… Reliability Enforcement System initialized!\")\n",
    "print(\"   â€¢ Semantic similarity-based grounding validation\")\n",
    "print(\"   â€¢ Fallback to word overlap if needed\")\n",
    "print(\"   â€¢ Claim extraction with transition phrase filtering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2656ae5a",
   "metadata": {},
   "source": [
    "#### 6.3 Enhanced Multi-Collection Selector UI Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "91d762c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Additional RAG UI Components defined!\n",
      "   â€¢ AdvancedCollectionSelector: Multi-select with stats\n",
      "   â€¢ RAGProgressTracker: Progress tracking with cancellation\n",
      "   â€¢ ResultsFormatter: Reusable result formatting\n",
      "   â€¢ EnhancedResultsDisplay: Simplified widget manager\n"
     ]
    }
   ],
   "source": [
    "# Section 6.3: Enhanced Multi-Collection Selector UI Component\n",
    "\n",
    "# ============================================================================\n",
    "# 1. Advanced Collection Selector (KEEP - under 100 lines)\n",
    "# ============================================================================\n",
    "\n",
    "class AdvancedCollectionSelector:\n",
    "    \"\"\"Advanced collection selector with file counts and metadata\"\"\"\n",
    "\n",
    "    def __init__(self, collections_manager):\n",
    "        self.manager = collections_manager\n",
    "        self.selected_collections = []\n",
    "\n",
    "    def create_selector_widget(self):\n",
    "        \"\"\"Create advanced collection selector with real-time stats\"\"\"\n",
    "        available_collections = self.manager.get_available_collections()\n",
    "        collection_checkboxes = {}\n",
    "        collection_info = {}\n",
    "\n",
    "        for collection_name in available_collections:\n",
    "            try:\n",
    "                collection = self.manager.get_collection(collection_name)\n",
    "                count = collection.count()\n",
    "\n",
    "                # Get sample filenames\n",
    "                data = collection.get(include=['metadatas'], limit=5)\n",
    "                filenames = set()\n",
    "                for metadata in data.get('metadatas', []):\n",
    "                    if metadata and metadata.get('filename'):\n",
    "                        filenames.add(metadata['filename'])\n",
    "\n",
    "                info_text = f\"{collection_name} ({count:,} chunks, {len(filenames)} files)\"\n",
    "                collection_info[collection_name] = {\n",
    "                    'count': count,\n",
    "                    'files': len(filenames),\n",
    "                    'sample_files': list(filenames)[:3]\n",
    "                }\n",
    "\n",
    "            except Exception as e:\n",
    "                info_text = f\"{collection_name} (error: {str(e)[:50]})\"\n",
    "                collection_info[collection_name] = {'error': str(e)}\n",
    "\n",
    "            checkbox = widgets.Checkbox(\n",
    "                value=False,\n",
    "                description=info_text,\n",
    "                style={'description_width': 'initial'},\n",
    "                layout=widgets.Layout(width='100%')\n",
    "            )\n",
    "            collection_checkboxes[collection_name] = checkbox\n",
    "\n",
    "        selection_summary = widgets.HTML(value=\"<p><em>No collections selected</em></p>\")\n",
    "\n",
    "        def update_selection(*args):\n",
    "            selected = [name for name, cb in collection_checkboxes.items() if cb.value]\n",
    "            self.selected_collections = selected\n",
    "\n",
    "            if selected:\n",
    "                total_chunks = sum(collection_info[name].get('count', 0) for name in selected)\n",
    "                total_files = sum(collection_info[name].get('files', 0) for name in selected)\n",
    "\n",
    "                summary_html = f\"\"\"\n",
    "                <div style='background: #f0f9ff; padding: 10px; border-radius: 5px; border: 1px solid #0ea5e9;'>\n",
    "                    <strong>Selected Collections ({len(selected)}):</strong><br/>\n",
    "                    â€¢ Total chunks: {total_chunks:,}<br/>\n",
    "                    â€¢ Total files: {total_files}<br/>\n",
    "                    â€¢ Collections: {', '.join(selected)}\n",
    "                </div>\n",
    "                \"\"\"\n",
    "            else:\n",
    "                summary_html = \"<p><em>No collections selected</em></p>\"\n",
    "\n",
    "            selection_summary.value = summary_html\n",
    "\n",
    "        for cb in collection_checkboxes.values():\n",
    "            cb.observe(update_selection, names='value')\n",
    "\n",
    "        selector_ui = widgets.VBox([\n",
    "            widgets.HTML(\"<h4>ðŸ“‚ Select Collections for RAG Queries:</h4>\"),\n",
    "            widgets.VBox(list(collection_checkboxes.values())),\n",
    "            selection_summary\n",
    "        ])\n",
    "\n",
    "        return selector_ui, collection_checkboxes\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Progress Tracker (KEEP - under 100 lines)\n",
    "# ============================================================================\n",
    "\n",
    "class RAGProgressTracker:\n",
    "    \"\"\"Real-time progress tracking for RAG operations\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.progress_bar = widgets.FloatProgress(\n",
    "            value=0, min=0, max=100,\n",
    "            description='Progress:',\n",
    "            bar_style='info',\n",
    "            layout=widgets.Layout(width='100%')\n",
    "        )\n",
    "\n",
    "        self.status_label = widgets.HTML(value=\"Ready\")\n",
    "\n",
    "        self.cancel_flag = threading.Event()\n",
    "        self.cancel_button = widgets.Button(\n",
    "            description='Cancel',\n",
    "            button_style='danger',\n",
    "            disabled=True,\n",
    "            layout=widgets.Layout(width='100px')\n",
    "        )\n",
    "\n",
    "        self.cancel_button.on_click(self._cancel_operation)\n",
    "\n",
    "    def _cancel_operation(self, button):\n",
    "        \"\"\"Cancel current operation\"\"\"\n",
    "        self.cancel_flag.set()\n",
    "        self.update_status(\"Cancelling...\", 0)\n",
    "        button.disabled = True\n",
    "\n",
    "    def start_operation(self, operation_name: str):\n",
    "        \"\"\"Start tracking an operation\"\"\"\n",
    "        self.cancel_flag.clear()\n",
    "        self.cancel_button.disabled = False\n",
    "        self.update_status(f\"Starting {operation_name}...\", 10)\n",
    "\n",
    "    def update_progress(self, progress: float, message: str = \"\"):\n",
    "        \"\"\"Update progress (0-100)\"\"\"\n",
    "        self.progress_bar.value = progress\n",
    "        if message:\n",
    "            self.status_label.value = f\"<span style='color: #0066cc;'>{message}</span>\"\n",
    "\n",
    "    def update_status(self, message: str, progress: float = None):\n",
    "        \"\"\"Update status message\"\"\"\n",
    "        self.status_label.value = f\"<span style='color: #0066cc;'>{message}</span>\"\n",
    "        if progress is not None:\n",
    "            self.progress_bar.value = progress\n",
    "\n",
    "    def complete_operation(self, message: str = \"Operation completed\"):\n",
    "        \"\"\"Mark operation as complete\"\"\"\n",
    "        self.progress_bar.value = 100\n",
    "        self.cancel_button.disabled = True\n",
    "        self.status_label.value = f\"<span style='color: #008800;'>âœ… {message}</span>\"\n",
    "\n",
    "    def error_operation(self, message: str = \"Operation failed\"):\n",
    "        \"\"\"Mark operation as failed\"\"\"\n",
    "        self.cancel_button.disabled = True\n",
    "        self.status_label.value = f\"<span style='color: #cc0000;'>âŒ {message}</span>\"\n",
    "\n",
    "    def get_widgets(self):\n",
    "        \"\"\"Get progress widgets for display\"\"\"\n",
    "        return widgets.VBox([\n",
    "            self.progress_bar,\n",
    "            widgets.HBox([self.status_label, self.cancel_button])\n",
    "        ])\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3. Results Formatter (NEW - pure formatting logic)\n",
    "# ============================================================================\n",
    "\n",
    "class ResultsFormatter:\n",
    "    \"\"\"Format RAG results as text (no widget logic)\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def format_page_info(ref: Dict) -> str:\n",
    "        \"\"\"Format page number/range for display\"\"\"\n",
    "        page = ref.get('page', 'N/A')\n",
    "        page_range = ref.get('page_range', '')\n",
    "        \n",
    "        if page_range:\n",
    "            return f\" ðŸ“„ {page_range}\"\n",
    "        elif page != 'N/A':\n",
    "            return f\" ðŸ“„ Page {page}\"\n",
    "        return \"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def format_scores(ref: Dict) -> str:\n",
    "        \"\"\"Format confidence and relevance scores\"\"\"\n",
    "        scores = []\n",
    "        if 'confidence_score' in ref:\n",
    "            scores.append(f\"Confidence: {ref['confidence_score']:.1%}\")\n",
    "        if 'relevance_score' in ref:\n",
    "            scores.append(f\"Relevance: {ref['relevance_score']:.3f}\")\n",
    "        elif 'scores' in ref and ref['scores'].get('final_score'):\n",
    "            scores.append(f\"Score: {ref['scores']['final_score']:.3f}\")\n",
    "        \n",
    "        return f\" ({', '.join(scores)})\" if scores else \"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def format_reference(ref: Dict, index: int) -> str:\n",
    "        \"\"\"Format single reference with all details\"\"\"\n",
    "        filename = ref.get('filename', 'Unknown')\n",
    "        collection = ref.get('collection', 'Unknown')\n",
    "        citation_id = ref.get('citation_id', f'cite_{index}')\n",
    "        \n",
    "        # Build reference line with page info\n",
    "        page_info = ResultsFormatter.format_page_info(ref)\n",
    "        line = f\"{index}. [{citation_id}]{page_info} - {filename}\"\n",
    "        \n",
    "        # Add collection and scores on next line\n",
    "        if collection != 'Unknown':\n",
    "            scores = ResultsFormatter.format_scores(ref)\n",
    "            line += f\"\\n   Collection: {collection}{scores}\"\n",
    "        \n",
    "        # Add snippet if available\n",
    "        snippet = ref.get('text_snippet', '')\n",
    "        if snippet:\n",
    "            line += f\"\\n   â””â”€ {snippet}\"\n",
    "        \n",
    "        return line\n",
    "    \n",
    "    @staticmethod\n",
    "    def format_result(result: Dict) -> str:\n",
    "        \"\"\"Format complete RAG result as text\"\"\"\n",
    "        lines = []\n",
    "        \n",
    "        # Answer section\n",
    "        lines.append(\"ðŸŽ¯ ANSWER\")\n",
    "        lines.append(\"=\" * 80)\n",
    "        lines.append(result.get('answer', 'No answer generated'))\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        # Confidence & reliability\n",
    "        confidence = result.get('confidence', 0)\n",
    "        lines.append(f\"ðŸ“Š CONFIDENCE: {confidence:.1%}\")\n",
    "        \n",
    "        reliability = result.get('reliability_validation', {})\n",
    "        if reliability:\n",
    "            grounding_score = reliability.get('grounding_score', 0)\n",
    "            lines.append(f\"ðŸ”— GROUNDING: {grounding_score:.1%}\")\n",
    "            if not reliability.get('is_grounded', True):\n",
    "                lines.append(\"âš ï¸  WARNING: Response may not be fully grounded in sources\")\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        # Search metadata\n",
    "        search_type = result.get('search_type', 'Unknown')\n",
    "        collections = result.get('collections_searched', [])\n",
    "        lines.append(f\"ðŸ” SEARCH TYPE: {search_type}\")\n",
    "        lines.append(f\"ðŸ“‚ COLLECTIONS: {', '.join(collections)}\")\n",
    "        \n",
    "        # Processing stats (if available)\n",
    "        stats = result.get('processing_stats', {})\n",
    "        if stats:\n",
    "            total = stats.get('total_found', 0)\n",
    "            processed = stats.get('total_processed', 0)\n",
    "            lines.append(f\"ðŸ“ˆ RESULTS: {processed} processed from {total} found\")\n",
    "            \n",
    "            dist = stats.get('confidence_distribution', {})\n",
    "            if dist:\n",
    "                lines.append(f\"ðŸŽ¯ CONFIDENCE DIST: High: {dist.get('high', 0)}, \"\n",
    "                           f\"Medium: {dist.get('medium', 0)}, Low: {dist.get('low', 0)}\")\n",
    "        \n",
    "        lines.append(\"\")\n",
    "        \n",
    "        # References\n",
    "        references = result.get('references', [])\n",
    "        if references:\n",
    "            lines.append(f\"ðŸ“Ž SOURCES ({len(references)}):\")\n",
    "            lines.append(\"=\" * 50)\n",
    "            \n",
    "            for i, ref in enumerate(references[:15], 1):\n",
    "                lines.append(ResultsFormatter.format_reference(ref, i))\n",
    "                lines.append(\"\")\n",
    "        \n",
    "        # Summary stats (if available)\n",
    "        summary_stats = result.get('summary_stats', {})\n",
    "        if summary_stats:\n",
    "            lines.append(\"ðŸ“ SUMMARY STATS:\")\n",
    "            actual = summary_stats.get('actual_words', 0)\n",
    "            target = summary_stats.get('target_words', 0)\n",
    "            lines.append(f\"   Words: {actual}/{target}\")\n",
    "            lines.append(f\"   Sources: {summary_stats.get('sources_used', 0)}\")\n",
    "            \n",
    "            if summary_stats.get('unique_pages'):\n",
    "                lines.append(f\"   Unique Pages: {summary_stats['unique_pages']}\")\n",
    "            if summary_stats.get('query_focus'):\n",
    "                lines.append(f\"   Query Focus: {summary_stats['query_focus']}\")\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "# ============================================================================\n",
    "# 3.5. Response Formatter (NEW - for structured responses)\n",
    "# ============================================================================\n",
    "\n",
    "class ResponseFormatter:\n",
    "    \"\"\"Format RAG responses with consistent structure\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def format_search_response(query: str, answer: str, references: List[Dict], \n",
    "                               confidence: float, stats: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"Format standard search response\"\"\"\n",
    "        return {\n",
    "            'query': query,\n",
    "            'search_type': 'Enhanced Search',\n",
    "            'answer': answer,\n",
    "            'references': references,\n",
    "            'confidence': confidence,\n",
    "            'processing_stats': stats,\n",
    "            'collections_searched': stats.get('collections', [])\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def format_comparison_report(report: Dict) -> str:\n",
    "        \"\"\"Format cross-reference comparison report as readable text\"\"\"\n",
    "        lines = []\n",
    "        \n",
    "        # Title and overview\n",
    "        lines.append(f\"ðŸ“‹ {report.get('title', 'Comparison Report')}\")\n",
    "        lines.append(\"=\" * 80)\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        # Document overview\n",
    "        if report.get('overview'):\n",
    "            lines.append(\"ðŸ“„ DOCUMENT OVERVIEW:\")\n",
    "            lines.append(report['overview'])\n",
    "            lines.append(\"\")\n",
    "        \n",
    "        # Analysis summary\n",
    "        summary = report.get('analysis_summary', {})\n",
    "        if summary:\n",
    "            lines.append(\"ðŸ“Š ANALYSIS SUMMARY:\")\n",
    "            lines.append(f\"   â€¢ Sections analyzed: {summary.get('total_sections_analyzed', 0)}\")\n",
    "            lines.append(f\"   â€¢ Misalignments found: {summary.get('misalignments_found', 0)}\")\n",
    "            lines.append(f\"   â€¢ Confidence: {summary.get('confidence_score', 0):.1%} ({summary.get('confidence_level', 'Unknown')})\")\n",
    "            lines.append(\"\")\n",
    "        \n",
    "        # Results\n",
    "        results = report.get('results', [])\n",
    "        if results:\n",
    "            lines.append(\"ðŸ” FINDINGS:\")\n",
    "            lines.append(\"-\" * 80)\n",
    "            \n",
    "            for i, result in enumerate(results, 1):\n",
    "                section = result.get('section', f'Finding {i}')\n",
    "                lines.append(f\"\\n{i}. {section}\")\n",
    "                \n",
    "                if result.get('severity'):\n",
    "                    lines.append(f\"   Severity: {result['severity']}\")\n",
    "                \n",
    "                if result.get('explanation'):\n",
    "                    lines.append(f\"   {result['explanation']}\")\n",
    "                \n",
    "                if result.get('target_statement'):\n",
    "                    stmt = result['target_statement'][:300]\n",
    "                    lines.append(f\"   Target: {stmt}{'...' if len(result['target_statement']) > 300 else ''}\")\n",
    "                \n",
    "                # Reference citations\n",
    "                refs = result.get('reference_citations', [])\n",
    "                if refs:\n",
    "                    lines.append(f\"   References ({len(refs)}):\")\n",
    "                    for ref in refs[:3]:\n",
    "                        fn = ref.get('filename', 'Unknown')\n",
    "                        pg = ref.get('page_num', 'N/A')\n",
    "                        lines.append(f\"     â€¢ {fn} (Page {pg})\")\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 4. Enhanced Results Display (REFACTORED - widget management only)\n",
    "# ============================================================================\n",
    "\n",
    "class EnhancedResultsDisplay:\n",
    "    \"\"\"Widget manager for displaying RAG results\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results_area = widgets.Output(\n",
    "            layout=widgets.Layout(\n",
    "                width='100%',\n",
    "                border='1px solid #e2e8f0',\n",
    "                padding='15px',\n",
    "                border_radius='8px',\n",
    "                background_color='#f8fafc',\n",
    "                overflow='auto',\n",
    "                max_height='600px'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Show placeholder\n",
    "        with self.results_area:\n",
    "            print(\"ðŸ’¡ Ready to search! Enter a query and click Search to see results here.\")\n",
    "    \n",
    "    def display_results(self, result: Dict[str, Any]):\n",
    "        \"\"\"Display formatted results in widget\"\"\"\n",
    "        formatted_text = ResultsFormatter.format_result(result)\n",
    "        \n",
    "        with self.results_area:\n",
    "            clear_output(wait=False)\n",
    "            print(formatted_text)\n",
    "    \n",
    "    def get_widget(self):\n",
    "        \"\"\"Get results display widget\"\"\"\n",
    "        return self.results_area\n",
    "\n",
    "\n",
    "print(\"âœ… Additional RAG UI Components defined!\")\n",
    "print(\"   â€¢ AdvancedCollectionSelector: Multi-select with stats\")\n",
    "print(\"   â€¢ RAGProgressTracker: Progress tracking with cancellation\")\n",
    "print(\"   â€¢ ResultsFormatter: Reusable result formatting\")\n",
    "print(\"   â€¢ EnhancedResultsDisplay: Simplified widget manager\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e31bc1",
   "metadata": {
    "id": "98e31bc1"
   },
   "source": [
    "#### 6.4.1 Main Enhanced RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c1ec6cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LLM-Powered Query Router - No Fallbacks\n",
      "   â€¢ LLM required for all query analysis\n",
      "   â€¢ Explicit errors when LLM fails\n",
      "   â€¢ Metadata-based document retrieval with year extraction from filenames\n"
     ]
    }
   ],
   "source": [
    "# LLM-Powered Query Router \n",
    "class LLMQueryRouter:\n",
    "    \"\"\"Use LLM to understand query intent - fails explicitly if LLM unavailable\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_engine, collections_manager):\n",
    "        self.llm = llm_engine\n",
    "        self.manager = collections_manager\n",
    "    \n",
    "    def analyze_query(self, query: str, collections: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Use LLM to analyze query - raises exception if LLM fails\"\"\"\n",
    "        \n",
    "        # Check LLM availability\n",
    "        if not self.llm or not hasattr(self.llm, 'generate_response'):\n",
    "            raise RuntimeError(\"LLM engine not available or not properly initialized\")\n",
    "        \n",
    "        # Get available metadata context\n",
    "        metadata_context = self._get_metadata_context(collections)\n",
    "        \n",
    "        prompt = f\"\"\"Analyze this query and output JSON only.\n",
    "\n",
    "Available files: {metadata_context['sample_files'][:10]}\n",
    "Available years: {metadata_context['years']}\n",
    "Available companies: {metadata_context['companies'][:5]}\n",
    "\n",
    "Query: \"{query}\"\n",
    "\n",
    "Determine:\n",
    "1. Intent: \"database_query\" (asking about files/database) OR \"search\" (asking about content) OR \"summarization\" OR \"comparison\"\n",
    "2. If database_query: extract filters (filename, year, company, author)\n",
    "3. If summarization: extract target document and word count\n",
    "4. If comparison: extract target and reference documents\n",
    "\n",
    "JSON format:\n",
    "{{\n",
    "  \"intent\": \"database_query|search|summarization|comparison\",\n",
    "  \"confidence\": 0.0-1.0,\n",
    "  \"filters\": {{\"filename\": \"partial name or null\", \"year\": \"2020 or null\", \"company\": \"name or null\"}},\n",
    "  \"target_document\": \"filename or null\",\n",
    "  \"reference_document\": \"filename or null\",\n",
    "  \"word_count\": 300\n",
    "}}\n",
    "\n",
    "JSON:\"\"\"\n",
    "        \n",
    "        # Call LLM - let exceptions propagate\n",
    "        response = self.llm.generate_response(prompt, temperature=0.1, max_tokens=200)\n",
    "        result = self._parse_llm_response(response)\n",
    "        \n",
    "        # Apply metadata-based document retrieval if needed\n",
    "        if result['intent'] == 'database_query' and result.get('filters'):\n",
    "            result['matched_documents'] = self._retrieve_by_metadata(\n",
    "                result['filters'], collections\n",
    "            )\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _get_metadata_context(self, collections: List[str]) -> Dict:\n",
    "        \"\"\"Extract available metadata for LLM context\"\"\"\n",
    "        files, years, companies = set(), set(), set()\n",
    "        \n",
    "        for col_name in collections[:2]:  # Limit to avoid token overflow\n",
    "            collection = self.manager.get_collection(col_name)\n",
    "            if not collection:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                data = collection.get(include=['metadatas'], limit=50)\n",
    "                for m in data.get('metadatas', []):\n",
    "                    if m:\n",
    "                        if m.get('filename'):\n",
    "                            files.add(m['filename'])\n",
    "                        if m.get('year'):\n",
    "                            years.add(m['year'])\n",
    "                        if m.get('company'):\n",
    "                            companies.add(m['company'])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return {\n",
    "            'sample_files': sorted(files)[:10],\n",
    "            'years': sorted(years, reverse=True)[:5],\n",
    "            'companies': sorted(companies)[:5]\n",
    "        }\n",
    "    \n",
    "    def _parse_llm_response(self, response: str) -> Dict:\n",
    "        \"\"\"Parse JSON from LLM response - raises ValueError if invalid\"\"\"\n",
    "        import json\n",
    "        match = re.search(r'\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}', response, re.DOTALL)\n",
    "        if not match:\n",
    "            raise ValueError(f\"LLM did not return valid JSON. Response: {response[:200]}\")\n",
    "        \n",
    "        try:\n",
    "            data = json.loads(match.group(0))\n",
    "        except json.JSONDecodeError as e:\n",
    "            raise ValueError(f\"LLM returned invalid JSON: {e}\")\n",
    "        \n",
    "        return {\n",
    "            'intent': data.get('intent', 'search'),\n",
    "            'confidence': data.get('confidence', 0.5),\n",
    "            'filters': data.get('filters', {}),\n",
    "            'target_document': data.get('target_document'),\n",
    "            'reference_document': data.get('reference_document'),\n",
    "            'word_count': data.get('word_count', 300)\n",
    "        }\n",
    "    \n",
    "    def _retrieve_by_metadata(self, filters: Dict, collections: List[str]) -> List[Dict]:\n",
    "        \"\"\"Retrieve documents using metadata filters - improved year matching\"\"\"\n",
    "        matched = []\n",
    "        \n",
    "        for col_name in collections:\n",
    "            collection = self.manager.get_collection(col_name)\n",
    "            if not collection:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                data = collection.get(include=['metadatas', 'documents'])\n",
    "                for i, metadata in enumerate(data.get('metadatas', [])):\n",
    "                    if not metadata:\n",
    "                        continue\n",
    "                    \n",
    "                    filename = metadata.get('filename', '')\n",
    "                    \n",
    "                    # Apply filename filter\n",
    "                    if filters.get('filename'):\n",
    "                        fname = filters['filename'].lower()\n",
    "                        if fname not in filename.lower():\n",
    "                            continue\n",
    "                    \n",
    "                    # Apply year filter - check metadata AND filename\n",
    "                    if filters.get('year'):\n",
    "                        year_str = str(filters['year'])\n",
    "                        # Extract last 2 digits for short year (2022 -> 22)\n",
    "                        year_short = year_str[2:] if len(year_str) == 4 else year_str\n",
    "                        \n",
    "                        # Check metadata year OR filename patterns\n",
    "                        metadata_year = str(metadata.get('year', ''))\n",
    "                        has_year = (\n",
    "                            year_str in metadata_year or \n",
    "                            year_short in metadata_year or\n",
    "                            year_str in filename or\n",
    "                            f\"_{year_short}\" in filename or  # Match _22 in barclays_1Q22\n",
    "                            f\"{year_short}_\" in filename or  # Match 22_ pattern\n",
    "                            f\"Q{year_short}\" in filename     # Match Q22 pattern\n",
    "                        )\n",
    "                        \n",
    "                        if not has_year:\n",
    "                            continue\n",
    "                    \n",
    "                    # Apply company filter - check metadata AND filename\n",
    "                    if filters.get('company'):\n",
    "                        comp = filters['company'].lower()\n",
    "                        company_meta = metadata.get('company', '').lower()\n",
    "                        if comp not in company_meta and comp not in filename.lower():\n",
    "                            continue\n",
    "                    \n",
    "                    # Document matches all filters\n",
    "                    matched.append({\n",
    "                        'filename': filename,\n",
    "                        'collection': col_name,\n",
    "                        'year': metadata.get('year'),\n",
    "                        'company': metadata.get('company'),\n",
    "                        'text': data['documents'][i][:300] if i < len(data.get('documents', [])) else ''\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Filter error in {col_name}: {e}\")\n",
    "                pass\n",
    "        \n",
    "        return matched\n",
    "\n",
    "\n",
    "print(\"âœ… LLM-Powered Query Router - No Fallbacks\")\n",
    "print(\"   â€¢ LLM required for all query analysis\")\n",
    "print(\"   â€¢ Explicit errors when LLM fails\")\n",
    "print(\"   â€¢ Metadata-based document retrieval with year extraction from filenames\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15547824",
   "metadata": {},
   "source": [
    "#### 6.4.2 Main Enhanced RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "33e1868e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Query Router - LLM-Dependent (No Fallbacks)\n",
      "   â€¢ All queries processed by LLM\n",
      "   â€¢ Explicit errors when LLM unavailable\n",
      "   â€¢ Cross-reference detection still uses patterns (not LLM-dependent)\n"
     ]
    }
   ],
   "source": [
    "# Query Router - LLM-Dependent (No Pattern Matching Fallback)\n",
    "class QueryRouter:\n",
    "    \"\"\"Simple query router that delegates to LLM - no fallback logic\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_router, cross_reference_comparator):\n",
    "        self.llm_router = llm_router\n",
    "        self.comparator = cross_reference_comparator\n",
    "    \n",
    "    def detect_query_type(self, query: str, collections: List[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Detect query type using LLM - raises exception if LLM fails\"\"\"\n",
    "        \n",
    "        # Check cross-reference comparison first (uses pattern detection, not LLM)\n",
    "        comparison_intent = self.comparator.detect_query_intent(query)\n",
    "        if comparison_intent['is_comparison_query']:\n",
    "            return {\n",
    "                'type': 'cross_reference_comparison',\n",
    "                'confidence': 0.9,\n",
    "                **comparison_intent\n",
    "            }\n",
    "        \n",
    "        # Use LLM for all other query analysis - no fallback\n",
    "        if not collections:\n",
    "            raise ValueError(\"Collections parameter required for LLM query analysis\")\n",
    "        \n",
    "        # LLM analysis - exceptions will propagate\n",
    "        analysis = self.llm_router.analyze_query(query, collections)\n",
    "        return {'type': analysis['intent'], **analysis}\n",
    "\n",
    "\n",
    "# Search Results Processor\n",
    "class SearchResultsProcessor:\n",
    "    \"\"\"Process and enrich search results with statistics\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stats = {\n",
    "            'total_found': 0,\n",
    "            'high_confidence': 0,\n",
    "            'medium_confidence': 0,\n",
    "            'low_confidence': 0,\n",
    "            'both_methods': 0,\n",
    "            'semantic_only': 0,\n",
    "            'bm25_only': 0\n",
    "        }\n",
    "    \n",
    "    def process_results(self, results: List[Dict], min_confidence: float = 0.3) -> List[Dict]:\n",
    "        \"\"\"Filter and enrich results, track statistics\"\"\"\n",
    "        self.stats['total_found'] = len(results)\n",
    "        processed = []\n",
    "        \n",
    "        for result in results:\n",
    "            confidence = result.get('confidence', 0)\n",
    "            \n",
    "            # Track confidence levels\n",
    "            if confidence >= 0.7:\n",
    "                self.stats['high_confidence'] += 1\n",
    "            elif confidence >= 0.5:\n",
    "                self.stats['medium_confidence'] += 1\n",
    "            else:\n",
    "                self.stats['low_confidence'] += 1\n",
    "            \n",
    "            # Track search methods\n",
    "            has_semantic = result.get('semantic_score', 0) > 0\n",
    "            has_bm25 = result.get('bm25_score', 0) > 0\n",
    "            \n",
    "            if has_semantic and has_bm25:\n",
    "                self.stats['both_methods'] += 1\n",
    "            elif has_semantic:\n",
    "                self.stats['semantic_only'] += 1\n",
    "            elif has_bm25:\n",
    "                self.stats['bm25_only'] += 1\n",
    "            \n",
    "            # Apply confidence filter\n",
    "            if confidence >= min_confidence:\n",
    "                result['confidence_level'] = self._get_confidence_level(confidence)\n",
    "                processed.append(result)\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def _get_confidence_level(self, confidence: float) -> str:\n",
    "        if confidence >= 0.7:\n",
    "            return 'high'\n",
    "        elif confidence >= 0.5:\n",
    "            return 'medium'\n",
    "        return 'low'\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Return accumulated statistics\"\"\"\n",
    "        return {\n",
    "            **self.stats,\n",
    "            'total_processed': self.stats['high_confidence'] + \n",
    "                              self.stats['medium_confidence'] + \n",
    "                              self.stats['low_confidence'],\n",
    "            'confidence_distribution': {\n",
    "                'high': self.stats['high_confidence'],\n",
    "                'medium': self.stats['medium_confidence'],\n",
    "                'low': self.stats['low_confidence']\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"âœ… Query Router - LLM-Dependent (No Fallbacks)\")\n",
    "print(\"   â€¢ All queries processed by LLM\")\n",
    "print(\"   â€¢ Explicit errors when LLM unavailable\")\n",
    "print(\"   â€¢ Cross-reference detection still uses patterns (not LLM-dependent)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bb97ce",
   "metadata": {},
   "source": [
    "#### 6.4.3 Advanced Hybrid Search Engine with BM25 Index Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4d16ad50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… AdvancedHybridSearchEngine defined\n"
     ]
    }
   ],
   "source": [
    "# Advanced Hybrid Search Engine with BM25 Index Management\n",
    "class AdvancedHybridSearchEngine:\n",
    "    \"\"\"Enhanced hybrid search with automatic BM25 index loading\"\"\"\n",
    "    \n",
    "    def __init__(self, collections_manager):\n",
    "        self.manager = collections_manager\n",
    "        self.fusion_engine = RRFFusionEngine()\n",
    "        self.loaded_collections = set()\n",
    "    \n",
    "    def ensure_bm25_indices_loaded(self, collection_names: List[str]):\n",
    "        \"\"\"Ensure BM25 indices are loaded for specified collections\"\"\"\n",
    "        for col_name in collection_names:\n",
    "            if col_name not in self.loaded_collections:\n",
    "                # Try to load existing index\n",
    "                if not self.manager.load_bm25_index(col_name):\n",
    "                    # Build new index if load fails\n",
    "                    print(f\"Building BM25 index for {col_name}...\")\n",
    "                    self.manager.build_bm25_index(col_name)\n",
    "                self.loaded_collections.add(col_name)\n",
    "    \n",
    "    def multi_collection_search(self, query: str, collection_names: List[str], \n",
    "                               top_k: int = 10) -> List[Dict]:\n",
    "        \"\"\"Search across multiple collections with fusion\"\"\"\n",
    "        # Ensure indices loaded\n",
    "        self.ensure_bm25_indices_loaded(collection_names)\n",
    "        \n",
    "        all_results = []\n",
    "        expanded_k = int(top_k * 2.5)\n",
    "        \n",
    "        for col_name in collection_names:\n",
    "            # Get BM25 results\n",
    "            bm25_results = self.manager.search_bm25(query, col_name, expanded_k)\n",
    "            \n",
    "            # Get semantic results\n",
    "            semantic_results = self.manager.semantic_search(query, col_name, expanded_k)\n",
    "            \n",
    "            # Fuse results\n",
    "            if bm25_results or semantic_results:\n",
    "                fused = self.fusion_engine.fuse(semantic_results, bm25_results)\n",
    "                \n",
    "                # Tag with source collection\n",
    "                for result in fused:\n",
    "                    result['source_collection'] = col_name\n",
    "                \n",
    "                all_results.extend(fused)\n",
    "        \n",
    "        # Re-sort all results\n",
    "        all_results.sort(key=lambda x: x.get('final_score', 0), reverse=True)\n",
    "        \n",
    "        return all_results[:top_k * len(collection_names)]\n",
    "\n",
    "print(\"âœ… AdvancedHybridSearchEngine defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e345a02a",
   "metadata": {},
   "source": [
    "#### 6.4.4 Main Enhanced RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7ec027b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Enhanced RAG System initialized!\n",
      "   â€¢ Query routing with specialized handlers\n",
      "   â€¢ Hybrid search with reranking\n",
      "   â€¢ Reliability validation\n",
      "   â€¢ Cross-reference comparison\n",
      "   â€¢ Enhanced summarization\n",
      "   â€¢ Collections: 2\n"
     ]
    }
   ],
   "source": [
    "# Main Enhanced RAG System\n",
    "class SpecCompliantRAGSystem:\n",
    "    \"\"\"Simplified RAG orchestrator - delegates to specialized components\"\"\"\n",
    "    \n",
    "    def __init__(self, collections_manager, llm_engine, embedding_model, cross_encoder=None):\n",
    "        self.collections_manager = collections_manager\n",
    "        self.llm_engine = llm_engine\n",
    "        self.embedding_model = embedding_model\n",
    "        self.cross_encoder = cross_encoder\n",
    "        \n",
    "        # Initialize hybrid search\n",
    "        self.hybrid_search = self._init_hybrid_search()\n",
    "        \n",
    "        # Initialize LLM router for intelligent query processing\n",
    "        self.llm_router = LLMQueryRouter(llm_engine, collections_manager)\n",
    "        \n",
    "        # Initialize specialized components\n",
    "        self.citation_system = EnhancedCitationSystem(collections_manager)\n",
    "        self.cross_doc_analyzer = CrossDocumentAnalyzer(llm_engine, collections_manager)\n",
    "        self.metadata_search = EnhancedMetadataSearch(collections_manager, self.llm_router)\n",
    "        self.enhanced_summarizer = EnhancedSummarizer(llm_engine, self.citation_system)\n",
    "        self.reliability_enforcer = ReliabilityEnforcer(llm_engine)\n",
    "        self.cross_reference_comparator = CrossReferenceComparator(\n",
    "            llm_engine, collections_manager, embedding_model, self.hybrid_search\n",
    "        )\n",
    "        \n",
    "        # Initialize helper utilities\n",
    "        self.query_router = QueryRouter(self.llm_router, self.cross_reference_comparator)\n",
    "        self.results_processor = SearchResultsProcessor()\n",
    "        self.formatter = ResponseFormatter()\n",
    "        \n",
    "        self.enhanced_mode = True\n",
    "    \n",
    "    def _init_hybrid_search(self):\n",
    "        \"\"\"Initialize hybrid search engine\"\"\"\n",
    "        try:\n",
    "            return globals().get('enhanced_hybrid_search') or \\\n",
    "                   AdvancedHybridSearchEngine(self.collections_manager)\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def process_query(self, query: str, collections: List[str], \n",
    "                     use_reranking: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"Main query processing with intelligent routing\"\"\"\n",
    "        \n",
    "        if not collections:\n",
    "            return self._error_response(query, 'No collections selected', collections)\n",
    "        \n",
    "        try:\n",
    "            # Detect query type and route (pass collections for LLM context)\n",
    "            analysis = self.query_router.detect_query_type(query, collections)\n",
    "            \n",
    "            # Route to appropriate handler\n",
    "            if analysis['type'] == 'database_query':\n",
    "                result = self.metadata_search.execute_metadata_query(query, collections)\n",
    "            \n",
    "            elif analysis['type'] == 'cross_reference_comparison':\n",
    "                result = self._handle_comparison(query, collections, analysis)\n",
    "            \n",
    "            elif analysis['type'] == 'summarization':\n",
    "                result = self._handle_summarization(query, collections, analysis)\n",
    "            \n",
    "            else:\n",
    "                result = self._handle_search(query, collections, use_reranking)\n",
    "            \n",
    "            # Apply reliability validation (skip for metadata/summaries)\n",
    "            if result.get('search_type') not in [\n",
    "                'Metadata Count Query', 'File List Query', \n",
    "                'Enhanced Document Summarization', 'Cross-Reference Comparison'\n",
    "            ]:\n",
    "                validation = self.reliability_enforcer.validate_response_grounding(\n",
    "                    query, result.get('answer', ''), result.get('references', [])\n",
    "                )\n",
    "                result['reliability_validation'] = validation\n",
    "                \n",
    "                if not validation['is_grounded']:\n",
    "                    result['confidence'] *= 0.5\n",
    "                    result['answer'] = \"âš ï¸ RELIABILITY WARNING: Response may not be fully grounded.\\n\\n\" + \\\n",
    "                                      result['answer']\n",
    "            \n",
    "            # Add metadata\n",
    "            result['enhanced_mode'] = self.enhanced_mode\n",
    "            result['processing_timestamp'] = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        except Exception as e:\n",
    "            return self._error_response(query, str(e), collections)\n",
    "    \n",
    "    def _handle_search(self, query: str, collections: List[str], \n",
    "                      use_reranking: bool) -> Dict[str, Any]:\n",
    "        \"\"\"Handle standard hybrid search with reranking\"\"\"\n",
    "        \n",
    "        if not self.hybrid_search:\n",
    "            return self._error_response(query, 'Hybrid search not available', collections)\n",
    "        \n",
    "        # Ensure BM25 indices loaded\n",
    "        self.hybrid_search.ensure_bm25_indices_loaded(collections)\n",
    "        \n",
    "        # Execute search\n",
    "        search_results = self.hybrid_search.multi_collection_search(query, collections, 15)\n",
    "        \n",
    "        if not search_results:\n",
    "            return {\n",
    "                'query': query,\n",
    "                'search_type': 'Enhanced Search',\n",
    "                'answer': 'No relevant information found.',\n",
    "                'references': [],\n",
    "                'confidence': 0.0,\n",
    "                'collections_searched': collections\n",
    "            }\n",
    "        \n",
    "        # Process results with statistics\n",
    "        processed_results = self.results_processor.process_results(search_results)\n",
    "        \n",
    "        # Apply reranking if requested\n",
    "        if use_reranking and self.cross_encoder and len(processed_results) > 1:\n",
    "            try:\n",
    "                processed_results = self.cross_encoder.rerank(query, processed_results, \n",
    "                                                             len(processed_results))\n",
    "            except Exception as e:\n",
    "                print(f\"Reranking failed: {e}\")\n",
    "        \n",
    "        # Limit to top 10\n",
    "        final_results = processed_results[:10]\n",
    "        \n",
    "        # Generate citations and response\n",
    "        citations = self.citation_system.generate_precise_citations(final_results)\n",
    "        cross_analysis = self.cross_doc_analyzer.analyze_cross_document_patterns(\n",
    "            final_results, query\n",
    "        )\n",
    "        answer = self._generate_response(query, final_results, citations)\n",
    "        \n",
    "        # Build references\n",
    "        references = self._build_references(final_results, citations)\n",
    "        \n",
    "        # Calculate confidence\n",
    "        confidence = np.mean([r.get('confidence', 0) for r in final_results[:5]]) \\\n",
    "                     if final_results else 0.0\n",
    "        \n",
    "        return self.formatter.format_search_response(\n",
    "            query, answer, references, confidence, \n",
    "            self.results_processor.get_stats()\n",
    "        )\n",
    "    \n",
    "    def _handle_summarization(self, query: str, collections: List[str], \n",
    "                             analysis: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"Handle document summarization\"\"\"\n",
    "        \n",
    "        filename = analysis.get('filename')\n",
    "        word_count = analysis.get('word_count', 300)\n",
    "        \n",
    "        if filename:\n",
    "            # Specific document summary\n",
    "            doc_finder = DocumentFinder(self.collections_manager)\n",
    "            documents = doc_finder.find_documents_by_name(filename, collections)\n",
    "            \n",
    "            if not documents:\n",
    "                return {\n",
    "                    'query': query,\n",
    "                    'search_type': 'Document Summarization',\n",
    "                    'answer': f'Document \"{filename}\" not found.',\n",
    "                    'references': [],\n",
    "                    'confidence': 0.0,\n",
    "                    'collections_searched': collections\n",
    "                }\n",
    "            \n",
    "            search_results = documents\n",
    "        else:\n",
    "            # Topic summary\n",
    "            if not self.hybrid_search:\n",
    "                return self._error_response(query, 'Hybrid search not available', collections)\n",
    "            \n",
    "            topic_query = re.sub(r'\\bsummariz\\w*\\b|\\b\\d+\\s*words?\\b', '', query, \n",
    "                                flags=re.IGNORECASE).strip() or \"main topics\"\n",
    "            search_results = self.hybrid_search.multi_collection_search(topic_query, collections, 12)\n",
    "        \n",
    "        if not search_results:\n",
    "            return {\n",
    "                'query': query,\n",
    "                'search_type': 'Summarization',\n",
    "                'answer': 'No content found to summarize.',\n",
    "                'references': [],\n",
    "                'confidence': 0.0,\n",
    "                'collections_searched': collections\n",
    "            }\n",
    "        \n",
    "        # Generate summary\n",
    "        summary_result = self.enhanced_summarizer.generate_summary(\n",
    "            search_results, query, word_count\n",
    "        )\n",
    "        \n",
    "        # Build references (single document with page range)\n",
    "        references = self._build_summary_reference(search_results, summary_result, collections)\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'search_type': 'Enhanced Document Summarization',\n",
    "            'answer': summary_result['summary'],\n",
    "            'references': references,\n",
    "            'confidence': summary_result['confidence'],\n",
    "            'collections_searched': collections,\n",
    "            'summary_stats': self._build_summary_stats(summary_result, references)\n",
    "        }\n",
    "    \n",
    "    def _handle_comparison(self, query: str, collections: List[str], \n",
    "                          analysis: Dict) -> Dict[str, Any]:\n",
    "        \"\"\"Handle cross-reference comparison\"\"\"\n",
    "        \n",
    "        target_doc = analysis.get('target_document')\n",
    "        reference_collection = analysis.get('reference_collection')\n",
    "        \n",
    "        if not target_doc or not reference_collection:\n",
    "            return {\n",
    "                'query': query,\n",
    "                'search_type': 'Cross-Reference Comparison',\n",
    "                'answer': 'Please specify target document and reference collection.',\n",
    "                'references': [],\n",
    "                'confidence': 0.0,\n",
    "                'collections_searched': collections,\n",
    "                'error': 'Missing target or reference'\n",
    "            }\n",
    "        \n",
    "        # Perform comparison\n",
    "        comparison_report = self.cross_reference_comparator.compare_documents(\n",
    "            target_doc=target_doc,\n",
    "            reference_collection=reference_collection,\n",
    "            target_collection=collections[0] if collections else None\n",
    "        )\n",
    "        \n",
    "        if comparison_report.get('status') == 'error':\n",
    "            return {\n",
    "                'query': query,\n",
    "                'search_type': 'Cross-Reference Comparison',\n",
    "                'answer': f\"Error: {comparison_report.get('message')}\",\n",
    "                'references': [],\n",
    "                'confidence': 0.0,\n",
    "                'collections_searched': collections\n",
    "            }\n",
    "        \n",
    "        # Format report\n",
    "        answer = self.formatter.format_comparison_report(comparison_report)\n",
    "        \n",
    "        # Extract references\n",
    "        references = []\n",
    "        for result in comparison_report.get('results', []):\n",
    "            for ref_cite in result.get('reference_citations', []):\n",
    "                references.append({\n",
    "                    'filename': ref_cite.get('filename', 'Unknown'),\n",
    "                    'page_num': ref_cite.get('page', 'N/A'),\n",
    "                    'collection': ref_cite.get('collection', reference_collection),\n",
    "                    'text': ref_cite.get('excerpt', '')\n",
    "                })\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'search_type': 'Cross-Reference Comparison',\n",
    "            'answer': answer,\n",
    "            'references': references,\n",
    "            'confidence': comparison_report.get('analysis_summary', {}).get('confidence_score', 0.0),\n",
    "            'collections_searched': collections,\n",
    "            'comparison_report': comparison_report\n",
    "        }\n",
    "    \n",
    "    def _generate_response(self, query: str, results: List[Dict], \n",
    "                          citations: List[Dict]) -> str:\n",
    "        \"\"\"Generate LLM response with citations\"\"\"\n",
    "        \n",
    "        context_parts = []\n",
    "        for i, result in enumerate(results[:5]):\n",
    "            if not result.get('text'):\n",
    "                continue\n",
    "            \n",
    "            citation = citations[i] if i < len(citations) else {'id': f'cite_{i+1}'}\n",
    "            filename = citation.get('filename', 'Unknown')\n",
    "            context_parts.append(f\"[{citation['id']} - {filename}]: {result['text'][:600]}\")\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)[:4000]\n",
    "        \n",
    "        prompt = f\"\"\"Answer using ONLY the provided context with proper citations.\n",
    "\n",
    "CONTEXT: {context}\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "REQUIREMENTS:\n",
    "- Cite every fact: (cite_X)\n",
    "- Professional paragraphs\n",
    "- Use only context information\n",
    "- If insufficient, state: \"Limited information in sources\"\n",
    "\n",
    "YOUR ANSWER:\"\"\"\n",
    "        \n",
    "        return self.llm_engine.generate_response(prompt, temperature=0.0)\n",
    "    \n",
    "    def _build_references(self, results: List[Dict], citations: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Build reference list from results and citations\"\"\"\n",
    "        references = []\n",
    "        \n",
    "        for i, result in enumerate(results):\n",
    "            if not result.get('text'):\n",
    "                continue\n",
    "            \n",
    "            metadata = result.get('metadata', {})\n",
    "            citation = citations[i] if i < len(citations) else {}\n",
    "            \n",
    "            references.append({\n",
    "                'filename': metadata.get('filename', 'Unknown'),\n",
    "                'collection': result.get('source_collection', metadata.get('collection', 'Unknown')),\n",
    "                'chunk_id': metadata.get('chunk_id'),\n",
    "                'text_snippet': result['text'][:200] + '...' if len(result['text']) > 200 else result['text'],\n",
    "                'relevance_score': result.get('final_score', 0),\n",
    "                'confidence': result.get('confidence', 0),\n",
    "                'citation_id': citation.get('id', f'cite_{i+1}')\n",
    "            })\n",
    "        \n",
    "        return references\n",
    "    \n",
    "    def _build_summary_reference(self, results: List[Dict], summary_result: Dict, \n",
    "                                 collections: List[str]) -> List[Dict]:\n",
    "        \"\"\"Build single document reference with page range for summaries\"\"\"\n",
    "        if not results:\n",
    "            return []\n",
    "        \n",
    "        filename = results[0].get('metadata', {}).get('filename', 'Unknown')\n",
    "        collection = results[0].get('collection', collections[0])\n",
    "        \n",
    "        # Collect unique pages\n",
    "        pages = []\n",
    "        for citation in summary_result.get('citations_used', []):\n",
    "            page = citation.get('page')\n",
    "            if page and page != 'N/A' and page not in pages:\n",
    "                pages.append(page)\n",
    "        \n",
    "        # Sort pages\n",
    "        try:\n",
    "            pages.sort(key=int)\n",
    "        except:\n",
    "            pages.sort()\n",
    "        \n",
    "        # Create page range string\n",
    "        if not pages:\n",
    "            page_range = \"Pages not available\"\n",
    "        elif len(pages) == 1:\n",
    "            page_range = f\"Page {pages[0]}\"\n",
    "        else:\n",
    "            page_range = f\"Pages {pages[0]}-{pages[-1]} ({len(pages)} pages)\"\n",
    "        \n",
    "        return [{\n",
    "            'filename': filename,\n",
    "            'collection': collection,\n",
    "            'page_range': page_range,\n",
    "            'pages': pages,\n",
    "            'citation_id': 'doc_1',\n",
    "            'confidence_score': summary_result['confidence']\n",
    "        }]\n",
    "    \n",
    "    def _build_summary_stats(self, summary_result: Dict, references: List[Dict]) -> Dict:\n",
    "        \"\"\"Build summary statistics\"\"\"\n",
    "        pages = references[0]['pages'] if references else []\n",
    "        \n",
    "        return {\n",
    "            'target_words': summary_result['target_words'],\n",
    "            'actual_words': summary_result['word_count'],\n",
    "            'sources_used': summary_result.get('sources_count', 0),\n",
    "            'unique_pages': len(pages),\n",
    "            'query_focus': summary_result.get('query_focus', 'general')\n",
    "        }\n",
    "    \n",
    "    def _error_response(self, query: str, error: str, collections: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Create standardized error response\"\"\"\n",
    "        return {\n",
    "            'query': query,\n",
    "            'search_type': 'Error',\n",
    "            'answer': f'Error: {error}',\n",
    "            'references': [],\n",
    "            'confidence': 0.0,\n",
    "            'collections_searched': collections,\n",
    "            'enhanced_mode': self.enhanced_mode,\n",
    "            'processing_timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }\n",
    "\n",
    "\n",
    "# Initialize system\n",
    "try:\n",
    "    required_components = ['collections_manager', 'llm_engine', 'embedding_model']\n",
    "    missing_components = [comp for comp in required_components if comp not in globals()]\n",
    "    \n",
    "    if missing_components:\n",
    "        print(f\"âŒ Missing: {missing_components}\")\n",
    "    else:\n",
    "        spec_rag = SpecCompliantRAGSystem(\n",
    "            collections_manager=collections_manager,\n",
    "            llm_engine=llm_engine,\n",
    "            embedding_model=embedding_model,\n",
    "            cross_encoder=cross_encoder if 'cross_encoder' in globals() else None\n",
    "        )\n",
    "        \n",
    "        print(\"âœ… Enhanced RAG System initialized!\")\n",
    "        print(\"   â€¢ Query routing with specialized handlers\")\n",
    "        print(\"   â€¢ Hybrid search with reranking\")\n",
    "        print(\"   â€¢ Reliability validation\")\n",
    "        print(\"   â€¢ Cross-reference comparison\")\n",
    "        print(\"   â€¢ Enhanced summarization\")\n",
    "        print(f\"   â€¢ Collections: {len(collections_manager.get_available_collections())}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Initialization failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11accab0",
   "metadata": {
    "id": "11accab0"
   },
   "source": [
    "## 7. Launch the complete RAG system interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88c986f",
   "metadata": {
    "id": "f88c986f"
   },
   "source": [
    "#### 7.1 Complete Functional RAG System UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5cd4e408",
   "metadata": {
    "id": "5cd4e408",
    "outputId": "6dfcf88d-2ef2-48d4-e03f-4b182f85948f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Complete RAG System UI initialized!\n"
     ]
    }
   ],
   "source": [
    "# Section 7.1: Complete Functional RAG System UI\n",
    "\n",
    "class ComprehensiveRAGInterface:\n",
    "    \"\"\"Complete RAG system interface with all capabilities\"\"\"\n",
    "\n",
    "    def __init__(self, spec_rag_system):\n",
    "        self.rag_system = spec_rag_system\n",
    "        self.formatter = ResultsFormatter()  # Use existing formatter\n",
    "        self.setup_ui_components()\n",
    "        self.animation_active = False\n",
    "\n",
    "    def setup_ui_components(self):\n",
    "        \"\"\"Initialize all UI components\"\"\"\n",
    "        \n",
    "        # 1. Collection Selector\n",
    "        collections = self.rag_system.collections_manager.get_available_collections()\n",
    "        self.collection_checkboxes = []\n",
    "        self.collection_map = {}\n",
    "\n",
    "        for col_name in collections:\n",
    "            cb = widgets.Checkbox(\n",
    "                value=True,\n",
    "                description=col_name,\n",
    "                layout=widgets.Layout(width='auto'),\n",
    "                style={'description_width': 'initial'}\n",
    "            )\n",
    "            self.collection_checkboxes.append(cb)\n",
    "            self.collection_map[cb] = col_name\n",
    "\n",
    "        self.compact_selector = widgets.HBox(\n",
    "            self.collection_checkboxes,\n",
    "            layout=widgets.Layout(justify_content='center', margin='10px 0')\n",
    "        )\n",
    "\n",
    "        # 2. Query Input\n",
    "        self.query_input = widgets.Textarea(\n",
    "            value='',\n",
    "            placeholder='Enter your query here...',\n",
    "            layout=widgets.Layout(width='95%', height='60px', margin='10px auto')\n",
    "        )\n",
    "\n",
    "        # 3. Answer Output (using HTML widget for formatted display)\n",
    "        self.answer_output = widgets.HTML(\n",
    "            value=self._get_placeholder_html(),\n",
    "            layout=widgets.Layout(width='95%', margin='6px auto')\n",
    "        )\n",
    "\n",
    "        # 4. Search Controls\n",
    "        self.search_button = widgets.Button(\n",
    "            description='â³ Search',\n",
    "            button_style='primary',\n",
    "            layout=widgets.Layout(width='100px', height='30px')\n",
    "        )\n",
    "\n",
    "        self.progress_bar = widgets.FloatProgress(\n",
    "            value=0, min=0, max=100,\n",
    "            layout=widgets.Layout(width='400px', height='25px')\n",
    "        )\n",
    "\n",
    "        self.status_label = widgets.HTML(value=\"Ready\")\n",
    "\n",
    "        self.cancel_button = widgets.Button(\n",
    "            description='Cancel',\n",
    "            button_style='danger',\n",
    "            layout=widgets.Layout(width='80px', height='30px'),\n",
    "            disabled=True\n",
    "        )\n",
    "\n",
    "        # 5. Advanced Options\n",
    "        self.rerank_checkbox = widgets.Checkbox(\n",
    "            value=True,\n",
    "            description='Use Cross-Encoder Reranking',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        self.expand_checkbox = widgets.Checkbox(\n",
    "            value=True,\n",
    "            description='Use Query Expansion',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "\n",
    "        self.advanced_options = widgets.Accordion([\n",
    "            widgets.VBox([self.rerank_checkbox, self.expand_checkbox])\n",
    "        ])\n",
    "        self.advanced_options.set_title(0, 'âš™ï¸ Advanced Options')\n",
    "        self.advanced_options.selected_index = None\n",
    "\n",
    "        # 6. Instructions\n",
    "        self.instructions = widgets.HTML(value=\"\"\"\n",
    "        <div style='background-color: #f8fafc; padding: 10px; border-radius: 6px; font-size: 12px; margin-top: 10px;'>\n",
    "            <b>ðŸ’¡ Tips:</b><br/>\n",
    "            â€¢ <b>Summarize:</b> \"Summarize barclays_1Q20_analyst_meeting_transcript.pdf in 300 words\"<br/>\n",
    "            â€¢ <b>Database:</b> \"List all files in each collection for year 2022\"<br/>\n",
    "            â€¢ <b>Cross-Reference:</b> \"Compare barclays_1Q20_analyst_meeting_transcript.pdf against PRA rules\"\n",
    "        </div>\n",
    "        \"\"\")\n",
    "\n",
    "        # Setup event handlers\n",
    "        self.search_button.on_click(self._on_search_clicked)\n",
    "        self.cancel_button.on_click(self._on_cancel_clicked)\n",
    "        self.cancel_flag = threading.Event()\n",
    "\n",
    "    def _get_placeholder_html(self) -> str:\n",
    "        \"\"\"Get placeholder HTML for answer output\"\"\"\n",
    "        return \"\"\"\n",
    "        <div style='max-height:800px; overflow:auto; font-family: monospace; white-space: pre-wrap;\n",
    "                    color: #334155; padding: 10px; border: 1px solid #e2e8f0; border-radius: 6px;'>\n",
    "        ðŸ’¡ Results will appear here after search\n",
    "        </div>\n",
    "        \"\"\"\n",
    "\n",
    "    def _on_cancel_clicked(self, button):\n",
    "        \"\"\"Handle cancel button click\"\"\"\n",
    "        self.cancel_flag.set()\n",
    "        self.animation_active = False\n",
    "        self.cancel_button.disabled = True\n",
    "        self.search_button.disabled = False\n",
    "        self.search_button.description = 'â³ Search'\n",
    "        self.status_label.value = \"<span style='color: #cc6600;'>âš ï¸ Search cancelled</span>\"\n",
    "\n",
    "    def _on_search_clicked(self, button):\n",
    "        \"\"\"Handle search button click\"\"\"\n",
    "        query = self.query_input.value.strip()\n",
    "        if not query:\n",
    "            self.status_label.value = \"<span style='color: #cc0000;'>âš ï¸ Enter a query</span>\"\n",
    "            return\n",
    "\n",
    "        # Get selected collections\n",
    "        collections = [col for cb, col in self.collection_map.items() if cb.value]\n",
    "\n",
    "        if not collections:\n",
    "            self.status_label.value = \"<span style='color: #cc0000;'>âš ï¸ Select at least one collection</span>\"\n",
    "            return\n",
    "\n",
    "        # Reset UI state\n",
    "        self._reset_ui_for_search()\n",
    "\n",
    "        # Start search in background thread\n",
    "        search_thread = threading.Thread(\n",
    "            target=self._execute_search,\n",
    "            args=(query, collections, self.rerank_checkbox.value)\n",
    "        )\n",
    "        search_thread.daemon = True\n",
    "        search_thread.start()\n",
    "\n",
    "        # Start button animation\n",
    "        animation_thread = threading.Thread(target=self._animate_button)\n",
    "        animation_thread.daemon = True\n",
    "        animation_thread.start()\n",
    "\n",
    "    def _reset_ui_for_search(self):\n",
    "        \"\"\"Reset UI state before starting search\"\"\"\n",
    "        self.answer_output.value = \"<div style='white-space: pre-wrap; font-family: monospace;'>Running search...</div>\"\n",
    "        self.search_button.disabled = True\n",
    "        self.animation_active = True\n",
    "        self.cancel_button.disabled = False\n",
    "        self.progress_bar.value = 0\n",
    "        self.status_label.value = \"<span style='color: #0066cc;'>Starting search...</span>\"\n",
    "        self.cancel_flag.clear()\n",
    "\n",
    "    def _animate_button(self):\n",
    "        \"\"\"Animate search button with spinning hourglass\"\"\"\n",
    "        import time\n",
    "        spinners = ['â³', 'âŒ›']\n",
    "        idx = 0\n",
    "        while self.animation_active:\n",
    "            self.search_button.description = f'{spinners[idx]} Searching...'\n",
    "            idx = (idx + 1) % len(spinners)\n",
    "            time.sleep(0.5)\n",
    "\n",
    "    def _execute_search(self, query: str, collections: List[str], use_reranking: bool):\n",
    "        \"\"\"Execute search in background thread\"\"\"\n",
    "        try:\n",
    "            self._update_progress(20, \"Initializing search...\")\n",
    "            \n",
    "            if self.cancel_flag.is_set():\n",
    "                return\n",
    "\n",
    "            self._update_progress(40, \"Searching collections...\")\n",
    "            \n",
    "            # Execute query with reranking parameter\n",
    "            result = self.rag_system.process_query(query, collections, use_reranking)\n",
    "\n",
    "            if self.cancel_flag.is_set():\n",
    "                return\n",
    "\n",
    "            self._update_progress(80, \"Processing results...\")\n",
    "\n",
    "            # Format and display results\n",
    "            self._display_results(result)\n",
    "\n",
    "            self._update_progress(100, \"âœ… Search completed successfully\", success=True)\n",
    "            self.cancel_button.disabled = True\n",
    "\n",
    "        except Exception as e:\n",
    "            self._handle_search_error(e)\n",
    "        \n",
    "        finally:\n",
    "            self._finalize_search()\n",
    "\n",
    "    def _update_progress(self, value: int, message: str, success: bool = False):\n",
    "        \"\"\"Update progress bar and status\"\"\"\n",
    "        self.progress_bar.value = value\n",
    "        color = '#008800' if success else '#0066cc'\n",
    "        self.status_label.value = f\"<span style='color: {color};'>{message}</span>\"\n",
    "\n",
    "    def _display_results(self, result: Dict[str, Any]):\n",
    "        \"\"\"Display formatted results using ResultsFormatter\"\"\"\n",
    "        formatted_text = self.formatter.format_result(result)\n",
    "        \n",
    "        # Escape HTML and wrap in styled div\n",
    "        from html import escape\n",
    "        escaped_text = escape(formatted_text)\n",
    "        \n",
    "        html = f\"\"\"\n",
    "        <div style='max-height:800px; overflow:auto; font-family: monospace; white-space: pre-wrap;\n",
    "                    color: #334155; padding: 10px; border: 1px solid #e2e8f0; border-radius: 6px;'>\n",
    "        {escaped_text}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        \n",
    "        self.answer_output.value = html\n",
    "\n",
    "    def _handle_search_error(self, error: Exception):\n",
    "        \"\"\"Handle search errors\"\"\"\n",
    "        error_msg = f\"Search failed: {str(error)}\"\n",
    "        self.progress_bar.value = 0\n",
    "        self.status_label.value = f\"<span style='color: #cc0000;'>âŒ {error_msg}</span>\"\n",
    "        self.cancel_button.disabled = True\n",
    "        \n",
    "        from html import escape\n",
    "        self.answer_output.value = f\"<div style='color:#cc0000;'>{escape(error_msg)}</div>\"\n",
    "\n",
    "    def _finalize_search(self):\n",
    "        \"\"\"Finalize search state\"\"\"\n",
    "        self.animation_active = False\n",
    "        self.search_button.disabled = False\n",
    "        self.search_button.description = 'â³ Search'\n",
    "\n",
    "    def display(self):\n",
    "        \"\"\"Build and display complete interface\"\"\"\n",
    "        header = widgets.HTML(\"\"\"\n",
    "        <div style='background: linear-gradient(135deg, #002147 0%, #003d82 100%);\n",
    "                    color: white; padding: 10px; border-radius: 8px; margin-bottom: 20px; text-align: center;'>\n",
    "            <h2 style='margin: 0; font-size: 24px;'>RAG System Interface</h2>\n",
    "        </div>\n",
    "        \"\"\")\n",
    "\n",
    "        search_controls = widgets.HBox([\n",
    "            self.search_button,\n",
    "            widgets.HTML(\"<div style='width: 20px;'></div>\"),\n",
    "            self.progress_bar,\n",
    "            widgets.HTML(\"<div style='width: 10px;'></div>\"),\n",
    "            self.cancel_button,\n",
    "        ], layout=widgets.Layout(justify_content='center', margin='10px 0'))\n",
    "\n",
    "        main_content = widgets.VBox([\n",
    "            self.compact_selector,\n",
    "            widgets.HTML(\"<div style='height: 10px;'></div>\"),\n",
    "            search_controls,\n",
    "            widgets.HBox([self.status_label], layout=widgets.Layout(justify_content='center')),\n",
    "            widgets.HTML(\"<div style='height: 10px;'></div>\"),\n",
    "            self.query_input,\n",
    "            self.answer_output,\n",
    "            widgets.HTML(\"<div style='height: 10px;'></div>\"),\n",
    "            self.advanced_options,\n",
    "            self.instructions\n",
    "        ], layout=widgets.Layout(align_items='center'))\n",
    "\n",
    "        complete_interface = widgets.VBox([\n",
    "            header,\n",
    "            main_content\n",
    "        ], layout=widgets.Layout(\n",
    "            width='95%',\n",
    "            max_width='1000px',\n",
    "            margin='0 auto',\n",
    "            padding='20px',\n",
    "            background_color='white',\n",
    "            border_radius='12px',\n",
    "            box_shadow='0 2px 8px rgba(0,0,0,0.1)'\n",
    "        ))\n",
    "\n",
    "        return complete_interface\n",
    "\n",
    "\n",
    "print(\"âœ… Complete RAG System UI initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37af2730",
   "metadata": {
    "id": "37af2730"
   },
   "source": [
    "#### 7.2 Launch the complete RAG system interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "af83364b",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "d2698e283d6446f184e82f9ce21fd0f9"
     ]
    },
    "id": "af83364b",
    "outputId": "71c2d729-9340-4a59-dda9-a3bde19d970a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b841d97bdce54dd38149ac802ed33f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value=\"\\n        <div style='background: linear-gradient(135deg, #002147 0%, #003d82 100%)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize and Launch Complete RAG Interface\n",
    "if 'spec_rag' not in globals():\n",
    "    print(\"spec_rag system not available. Please run Section 7 first.\")\n",
    "else:\n",
    "    # Initialize the comprehensive interface\n",
    "    comprehensive_rag_interface = ComprehensiveRAGInterface(spec_rag)\n",
    "\n",
    "    # Display the interface ONLY (no print statements after)\n",
    "    display(comprehensive_rag_interface.display())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "87bfda1e",
    "7b6923b2",
    "fc203c75",
    "93709a71",
    "bx9quo1jRCEp",
    "8b79a21f",
    "KylTjGTsRS0x",
    "c95802e6",
    "QcINqapsUUGb",
    "38a2f5b0",
    "Z9kZNm1OUmyd",
    "79343c4b",
    "d2f45e35",
    "9ffef275",
    "24a65aed",
    "f85b1a92",
    "46f3203f",
    "f177654c",
    "78411126",
    "b858469e",
    "b36978eb",
    "8d98109c",
    "9568dad1",
    "2b719cd7",
    "3956ee7a",
    "cc77a18b",
    "36579326",
    "8fa82f95",
    "405db11b",
    "79fb4f24",
    "392dddfc",
    "46cc5056",
    "dcaa15d8",
    "n2dflreae-ZT",
    "4f212af5",
    "3fe33bd5",
    "db5dc895",
    "86084ce4",
    "7a4aada4",
    "c1e9aad5",
    "136932e4",
    "9d433344",
    "e07eec5c",
    "ae3d811c",
    "5CX9gRzGgyyd",
    "qDEGp3UTGoq8"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rag-transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0025cf09b3e74298acf17160ed680e38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7aa4d76a95ad45b798bf831bbf1a59fa",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9c9293eca0e042458da526ea365549d4",
      "value": "â€‡2991/2991â€‡[00:35&lt;00:00,â€‡88.23it/s]"
     }
    },
    "011125111788426885b873cafbd87a78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "02248ae2fb414bffba462faec4e10688": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_605f28c35ef4440cae089adae8cce346",
       "IPY_MODEL_99745fb34d784204bf8a6584dbd58d11",
       "IPY_MODEL_98e05433f6d644d68319f9ee6ad0365e"
      ],
      "layout": "IPY_MODEL_9e871ff59d3d4839ae34944515127c8f"
     }
    },
    "02f5083e008841188b41e5d385f54c7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "055352abc36643b8bf092d0ee9262e53": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3c20a3fb99a3438692cb5107774b0ce3",
       "IPY_MODEL_677b520b744c4187807e9479d6ad4eaa",
       "IPY_MODEL_b87f211d6b714915a3544d78972584b2"
      ],
      "layout": "IPY_MODEL_0df775e4bcff4c6cacf7de8343acc494"
     }
    },
    "0640cd714c0f4eb1a22726bf46b5dd18": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "067359b8e9164cb4a279a44a8bb023b1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "06739ce3b5a54ab38fcdf0c596b9aa82": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "07a40955d5ae4b7a96419adfb80ad2a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a33ed0a08b0c4b3f9eb5a0e9fcf6991d",
       "IPY_MODEL_98010a8969af428b91ec62cf5c92c6c7",
       "IPY_MODEL_4f1116b208c14ad2b58f926b7f2ee91e"
      ],
      "layout": "IPY_MODEL_9ce72a461cd848f7a65c8396466f5c3b"
     }
    },
    "09702809260543fc8d2d098ba2eff48b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0a380cc99b1a445d9360116d41fb92c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0bf2fb6b6f084cec912a4c6c4168b223": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7c9acbfc6d2b47a0918f7a4b3346c3e0",
      "max": 125,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1d1a81d1e3eb4e53b70636edfee84e84",
      "value": 125
     }
    },
    "0d747272701a4dbea08cb3a315b3ed2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0df775e4bcff4c6cacf7de8343acc494": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "100e97e7f6d84d0683a4c705e4fe2eff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af012e52c3ad4f539164a67364831a55",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_84e01f7ee39844b796f03a11b9be1407",
      "value": "Sentimentâ€‡analysis:â€‡100%"
     }
    },
    "141234bcec25497bb1c019566f17960f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "143789d7c3d34a479c14cc3536578649": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "14858ecbb38946758db1f1fd5d3fe2e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "16a23106d15f412995eb9ba0329c1886": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1763329f29de4e42b83c9517fe9499d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1beafb96d3d94a868938b93b92e8dbd3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d1a81d1e3eb4e53b70636edfee84e84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1dfab6eccd434df1b2fb51a03d1c6209": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "1e5e4a4748ce45a18da266533d9c85e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_31456934173048929144dc3f620f7edd",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c07c4842d1134e74bea6598bbad88766",
      "value": "README.md:â€‡"
     }
    },
    "1f8306c9f7e04297b3e752213242e32c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20c9e354331e4f98bd73bb9bfdc1a138": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21c38215f12849bfb1e25f78bfdcb140": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "23c4ae2ca8f5472a95550619a144687e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "24c7270a584b41be8820167b95a95772": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25ebfb9caa314661b0ab5141d017758c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d241d8f02dd245edbefaccc0499cc64d",
      "max": 349,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_23c4ae2ca8f5472a95550619a144687e",
      "value": 349
     }
    },
    "2616ee7a88484474b70321af416a0176": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "273a066dd98b49658189b8e03399444b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DropdownModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DropdownModel",
      "_options_labels": [
       "earnings"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "DropdownView",
      "description": "Select:",
      "description_tooltip": null,
      "disabled": true,
      "index": 0,
      "layout": "IPY_MODEL_282e9850d2b0492cbae3b7c73c01fef2",
      "style": "IPY_MODEL_f0201465ea594ee2a7562463453c773d"
     }
    },
    "27fc018c9245466ebc03ee5f94bbb540": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3089b5d9bff6476f8632837f58bedf95",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_14858ecbb38946758db1f1fd5d3fe2e6",
      "value": "config.json:â€‡100%"
     }
    },
    "282e9850d2b0492cbae3b7c73c01fef2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29294a8b8e5b4b4a9eff16e89beee7bd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2a0d6f5245814cd39b5790b97e61deb8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ccbeba7ed9ec4194a5a277cf4754d32b",
       "IPY_MODEL_7a8a48a861774310b3f4be6f6375eae6",
       "IPY_MODEL_45275f1f6a62484ba93727f83deecba2"
      ],
      "layout": "IPY_MODEL_53aec41823db4ae99f038b95f884f5f6"
     }
    },
    "2a10cc5788f845f1a05ff9c277e3d318": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ddd12074a2048d2979f5fd980594c63": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f222110d2fa4738810ad83b97b4b7f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "308459b8458b4affb27a58bd8b7c2e66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b99e78a40ea444eab58749ed67c2cc4",
      "max": 439101405,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_585c8e51b9a64aa9ba9f43a59744b8ff",
      "value": 439101405
     }
    },
    "3089b5d9bff6476f8632837f58bedf95": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "311b0478d5a9425bbd28e2d82cc96a3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9df0b1218b4435c89f36069e73f7262",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f4ea57dee719490290fe2b945d291a23",
      "value": "Chunkingâ€‡files:â€‡100%"
     }
    },
    "31456934173048929144dc3f620f7edd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "31f95e14f4a248729c23f5631b1853bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "325f8734571442049c1a0f2d1442a0ac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32d7a49e2aa24ac58ee4c3eff272e3e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "333cab60c2da494694b92af7d03ee31c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "34613f7903194d29b9544094280ad8e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "35bbb9f8bae24e99bbbf08dfbe766b75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_067359b8e9164cb4a279a44a8bb023b1",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_143789d7c3d34a479c14cc3536578649",
      "value": "model.safetensors:â€‡100%"
     }
    },
    "37baf24762364d0290cd6b647f7e90b8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "38b5bef79d3a45fbb4d2783406c625b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "398a0b724d6c40cdb18dbda2554472d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "39a63fff1694413b8ed1449d9e8d88df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3b2d4df7d69043329153ec1355a8d14d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3c20a3fb99a3438692cb5107774b0ce3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_655d8c6e9eff49e4b8959eee9181a12b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_38b5bef79d3a45fbb4d2783406c625b4",
      "value": "model.safetensors:â€‡100%"
     }
    },
    "3ee652ae942346528f4a06ee5ff37d6a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40bae95b0e714dd0a833cfc6a8609eee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "428946310cd848f991a5943a8a288fbd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45275f1f6a62484ba93727f83deecba2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a0bda4f4f6a4b44b36511d35d946199",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ba38df98b23d4d42bf7a950be5e5a8d0",
      "value": "â€‡366/366â€‡[00:00&lt;00:00,â€‡18.4kB/s]"
     }
    },
    "4598e7a4a68b49b5a5f2a891b007a19d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "47295889af0740c8b319bd7db2369b98": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a23a8299b72642828e8bf2b9b3e9c6dd",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_02f5083e008841188b41e5d385f54c7b",
      "value": "â€‡439M/439Mâ€‡[00:08&lt;00:00,â€‡79.7MB/s]"
     }
    },
    "47f4738cf0d840b1a7f6e6533eeed61f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ffadaf728bbf44cc8e36446a7aa2af46",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_7b7cff9050cb45f987d4ef2d65e75c4c",
      "value": "â€‡226k/?â€‡[00:00&lt;00:00,â€‡7.35MB/s]"
     }
    },
    "48b9e3ee3a274d26b6bef10cb805c412": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a307d1b81e447d6a231e13a86fa8276": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4a4b5dba9b3543e686338a482f83d7d5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e8c4514f72c941b69089d35281e972fc",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d1f4028c2c754343b68e8b7cb5d059b5",
      "value": "pytorch_model.bin:â€‡100%"
     }
    },
    "4bcf1bcb4ed14ebd927654daa156e47c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a3d711a16e264baca2c220562b610380",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2616ee7a88484474b70321af416a0176",
      "value": 1
     }
    },
    "4bf239257b504d5a88cdee43a6a24d27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "RadioButtonsModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "RadioButtonsModel",
      "_options_labels": [
       "Use Existing Collection",
       "Create New Collection"
      ],
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "RadioButtonsView",
      "description": "Action:",
      "description_tooltip": null,
      "disabled": false,
      "index": 1,
      "layout": "IPY_MODEL_4598e7a4a68b49b5a5f2a891b007a19d",
      "style": "IPY_MODEL_7b877a6c71ae42d5967dc47b5a10c19f"
     }
    },
    "4cb64aa997db4b14afb7a21635a0bfd0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_141234bcec25497bb1c019566f17960f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9170bfb81b3d4a11827455ff1fe8e4cd",
      "value": "â€‡779/779â€‡[00:00&lt;00:00,â€‡51.2kB/s]"
     }
    },
    "4cc56f7d6eba4c86be3c9c370033543f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d91973b704541b6946bd81207b7b85d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ada91f771f404f02ac49ca3b4beaef05",
      "max": 41,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_891ef269dff34f60bfb0762558b71e6a",
      "value": 41
     }
    },
    "4e50b0fc38b448078c0f284cb3f96d59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f509299b1ff74309bc7301637c6aea84",
       "IPY_MODEL_72fc22111351409ea86296557f01d0c7",
       "IPY_MODEL_47f4738cf0d840b1a7f6e6533eeed61f"
      ],
      "layout": "IPY_MODEL_d32c6bd92d304d8a89c2c78e49a387a5"
     }
    },
    "4f1116b208c14ad2b58f926b7f2ee91e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f7b7ec82c79c442c8ff7fd341db187f3",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f10815bd490146cb8ccc0a7ebc1091ba",
      "value": "â€‡232k/?â€‡[00:00&lt;00:00,â€‡7.61MB/s]"
     }
    },
    "508a29f08800481ab68e1ee9b655bc6f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "50b9ed7391c44f229ae719bc984d2ae1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51228e359a1f40b59d86fd23337e104e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "53aec41823db4ae99f038b95f884f5f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "53d00d8abfce4a089712dc8dc313e551": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ffbe959a1106473ab2a0ebfe989018c3",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_0a380cc99b1a445d9360116d41fb92c4",
      "value": "tokenizer.json:â€‡"
     }
    },
    "56884ad067b94258826ce20bc5327807": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_508a29f08800481ab68e1ee9b655bc6f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9b4a1ecb8bc445b69b3dfdc6e18fd658",
      "value": "â€‡94.6k/?â€‡[00:00&lt;00:00,â€‡7.08MB/s]"
     }
    },
    "56b385661ac84a56b93d545f597890bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f161ff575f3d41258f107bd48d999e2a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_640c048bc0334f8183710dded3534467",
      "value": "sentence_bert_config.json:â€‡100%"
     }
    },
    "56b9c6143d70447c8bb465e739b3f7be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_53d00d8abfce4a089712dc8dc313e551",
       "IPY_MODEL_605bdb8f930d406ab4b75bd313368171",
       "IPY_MODEL_a9b702ed3ca14c6caa7c50b8cc75b808"
      ],
      "layout": "IPY_MODEL_9b55655f52e141359c1fa416c6258130"
     }
    },
    "56ce2670b5d64609a1318e649293c5f3": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_7b1288b3e4e043c1ab056f3b1f44e810",
      "msg_id": "",
      "outputs": [
       {
        "name": "stdout",
        "output_type": "stream",
        "text": [
         "Created new collection: 'pra_rules'\n",
         "Created new collection: 'pra_rules'\n",
         "Active collection set to: 'pra_rules'\n"
        ]
       }
      ]
     }
    },
    "585c8e51b9a64aa9ba9f43a59744b8ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "590bb1e289474fc2b8b4b44080ea959f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "59f07e8d71634443bb25868524f4cea9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b11374aa95043498807e683c2c52398": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_50b9ed7391c44f229ae719bc984d2ae1",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_833cd928eaec4488941db2214b1d3e4f",
      "value": "â€‡94/94â€‡[02:05&lt;00:00,â€‡â€‡1.28it/s]"
     }
    },
    "5b64fbbf9b954f01ae804fbb984b79e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5b99e78a40ea444eab58749ed67c2cc4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d18f8bc83b1472493490e2166817ba3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_29294a8b8e5b4b4a9eff16e89beee7bd",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b58ba8fd807649f597c5f756f0d06aa7",
      "value": "â€‡41/41â€‡[00:00&lt;00:00,â€‡314.12file/s,â€‡file=Marketâ€‡Risk_16-09-2025.pdf,â€‡chunks=16,â€‡total_chunks=2991]"
     }
    },
    "5e8566d45d3242d1bda993cffd6dd24a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "605bdb8f930d406ab4b75bd313368171": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_590bb1e289474fc2b8b4b44080ea959f",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_70b16cb1feef442e995bad215de8502f",
      "value": 1
     }
    },
    "605f28c35ef4440cae089adae8cce346": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e5185ba9714648eba24081581e8b309f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_70b39d198820496e973ff45773b670ce",
      "value": "config.json:â€‡100%"
     }
    },
    "6073bfc4b1744d148a4fae22a751ec3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "608e3756b92d4a298eb9cd237ca40819": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "610c6838db514d8a826f49899438c302": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d4904c83f08f4089be8f34a6f416d39d",
       "IPY_MODEL_e7ec48374ac443d0ab0e092ef32516b9",
       "IPY_MODEL_4cb64aa997db4b14afb7a21635a0bfd0"
      ],
      "layout": "IPY_MODEL_40bae95b0e714dd0a833cfc6a8609eee"
     }
    },
    "640c048bc0334f8183710dded3534467": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "64607f894dee4f1ca4d091a7a57ec50b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a9155532191244819dcd06dd91fbcdeb",
       "IPY_MODEL_b9b3496df4ec4d21b58f483488a81c58",
       "IPY_MODEL_b0dfa7efbdc941bbab21ca7a69f3e288"
      ],
      "layout": "IPY_MODEL_37baf24762364d0290cd6b647f7e90b8"
     }
    },
    "655d8c6e9eff49e4b8959eee9181a12b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66fab4aa7d894d1fac6a01ee3c49a941": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f2ec9f58668b4f6dbdca6073ec463dde",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_1763329f29de4e42b83c9517fe9499d1",
      "value": "Batches:â€‡100%"
     }
    },
    "677b520b744c4187807e9479d6ad4eaa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_428946310cd848f991a5943a8a288fbd",
      "max": 439044180,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4a307d1b81e447d6a231e13a86fa8276",
      "value": 439044180
     }
    },
    "67846eb0a87c4b9f9fc18420f72a9692": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "TextareaModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "TextareaModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "TextareaView",
      "continuous_update": true,
      "description": "Description:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_1f8306c9f7e04297b3e752213242e32c",
      "placeholder": "Optional: Enter collection description",
      "rows": 2,
      "style": "IPY_MODEL_a29eb5484bd84d15a246755ec07411e7",
      "value": ""
     }
    },
    "68381eb90f49438ebb082bf35be5b0c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6a04291428514dbdb0498885efaa01af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2ddd12074a2048d2979f5fd980594c63",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_870222a9c6664250ba5b289ae3f53a92",
      "value": "Processingâ€‡files:â€‡100%"
     }
    },
    "6a5454b490ed4cafb05e2ae47c25fcb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "success",
      "description": "Confirm Selection",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_20c9e354331e4f98bd73bb9bfdc1a138",
      "style": "IPY_MODEL_1dfab6eccd434df1b2fb51a03d1c6209",
      "tooltip": ""
     }
    },
    "6b457f6bdc724fd49699c24220d8c3de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6b8368f50f434023b3151b72301aa39a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_edf6a04bded343ee9166d3eae6d64ab1",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_6f3ee0bde6ab41e281fed26154e62103",
      "value": "â€‡349/349â€‡[00:00&lt;00:00,â€‡42.1kB/s]"
     }
    },
    "6bdbb23315804666bfc78bdfe1c7a54f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4cc56f7d6eba4c86be3c9c370033543f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_8c96043c30be48d191b6ceef997a5a0f",
      "value": "â€‡52.0/52.0â€‡[00:00&lt;00:00,â€‡3.45kB/s]"
     }
    },
    "6c1ecca5215d41d89a0f830cd8c25225": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_100e97e7f6d84d0683a4c705e4fe2eff",
       "IPY_MODEL_a903083c92cd4b788191bf6e9ee46695",
       "IPY_MODEL_0025cf09b3e74298acf17160ed680e38"
      ],
      "layout": "IPY_MODEL_3ee652ae942346528f4a06ee5ff37d6a"
     }
    },
    "6ee385ee9ec2478786a2afacaed46903": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f3ee0bde6ab41e281fed26154e62103": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6fc3e6fe1c684a638504dea7e1b664c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "70b16cb1feef442e995bad215de8502f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "70b39d198820496e973ff45773b670ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "711e33111d9f480c9a634f3ec23c63c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "72fc22111351409ea86296557f01d0c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e0160cfe36c74422b1091fa6eb4ae8bd",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2f222110d2fa4738810ad83b97b4b7f0",
      "value": 1
     }
    },
    "73ca92175cfc4616b103334eecaa2e7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7526b1d3d6b046bd8151241402cb2185": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4a4b5dba9b3543e686338a482f83d7d5",
       "IPY_MODEL_308459b8458b4affb27a58bd8b7c2e66",
       "IPY_MODEL_47295889af0740c8b319bd7db2369b98"
      ],
      "layout": "IPY_MODEL_24c7270a584b41be8820167b95a95772"
     }
    },
    "75fdcd047c1f4cbe8fa1e09ff686f1e9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "78ad3fa9cda44354a406e245277bc06c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7a0bda4f4f6a4b44b36511d35d946199": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a8a48a861774310b3f4be6f6375eae6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a10cc5788f845f1a05ff9c277e3d318",
      "max": 366,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0d747272701a4dbea08cb3a315b3ed2f",
      "value": 366
     }
    },
    "7a995d4f93cb42378850d78897a9fe68": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7aa4d76a95ad45b798bf831bbf1a59fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b1288b3e4e043c1ab056f3b1f44e810": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b7cff9050cb45f987d4ef2d65e75c4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7b877a6c71ae42d5967dc47b5a10c19f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "initial"
     }
    },
    "7c9acbfc6d2b47a0918f7a4b3346c3e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7daad003234b474daf0d91a4fcc15bb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6a04291428514dbdb0498885efaa01af",
       "IPY_MODEL_febc47fe39844623a20ac1dc515ece75",
       "IPY_MODEL_8f606ffbd4aa4fffbc20e1ac6d9befa0"
      ],
      "layout": "IPY_MODEL_21c38215f12849bfb1e25f78bfdcb140"
     }
    },
    "7f04e87948cc4ade8dc23cb39377ad8f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fba7798ba0d54431b8742df5f26ba51b",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_a828c12cde2f4df896d1eb55c871519c",
      "value": "â€‡125/125â€‡[00:00&lt;00:00,â€‡9.37kB/s]"
     }
    },
    "833b03027f334e738ce01bb374d443c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bce9801e5bf741e5802ec329b896b865",
       "IPY_MODEL_25ebfb9caa314661b0ab5141d017758c",
       "IPY_MODEL_6b8368f50f434023b3151b72301aa39a"
      ],
      "layout": "IPY_MODEL_b9d3d39bcfb244bc9a891e2d59cedb90"
     }
    },
    "833cd928eaec4488941db2214b1d3e4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "84e01f7ee39844b796f03a11b9be1407": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "870222a9c6664250ba5b289ae3f53a92": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "891ef269dff34f60bfb0762558b71e6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8ae89d9f25ec41cdbfd7bd1de22d7cc0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8b5327ec92eb4f87a93403538fa28fea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_66fab4aa7d894d1fac6a01ee3c49a941",
       "IPY_MODEL_f612d957ea1c45ca8a3418128ed4d0bb",
       "IPY_MODEL_5b11374aa95043498807e683c2c52398"
      ],
      "layout": "IPY_MODEL_cb28e591587c4bd1b5e14c27d883b6f2"
     }
    },
    "8c833970702c4cf09e893f64f2485c0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bb943a030ce847f4baec8737b2a9da2c",
       "IPY_MODEL_0bf2fb6b6f084cec912a4c6c4168b223",
       "IPY_MODEL_7f04e87948cc4ade8dc23cb39377ad8f"
      ],
      "layout": "IPY_MODEL_51228e359a1f40b59d86fd23337e104e"
     }
    },
    "8c96043c30be48d191b6ceef997a5a0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f3f797232bf41d58f0c302a156fe6e3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f606ffbd4aa4fffbc20e1ac6d9befa0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_398a0b724d6c40cdb18dbda2554472d6",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d554f4d63f3b498e8510afc02dd3ca38",
      "value": "â€‡41/41â€‡[00:59&lt;00:00,â€‡â€‡1.48s/it,â€‡failed=0,â€‡success=41]"
     }
    },
    "9170bfb81b3d4a11827455ff1fe8e4cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "92a781762a7f44a1aaa4f9c75a60cb9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_311b0478d5a9425bbd28e2d82cc96a3b",
       "IPY_MODEL_4d91973b704541b6946bd81207b7b85d",
       "IPY_MODEL_5d18f8bc83b1472493490e2166817ba3"
      ],
      "layout": "IPY_MODEL_ee6f1b3e8d67403fa2594fb5fbfa39f8"
     }
    },
    "94bb9295d9c74b0b82d54c48c88fc5b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9757ad7f25c54c2f9349332160d6111b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_35bbb9f8bae24e99bbbf08dfbe766b75",
       "IPY_MODEL_d4792208411c49f9b50cbd629efdac2c",
       "IPY_MODEL_e861f7a725574e368fefbfeea0fbab92"
      ],
      "layout": "IPY_MODEL_f18c5e64a0ee42adbe415805ec6e7c7c"
     }
    },
    "98010a8969af428b91ec62cf5c92c6c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aef208e7e95443fb82b78e192050219c",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6073bfc4b1744d148a4fae22a751ec3b",
      "value": 1
     }
    },
    "98e05433f6d644d68319f9ee6ad0365e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d01771254f1747a1b7f06a8bc10d333e",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_73ca92175cfc4616b103334eecaa2e7b",
      "value": "â€‡191/191â€‡[00:00&lt;00:00,â€‡8.67kB/s]"
     }
    },
    "99745fb34d784204bf8a6584dbd58d11": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ac7f2ab4a21a4465bd7f6f13a1d6b3a4",
      "max": 191,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8ae89d9f25ec41cdbfd7bd1de22d7cc0",
      "value": 191
     }
    },
    "9b4a1ecb8bc445b69b3dfdc6e18fd658": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9b55655f52e141359c1fa416c6258130": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9c9293eca0e042458da526ea365549d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9ce72a461cd848f7a65c8396466f5c3b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e871ff59d3d4839ae34944515127c8f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a23a8299b72642828e8bf2b9b3e9c6dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a29eb5484bd84d15a246755ec07411e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a33ed0a08b0c4b3f9eb5a0e9fcf6991d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e8795a4f719840c3a02d9bd0cc822e8e",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ea78889dd13b4ce18a0ed64e46572cb4",
      "value": "vocab.txt:â€‡"
     }
    },
    "a3d711a16e264baca2c220562b610380": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "a828c12cde2f4df896d1eb55c871519c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a903083c92cd4b788191bf6e9ee46695": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a995d4f93cb42378850d78897a9fe68",
      "max": 2991,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_39a63fff1694413b8ed1449d9e8d88df",
      "value": 2991
     }
    },
    "a9155532191244819dcd06dd91fbcdeb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_da1a9a7561654e3baa8d5e5d0c752bfb",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_6b457f6bdc724fd49699c24220d8c3de",
      "value": "config_sentence_transformers.json:â€‡100%"
     }
    },
    "a9b702ed3ca14c6caa7c50b8cc75b808": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_baa965f5e6b14c46a046d393fba6eab9",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_608e3756b92d4a298eb9cd237ca40819",
      "value": "â€‡711k/?â€‡[00:00&lt;00:00,â€‡10.5MB/s]"
     }
    },
    "aab40088907042ffa62b271529bcf09d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ab18b97898d44a90a295809f45c1efda": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_06739ce3b5a54ab38fcdf0c596b9aa82",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_34613f7903194d29b9544094280ad8e5",
      "value": "â€‡533/533â€‡[00:00&lt;00:00,â€‡22.3kB/s]"
     }
    },
    "ac7f2ab4a21a4465bd7f6f13a1d6b3a4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad49598e795f4d42ac8ff98436b6f842": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_de421b64497a426792f0eb3362984d5f",
      "max": 52,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5b64fbbf9b954f01ae804fbb984b79e7",
      "value": 52
     }
    },
    "ada91f771f404f02ac49ca3b4beaef05": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aef208e7e95443fb82b78e192050219c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "af012e52c3ad4f539164a67364831a55": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b0dfa7efbdc941bbab21ca7a69f3e288": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_68381eb90f49438ebb082bf35be5b0c7",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_e4ac3c8083c44d649b3ada9e1cdb5e87",
      "value": "â€‡124/124â€‡[00:00&lt;00:00,â€‡7.90kB/s]"
     }
    },
    "b58ba8fd807649f597c5f756f0d06aa7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b67464f9f18c4d79a115d460807f9797": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "TextModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "TextModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "TextView",
      "continuous_update": true,
      "description": "New Name:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_ec9b252f9a064afc8e5fb4930e5df670",
      "placeholder": "Enter new collection name",
      "style": "IPY_MODEL_3b2d4df7d69043329153ec1355a8d14d",
      "value": "pra_rules"
     }
    },
    "b87f211d6b714915a3544d78972584b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5e8566d45d3242d1bda993cffd6dd24a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_f650c5c75dbc4a01a2f2dcfe9c952b00",
      "value": "â€‡439M/439Mâ€‡[00:10&lt;00:00,â€‡48.7MB/s]"
     }
    },
    "b9b3496df4ec4d21b58f483488a81c58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_efe44030aab94184b0033e4da4a79158",
      "max": 124,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_78ad3fa9cda44354a406e245277bc06c",
      "value": 124
     }
    },
    "b9d3d39bcfb244bc9a891e2d59cedb90": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba38df98b23d4d42bf7a950be5e5a8d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "baa965f5e6b14c46a046d393fba6eab9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb943a030ce847f4baec8737b2a9da2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0640cd714c0f4eb1a22726bf46b5dd18",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_aab40088907042ffa62b271529bcf09d",
      "value": "special_tokens_map.json:â€‡100%"
     }
    },
    "bce9801e5bf741e5802ec329b896b865": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_31f95e14f4a248729c23f5631b1853bf",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_eb3fe9ae2cd4493189b6d887726e1e83",
      "value": "modules.json:â€‡100%"
     }
    },
    "c07c4842d1134e74bea6598bbad88766": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c75919cc28a6413dbbbed67b3626482b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1e5e4a4748ce45a18da266533d9c85e2",
       "IPY_MODEL_4bcf1bcb4ed14ebd927654daa156e47c",
       "IPY_MODEL_56884ad067b94258826ce20bc5327807"
      ],
      "layout": "IPY_MODEL_59f07e8d71634443bb25868524f4cea9"
     }
    },
    "cb28e591587c4bd1b5e14c27d883b6f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ccbeba7ed9ec4194a5a277cf4754d32b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ec28d70c853c4302bc9c53811f8dd241",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_6fc3e6fe1c684a638504dea7e1b664c6",
      "value": "tokenizer_config.json:â€‡100%"
     }
    },
    "d01771254f1747a1b7f06a8bc10d333e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d0ad5334aef748c0b138c6e97554566d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d1f4028c2c754343b68e8b7cb5d059b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d241d8f02dd245edbefaccc0499cc64d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d2548a924bf64ff3b48b8ee390c9b192": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d32c6bd92d304d8a89c2c78e49a387a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d43b73ef7dcf4768a6bfb702e0a54249": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d4792208411c49f9b50cbd629efdac2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_16a23106d15f412995eb9ba0329c1886",
      "max": 1340616616,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d43b73ef7dcf4768a6bfb702e0a54249",
      "value": 1340616616
     }
    },
    "d4904c83f08f4089be8f34a6f416d39d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8f3f797232bf41d58f0c302a156fe6e3",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_333cab60c2da494694b92af7d03ee31c",
      "value": "config.json:â€‡100%"
     }
    },
    "d4c30e040f2a49d0855e4445b487a763": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_27fc018c9245466ebc03ee5f94bbb540",
       "IPY_MODEL_dcf77be27f0b4eb98dcc1950b76fc5d9",
       "IPY_MODEL_ab18b97898d44a90a295809f45c1efda"
      ],
      "layout": "IPY_MODEL_1beafb96d3d94a868938b93b92e8dbd3"
     }
    },
    "d554f4d63f3b498e8510afc02dd3ca38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d7711e822a624d3ea0bfbb814a620401": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da1a9a7561654e3baa8d5e5d0c752bfb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc0018e78e8d45d18d397f7ee2d6d342": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4bf239257b504d5a88cdee43a6a24d27",
       "IPY_MODEL_273a066dd98b49658189b8e03399444b",
       "IPY_MODEL_b67464f9f18c4d79a115d460807f9797",
       "IPY_MODEL_67846eb0a87c4b9f9fc18420f72a9692",
       "IPY_MODEL_6a5454b490ed4cafb05e2ae47c25fcb5",
       "IPY_MODEL_56ce2670b5d64609a1318e649293c5f3"
      ],
      "layout": "IPY_MODEL_94bb9295d9c74b0b82d54c48c88fc5b4"
     }
    },
    "dcf77be27f0b4eb98dcc1950b76fc5d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6ee385ee9ec2478786a2afacaed46903",
      "max": 533,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d2548a924bf64ff3b48b8ee390c9b192",
      "value": 533
     }
    },
    "de421b64497a426792f0eb3362984d5f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0160cfe36c74422b1091fa6eb4ae8bd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "e4ac3c8083c44d649b3ada9e1cdb5e87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e5185ba9714648eba24081581e8b309f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5381ec6ff6a4abdb5ea26b3e92b918d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e7ec48374ac443d0ab0e092ef32516b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_325f8734571442049c1a0f2d1442a0ac",
      "max": 779,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_711e33111d9f480c9a634f3ec23c63c4",
      "value": 779
     }
    },
    "e861f7a725574e368fefbfeea0fbab92": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d7711e822a624d3ea0bfbb814a620401",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_011125111788426885b873cafbd87a78",
      "value": "â€‡1.34G/1.34Gâ€‡[00:19&lt;00:00,â€‡59.9MB/s]"
     }
    },
    "e8795a4f719840c3a02d9bd0cc822e8e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8c4514f72c941b69089d35281e972fc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e9df0b1218b4435c89f36069e73f7262": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea78889dd13b4ce18a0ed64e46572cb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb3fe9ae2cd4493189b6d887726e1e83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ec28d70c853c4302bc9c53811f8dd241": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ec9b252f9a064afc8e5fb4930e5df670": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "edf6a04bded343ee9166d3eae6d64ab1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee6f1b3e8d67403fa2594fb5fbfa39f8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "efe44030aab94184b0033e4da4a79158": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0201465ea594ee2a7562463453c773d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f10815bd490146cb8ccc0a7ebc1091ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f161ff575f3d41258f107bd48d999e2a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f16b068e44bd4344a6826201e574e563": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_56b385661ac84a56b93d545f597890bd",
       "IPY_MODEL_ad49598e795f4d42ac8ff98436b6f842",
       "IPY_MODEL_6bdbb23315804666bfc78bdfe1c7a54f"
      ],
      "layout": "IPY_MODEL_e5381ec6ff6a4abdb5ea26b3e92b918d"
     }
    },
    "f18c5e64a0ee42adbe415805ec6e7c7c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2ec9f58668b4f6dbdca6073ec463dde": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4ea57dee719490290fe2b945d291a23": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f509299b1ff74309bc7301637c6aea84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f9bac6a1fb5446a293c460944919c2a4",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_09702809260543fc8d2d098ba2eff48b",
      "value": "vocab.txt:â€‡"
     }
    },
    "f612d957ea1c45ca8a3418128ed4d0bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_48b9e3ee3a274d26b6bef10cb805c412",
      "max": 94,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_32d7a49e2aa24ac58ee4c3eff272e3e5",
      "value": 94
     }
    },
    "f650c5c75dbc4a01a2f2dcfe9c952b00": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f7b7ec82c79c442c8ff7fd341db187f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f9bac6a1fb5446a293c460944919c2a4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fba7798ba0d54431b8742df5f26ba51b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "febc47fe39844623a20ac1dc515ece75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_75fdcd047c1f4cbe8fa1e09ff686f1e9",
      "max": 41,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d0ad5334aef748c0b138c6e97554566d",
      "value": 41
     }
    },
    "ffadaf728bbf44cc8e36446a7aa2af46": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ffbe959a1106473ab2a0ebfe989018c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
